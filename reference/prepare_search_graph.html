<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="Create a graph using existing nearest neighbor data to balance search
speed and accuracy using the occlusion pruning and truncation strategies
of Harwood and Drummond (2016)."><title>Nearest Neighbor Graph Refinement — prepare_search_graph • rnndescent</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Nearest Neighbor Graph Refinement — prepare_search_graph"><meta property="og:description" content="Create a graph using existing nearest neighbor data to balance search
speed and accuracy using the occlusion pruning and truncation strategies
of Harwood and Drummond (2016)."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">rnndescent</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.16</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item">
  <a class="nav-link" href="../articles/rnndescent.html">Get started</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/brute-force.html">Brute Force Search</a>
    <a class="dropdown-item" href="../articles/nearest-neighbor-descent.html">Nearest Neighbor Descent</a>
    <a class="dropdown-item" href="../articles/random-partition-forests.html">Random Partition Forests</a>
    <a class="dropdown-item" href="../articles/querying-data.html">Querying Data</a>
    <a class="dropdown-item" href="../articles/hubness.html">Hubness</a>
    <a class="dropdown-item" href="../articles/metrics.html">Metrics</a>
    <div class="dropdown-divider"></div>
    <a class="dropdown-item" href="../articles/index.html">More articles...</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/jlmelville/rnndescent/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Nearest Neighbor Graph Refinement</h1>
      <small class="dont-index">Source: <a href="https://github.com/jlmelville/rnndescent/blob/HEAD/R/rnndescent.R" class="external-link"><code>R/rnndescent.R</code></a></small>
      <div class="d-none name"><code>prepare_search_graph.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Create a graph using existing nearest neighbor data to balance search
speed and accuracy using the occlusion pruning and truncation strategies
of Harwood and Drummond (2016).</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">prepare_search_graph</span><span class="op">(</span></span>
<span>  <span class="va">data</span>,</span>
<span>  <span class="va">graph</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>  diversify_prob <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  pruning_degree_multiplier <span class="op">=</span> <span class="fl">1.5</span>,</span>
<span>  n_threads <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  obs <span class="op">=</span> <span class="st">"R"</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>data</dt>
<dd><p>Matrix of <code>n</code> items, with observations in the rows and features
in the columns. Optionally, input can be passed with observations in the
columns, by setting <code>obs = "C"</code>, which should be more efficient. Possible
formats are <code><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">base::data.frame()</a></code>, <code><a href="https://rdrr.io/r/base/matrix.html" class="external-link">base::matrix()</a></code> or
<code><a href="https://rdrr.io/pkg/Matrix/man/sparseMatrix.html" class="external-link">Matrix::sparseMatrix()</a></code>. Sparse matrices should be in <code>dgCMatrix</code> format.
Dataframes will be converted to <code>numerical</code> matrix format internally, so if
your data columns are <code>logical</code> and intended to be used with the
specialized binary <code>metric</code>s, you should convert it to a logical matrix
first (otherwise you will get the slower dense numerical version).</p></dd>


<dt>graph</dt>
<dd><p>neighbor graph for <code>data</code>, a list containing:</p><ul><li><p><code>idx</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor indices of
the data in <code>data</code>.</p></li>
<li><p><code>dist</code> an <code>n</code> by <code>k</code> matrix containing the nearest neighbor distances.</p></li>
</ul></dd>


<dt>metric</dt>
<dd><p>Type of distance calculation to use. One of:</p><ul><li><p><code>"braycurtis"</code></p></li>
<li><p><code>"canberra"</code></p></li>
<li><p><code>"chebyshev"</code></p></li>
<li><p><code>"correlation"</code> (1 minus the Pearson correlation)</p></li>
<li><p><code>"cosine"</code></p></li>
<li><p><code>"dice"</code></p></li>
<li><p><code>"euclidean"</code></p></li>
<li><p><code>"hamming"</code></p></li>
<li><p><code>"hellinger"</code></p></li>
<li><p><code>"jaccard"</code></p></li>
<li><p><code>"jensenshannon"</code></p></li>
<li><p><code>"kulsinski"</code></p></li>
<li><p><code>"sqeuclidean"</code> (squared Euclidean)</p></li>
<li><p><code>"manhattan"</code></p></li>
<li><p><code>"rogerstanimoto"</code></p></li>
<li><p><code>"russellrao"</code></p></li>
<li><p><code>"sokalmichener"</code></p></li>
<li><p><code>"sokalsneath"</code></p></li>
<li><p><code>"spearmanr"</code> (1 minus the Spearman rank correlation)</p></li>
<li><p><code>"symmetrickl"</code> (symmetric Kullback-Leibler divergence)</p></li>
<li><p><code>"tsss"</code> (Triangle Area Similarity-Sector Area Similarity or TS-SS
metric)</p></li>
<li><p><code>"yule"</code></p></li>
</ul><p>For non-sparse data, the following variants are available with
preprocessing: this trades memory for a potential speed up during the
distance calculation. Some minor numerical differences should be expected
compared to the non-preprocessed versions:</p><ul><li><p><code>"cosine-preprocess"</code>: <code>cosine</code> with preprocessing.</p></li>
<li><p><code>"correlation-preprocess"</code>: <code>correlation</code> with preprocessing.</p></li>
</ul><p>For non-sparse binary data passed as a <code>logical</code> matrix, the following
metrics have specialized variants which should be substantially faster than
the non-binary variants (in other cases the logical data will be treated as
a dense numeric vector of 0s and 1s):</p><ul><li><p><code>"dice"</code></p></li>
<li><p><code>"hamming"</code></p></li>
<li><p><code>"jaccard"</code></p></li>
<li><p><code>"kulsinski"</code></p></li>
<li><p><code>"matching"</code></p></li>
<li><p><code>"rogerstanimoto"</code></p></li>
<li><p><code>"russellrao"</code></p></li>
<li><p><code>"sokalmichener"</code></p></li>
<li><p><code>"sokalsneath"</code></p></li>
<li><p><code>"yule"</code></p></li>
</ul></dd>


<dt>diversify_prob</dt>
<dd><p>the degree of diversification of the search graph
by removing unnecessary edges through occlusion pruning. This should take a
value between <code>0</code> (no diversification) and <code>1</code> (remove as many edges as
possible) and is treated as the probability of a neighbor being removed if
it is found to be an "occlusion". If item <code>p</code> and <code>q</code>, two members of the
neighbor list of item <code>i</code>, are closer to each other than they are to <code>i</code>,
then the nearer neighbor <code>p</code> is said to "occlude" <code>q</code>. It is likely that
<code>q</code> will be in the neighbor list of <code>p</code> so there is no need to retain it in
the neighbor list of <code>i</code>. You may also set this to <code>NULL</code> to skip any
occlusion pruning. Note that occlusion pruning is carried out twice, once
to the forward neighbors, and once to the reverse neighbors. Reducing this
value will result in a more dense graph. This is similar to increasing the
"alpha" parameter used by in the DiskAnn pruning method of Subramanya and
co-workers (2014).</p></dd>


<dt>pruning_degree_multiplier</dt>
<dd><p>How strongly to truncate the final neighbor
list for each item. The neighbor list of each item will be truncated to
retain only the closest <code>d</code> neighbors, where
<code>d = k * pruning_degree_multiplier</code>, and <code>k</code> is the
original number of neighbors per item in <code>graph</code>. Roughly, values
larger than <code>1</code> will keep all the nearest neighbors of an item, plus
the given fraction of reverse neighbors (if they exist). For example,
setting this to <code>1.5</code> will keep all the forward neighbors and then
half as many of the reverse neighbors, although exactly which neighbors are
retained is also dependent on any occlusion pruning that occurs. Set this
to <code>NULL</code> to skip this step.</p></dd>


<dt>n_threads</dt>
<dd><p>Number of threads to use.</p></dd>


<dt>verbose</dt>
<dd><p>If <code>TRUE</code>, log information to the console.</p></dd>


<dt>obs</dt>
<dd><p>set to <code>"C"</code> to indicate that the input <code>data</code> orientation stores
each observation as a column. The default <code>"R"</code> means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p>a search graph for <code>data</code> based on <code>graph</code>, represented as a sparse
matrix, suitable for use with <code><a href="graph_knn_query.html">graph_knn_query()</a></code>.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>An approximate nearest neighbor graph is not very useful for querying via
<code><a href="graph_knn_query.html">graph_knn_query()</a></code>, especially if the query data is initialized randomly:
some items in the data set may not be in the nearest neighbor list of any
other item and can therefore never be returned as a neighbor, no matter how
close they are to the query. Even those which do appear in at least one
neighbor list may not be reachable by expanding an arbitrary starting list if
the neighbor graph contains disconnected components.</p>
<p>Converting the directed graph represented by the neighbor graph to an
undirected graph by adding an edge from item <code>j</code> to <code>i</code> if
an edge exists from <code>i</code> to <code>j</code> (i.e. creating the mutual neighbor
graph) solves the problems above, but can result in inefficient searches.
Although the out-degree of each item is restricted to the number of neighbors
the in-degree has no such restrictions: a given item could be very "popular"
and in a large number of neighbors lists. Therefore mutualizing the neighbor
graph can result in some items with a large number of neighbors to search.
These usually have very similar neighborhoods so there is nothing to be
gained from searching all of them.</p>
<p>To balance accuracy and search time, the following procedure is carried out:</p><ol><li><p>The graph is "diversified" by occlusion pruning.</p></li>
<li><p>The reverse graph is formed by reversing the direction of all edges in
the pruned graph.</p></li>
<li><p>The reverse graph is diversified by occlusion pruning.</p></li>
<li><p>The pruned forward and pruned reverse graph are merged.</p></li>
<li><p>The outdegree of each node in the merged graph is truncated.</p></li>
<li><p>The truncated merged graph is returned as the prepared search graph.</p></li>
</ol><p>Explicit zero distances in the <code>graph</code> will be converted to a small positive
number to avoid being dropped in the sparse representation. The one exception
is the "self" distance, i.e. any edge in the <code>graph</code> which links a node to
itself (the diagonal of the sparse distance matrix). These trivial edges
aren't useful for search purposes and are always dropped.</p>
    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Harwood, B., &amp; Drummond, T. (2016).
Fanng: Fast approximate nearest neighbour graphs.
In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>
(pp. 5713-5722).</p>
<p>Jayaram Subramanya, S., Devvrit, F., Simhadri, H. V., Krishnawamy, R., &amp; Kadekodi, R. (2019).
Diskann: Fast accurate billion-point nearest neighbor search on a single node.
<em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># 100 reference iris items</span></span></span>
<span class="r-in"><span><span class="va">iris_ref</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"setosa"</span>, <span class="st">"versicolor"</span><span class="op">)</span>, <span class="op">]</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 50 query items</span></span></span>
<span class="r-in"><span><span class="va">iris_query</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"versicolor"</span>, <span class="op">]</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># First, find the approximate 4-nearest neighbor graph for the references:</span></span></span>
<span class="r-in"><span><span class="va">ref_ann_graph</span> <span class="op">&lt;-</span> <span class="fu"><a href="nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris_ref</span>, k <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Create a graph for querying with</span></span></span>
<span class="r-in"><span><span class="va">ref_search_graph</span> <span class="op">&lt;-</span> <span class="fu">prepare_search_graph</span><span class="op">(</span><span class="va">iris_ref</span>, <span class="va">ref_ann_graph</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Using the search graph rather than the ref_ann_graph directly may give</span></span></span>
<span class="r-in"><span><span class="co"># more accurate or faster results</span></span></span>
<span class="r-in"><span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span> <span class="fu"><a href="graph_knn_query.html">graph_knn_query</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  query <span class="op">=</span> <span class="va">iris_query</span>, reference <span class="op">=</span> <span class="va">iris_ref</span>,</span></span>
<span class="r-in"><span>  reference_graph <span class="op">=</span> <span class="va">ref_search_graph</span>, k <span class="op">=</span> <span class="fl">4</span>, metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span></span>
<span class="r-in"><span>  verbose <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> 00:54:22 Using alt metric 'sqeuclidean' for 'euclidean'</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> 00:54:22 Initializing from random neighbors</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> 00:54:22 Generating random k-nearest neighbor graph from reference with k = 4</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> 00:54:22 Searching nearest neighbor graph with epsilon = 0.1 and max_search_fraction = 1</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> 00:54:22 Finished</span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by James Melville.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer></div>

  

  

  </body></html>

