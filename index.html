<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="The Nearest Neighbor Descent method for finding approximate
    nearest neighbors by by Dong and co-workers (2010)
    &lt;doi:10.1145/1963405.1963487&gt;. Based on the Python package
    PyNNDescent &lt;https://github.com/lmcinnes/pynndescent&gt;.">
<title>Nearest Neighbor Descent Method for Approximate Nearest Neighbors • rnndescent</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Nearest Neighbor Descent Method for Approximate Nearest Neighbors">
<meta property="og:description" content="The Nearest Neighbor Descent method for finding approximate
    nearest neighbors by by Dong and co-workers (2010)
    &lt;doi:10.1145/1963405.1963487&gt;. Based on the Python package
    PyNNDescent &lt;https://github.com/lmcinnes/pynndescent&gt;.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="index.html">rnndescent</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.16</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="articles/rnndescent.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="articles/brute-force.html">Brute Force Search</a>
    <a class="dropdown-item" href="articles/nearest-neighbor-descent.html">Nearest Neighbor Descent</a>
    <a class="dropdown-item" href="articles/random-partition-forests.html">Random Partition Forests</a>
    <a class="dropdown-item" href="articles/querying-data.html">Querying Data</a>
    <a class="dropdown-item" href="articles/hubness.html">Hubness</a>
    <a class="dropdown-item" href="articles/metrics.html">Metrics</a>
    <div class="dropdown-divider"></div>
    <a class="dropdown-item" href="articles/index.html">More articles...</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/jlmelville/rnndescent/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="rnndescent">rnndescent<a class="anchor" aria-label="anchor" href="#rnndescent"></a>
</h1></div>
<!-- badges: start -->

<p>An R package for finding approximate nearest neighbors, translated from the Python package <a href="https://github.com/lmcinnes/pynndescent" class="external-link">PyNNDescent</a> written by the great Leland McInnes. As the name suggests, it uses the Nearest Neighbor Descent method (<a href="https://doi.org/10.1145/1963405.1963487" class="external-link">Dong et al., 2011</a>), but also makes use of Random Partition Trees (<a href="https://doi.org/10.1145/1374376.1374452" class="external-link">Dasgupta and Freund, 2008</a>) as well as ideas from <a href="https://doi.org/10.1109/CVPR.2016.616" class="external-link">FANNG</a> and <a href="https://github.com/yahoojapan/NGT" class="external-link">NGT</a>.</p>
<p>Tantalizingly close to being releasable, you can now use rnndescent for:</p>
<ul>
<li>optimizing an initial set of nearest neighbors, e.g. those generated by <a href="https://cran.r-project.org/package=RcppAnnoy" class="external-link">RcppAnnoy</a> or <a href="https://cran.r-project.org/package=RcppHNSW" class="external-link">RcppHNSW</a>.</li>
<li>using this package for nearest neighbor search all on its own…</li>
<li>… including finding nearest neighbors on sparse data, which most other packages in the R ecosystem cannot do.</li>
<li>and a much larger number of metrics than most other packages.</li>
</ul>
<div class="section level2">
<h2 id="documentation">Documentation<a class="anchor" aria-label="anchor" href="#documentation"></a>
</h2>
<p>See the <a href="https://jlmelville.github.io/rnndescent/articles/rnndescent.html">Get Started</a> article for the basics. The other <a href="https://jlmelville.github.io/rnndescent/articles/">vignettes</a> go into more detail.</p>
</div>
<div class="section level2">
<h2 id="current-status">Current Status<a class="anchor" aria-label="anchor" href="#current-status"></a>
</h2>
<p><em>19 Nov 2023</em> The <code>rnnd_build</code> function and <code>rnnd_query</code> functions have been added which simplify creating a knn/building an index and querying it, respectively and should be the main way of using the package. The other functions remain should you need more flexibility. Some functions have been removed: the local scaling and the standalone distance functions. The latter could return in a different package at some point.</p>
<div class="section level3">
<h3 id="missing-features">Missing Features<a class="anchor" aria-label="anchor" href="#missing-features"></a>
</h3>
<p>Compared to PyNNDescent, <code>rnndescent</code> is currently lacking, in decreasing order of likelihood of implementation:</p>
<ul>
<li>Only parallel batch queries are currently supported. This means that if you are trying to stream queries, where you are only querying one item at a time, you will get no parallelism.</li>
<li>The index is always passed between the C++ and R layers when building an index and querying. This is useful for portability as its easy to serialize the index (you can use <code>saveRDS</code> like any R data for example), but it’s not very efficient. Keeping the index as an R-wrapped C++ class has its own downsides but would fix that.</li>
<li>The index can also get <em>very</em> large for large (and high-dimensional) datasets.</li>
<li>Some of the distance metrics. A large number are currently supported though. See <code>Missing Metrics</code> below for those that are currently not available.</li>
<li>Custom metrics. This just isn’t feasible with a C++ implementation.</li>
</ul>
<p>The issues around index serialization and parallel behavior make <code>rnndescent</code> currently unsuitable for streaming applications where you are querying one item at a time. If you are doing batch queries, where you are querying multiple items at once, then <code>rnndescent</code> should be fine: for example, generating nearest neighbors for UMAP (maybe for use with <a href="https://github.com/jlmelville/uwot" class="external-link">uwot</a>). Dimensionality reduction is my personal use case for nearest neighbors calculation and I would like to get <code>rnndescent</code> onto CRAN in a useful-for-something state. As a result I am not targeting an initial release to support the streaming case. I would like to fix this for a subsequent release.</p>
<p>Also there is no specialized distance code to take advantage of AVX etc., so <code>rnndescent</code> is going to run slower than other packages. This wouldn’t be allowed on CRAN anyway, but might be a nice-to-have for installing from github.</p>
</div>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>No CRAN release, so you must install from this repo:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("pak")</span></span>
<span><span class="fu">pak</span><span class="fu">::</span><span class="fu"><a href="https://pak.r-lib.org/reference/pkg_install.html" class="external-link">pkg_install</a></span><span class="op">(</span><span class="st">"jlmelville/rnndescent"</span><span class="op">)</span></span></code></pre></div>
<p>This packages makes use of C++ code which must be compiled. You may have to carry out a few extra steps before being able to build:</p>
<p><strong>Windows</strong>: install <a href="https://cran.r-project.org/bin/windows/Rtools/" class="external-link">Rtools</a> and ensure <code>C:\Rtools\bin</code> is on your path.</p>
<p><strong>Mac OS X</strong>: using a custom <code>~/.R/Makevars</code> <a href="https://github.com/jlmelville/uwot/issues/1" class="external-link">may cause linking errors</a>. This sort of thing is a potential problem on all platforms but seems to bite Mac owners more. <a href="https://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#Installation-of-source-packages" class="external-link">The R for Mac OS X FAQ</a> may be helpful here to work out what you can get away with. To be on the safe side, I would advise building without a custom <code>Makevars</code>.</p>
<p>rnndescent uses C++17. This shouldn’t be too big a problem but not all R platforms support it (sorry if this affects you).</p>
</div>
<div class="section level2">
<h2 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h2>
<p>Optimizing an initial set of approximate nearest neighbors:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jlmelville.github.io/rnndescent/">rnndescent</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># both hnsw_knn and nnd_knn will remove non-numeric columns from data-frames</span></span>
<span><span class="co"># for you, but to avoid confusion, these examples will use a matrix</span></span>
<span><span class="va">irism</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="op">-</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># If you just want sensible defaults that will probably work:</span></span>
<span><span class="va">iris_knn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/rnnd_knn.html">rnnd_knn</a></span><span class="op">(</span><span class="va">irism</span>, k <span class="op">=</span> <span class="fl">15</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># If you would like to query new data, then you should build an index</span></span>
<span><span class="va">iris_index</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/rnnd_build.html">rnnd_build</a></span><span class="op">(</span><span class="va">irism</span><span class="op">)</span></span>
<span><span class="co"># the nearest neighbor graph is in iris_index$graph</span></span>
<span></span>
<span><span class="co"># For more control:</span></span>
<span><span class="co"># Generate a Random Projection knn (set n_threads for parallel search):</span></span>
<span><span class="va">iris_rp_nn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/rpf_knn.html">rpf_knn</a></span><span class="op">(</span><span class="va">irism</span>, k <span class="op">=</span> <span class="fl">15</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># nn descent improves results: set verbose = TRUE and progress = "dist", to </span></span>
<span><span class="co"># track distance sum progress over iterations</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_rp_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    progress <span class="op">=</span> <span class="st">"dist"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># search can be multi-threaded</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span></span>
<span>    <span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_rp_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># a (potentially) faster version of the algorithm is available that avoids some </span></span>
<span><span class="co"># repeated distance calculations at the cost of using more memory. Currently off</span></span>
<span><span class="co"># by default.</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span></span>
<span>    <span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_rp_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    low_memory <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span><span class="co"># You can optimize results from other methods or packages too:  </span></span>
<span><span class="co"># install.packages("RcppHNSW")</span></span>
<span><span class="co"># Use settings that don't get perfect results straight away</span></span>
<span><span class="va">iris_hnsw_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">RcppHNSW</span><span class="fu">::</span><span class="fu">hnsw_knn</span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>    M <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    distance <span class="op">=</span> <span class="st">"euclidean"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span></span>
<span>    <span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_hnsw_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    low_memory <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>You can also search the neighbor graph with new query items:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 100 reference iris items</span></span>
<span><span class="va">iris_ref</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"setosa"</span>, <span class="st">"versicolor"</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># 50 query items</span></span>
<span><span class="va">iris_query</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"versicolor"</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># The simple way. First build an index:</span></span>
<span><span class="va">index</span> <span class="op">&lt;-</span> <span class="va">iris_index</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/rnnd_build.html">rnnd_build</a></span><span class="op">(</span><span class="va">iris_ref</span>, prepare <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># Then query it</span></span>
<span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/rnnd_query.html">rnnd_query</a></span><span class="op">(</span><span class="va">index</span>, <span class="va">iris_query</span>, k <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># For more control:</span></span>
<span><span class="co"># First, find the approximate 10-nearest neighbor graph for the references:</span></span>
<span><span class="va">iris_ref_knn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris_ref</span>, k <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># For each item in iris_query find the 10 nearest neighbors in iris_ref</span></span>
<span><span class="co"># You need to pass both the reference data and the knn graph.</span></span>
<span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/graph_knn_query.html">graph_knn_query</a></span><span class="op">(</span></span>
<span>    query <span class="op">=</span> <span class="va">iris_query</span>,</span>
<span>    reference <span class="op">=</span> <span class="va">iris_ref</span>,</span>
<span>    reference_graph <span class="op">=</span> <span class="va">iris_ref_knn</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">10</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Although the above example shows the basic procedure of building the graph and then querying new data with it, the raw nearest neighbor graph isn’t very efficient in general. It’s highly advisable to use a refined search graph based on the original data and neighbor graph:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_search_graph</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/prepare_search_graph.html">prepare_search_graph</a></span><span class="op">(</span><span class="va">iris_ref</span>, <span class="va">iris_ref_knn</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/graph_knn_query.html">graph_knn_query</a></span><span class="op">(</span></span>
<span>    query <span class="op">=</span> <span class="va">iris_query</span>,</span>
<span>    reference <span class="op">=</span> <span class="va">iris_ref</span>,</span>
<span>    reference_graph <span class="op">=</span> <span class="va">iris_search_graph</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>See the help text to <code>prepare_search_graph</code> for various parameters you can change to control the trade off between speed and search accuracy.</p>
</div>
<div class="section level2">
<h2 id="initialization">Initialization<a class="anchor" aria-label="anchor" href="#initialization"></a>
</h2>
<p>The default for <code>nnd_knn</code> is to initialize with random neighbors. Set <code>init = "tree"</code> to use the random partition tree initialization.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jlmelville.github.io/rnndescent/">rnndescent</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">irism</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="op">-</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># picks indices at random and then carries out nearest neighbor descent</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>  k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>  n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># if you want the random indices and their distances:</span></span>
<span><span class="va">iris_rand_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/random_knn.html">random_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span><span class="co"># use RP tree initialization</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>  k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>  n_threads <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  init <span class="op">=</span> <span class="st">"tree"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>For initializing a knn query, there is also <code>random_knn_query</code>.</p>
</div>
<div class="section level2">
<h2 id="brute-force">Brute Force<a class="anchor" aria-label="anchor" href="#brute-force"></a>
</h2>
<p>For comparison with exact results, there is also <code>brute_force_knn</code> and <code>brute_force_knn_query</code> functions, that will generate the exact nearest neighbors by the simple process of trying every possible pair in the dataset. Obviously this becomes a very time consuming process as your dataset grows in size, even with multithreading (although the <code>iris</code> dataset in the example below doesn’t present any issues).</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_exact_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/brute_force_knn.html">brute_force_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="merging">Merging<a class="anchor" aria-label="anchor" href="#merging"></a>
</h2>
<p>Also available are two functions for merging multiple approximate nearest neighbor graphs, which will result in a new graph which is at least as good as the best graph provided. For merging pairs of graphs, use <code>merge_knn</code>:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1337</span><span class="op">)</span></span>
<span><span class="co"># Nearest neighbor descent with 15 neighbors for iris three times,</span></span>
<span><span class="co"># starting from a different random initialization each time</span></span>
<span><span class="va">iris_rnn1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_rnn2</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Merged results should be an improvement over either individual results</span></span>
<span><span class="va">iris_mnn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/merge_knn.html">merge_knn</a></span><span class="op">(</span><span class="va">iris_rnn1</span>, <span class="va">iris_rnn2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_mnn</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_rnn1</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_mnn</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_rnn2</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span></code></pre></div>
<p>If you have more than two graphs stored in memory, it’s more efficient to use the list-based version, <code>merge_knnl</code>:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1337</span><span class="op">)</span></span>
<span><span class="co"># Nearest neighbor descent with 15 neighbors for iris three times,</span></span>
<span><span class="co"># starting from a different random initialization each time</span></span>
<span><span class="va">iris_rnn1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_rnn2</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_rnn3</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">iris_mnn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/merge_knnl.html">merge_knnl</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">iris_rnn1</span>, <span class="va">iris_rnn2</span>, <span class="va">iris_rnn3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="hubness-diagnostic">Hubness Diagnostic<a class="anchor" aria-label="anchor" href="#hubness-diagnostic"></a>
</h2>
<p>The <code>k_occur</code> function takes an <code>idx</code> matrix and returns a vector of the k-occurrences for each item in the dataset. This is just the number of times an item was found in the k-nearest neighbor list of another item. If you think of the <code>idx</code> matrix as representing a directed graph where the element <code>idx[i, j]</code> in the matrix is an edge from node <code>i</code> to node <code>idx[i, j]</code>, then the k_occurrences are calculated by reversing each edge and then counts the number of edges incident to each node. Alternatively, in the nomenclature of nearest neighbor descent, it’s the size of the “reverse neighbor” list for each node.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_nnd</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span><span class="op">)</span></span>
<span><span class="va">kos</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/k_occur.html">k_occur</a></span><span class="op">(</span><span class="va">iris_nnd</span><span class="op">$</span><span class="va">idx</span><span class="op">)</span></span></code></pre></div>
<p>The k-occurrence can take a value between 0 and <code>N</code> the number of items in the dataset. Values much larger than <code>k</code> indicate that the item is potentially a hub. The presence of hubs in a dataset can reduce the accuracy of the approximate nearest neighbors returned by nearest neighbor descent, but the presence of hubs as determined by the distribution of k-occurrences is quite robust even in the case of an approximate nearest neighbor graph of low accuracy. Therefore calculating the k-occurrences on the output of nearest neighbor descent is worth doing: if the maximum k-occurrence is a lot larger than <code>k</code> (I suggest <code>10 * k</code> as a danger sign), then the accuracy of the approximate nearest neighbors may be compromised. Items with low k-occurrences are most likely to be affected in this way. Increasing <code>k</code> or the <code>max_candidates</code> parameter can help in these situations. Alternatively, querying the data against itself with <code>graph_knn_query</code> can help:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Purposely don't do a very good job with NND so we have something to improve</span></span>
<span><span class="va">iris_nnd</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_search_graph</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/prepare_search_graph.html">prepare_search_graph</a></span><span class="op">(</span><span class="va">iris</span>, <span class="va">iris_nnd</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># query and reference are the same</span></span>
<span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/graph_knn_query.html">graph_knn_query</a></span><span class="op">(</span></span>
<span>    query <span class="op">=</span> <span class="va">iris</span>,</span>
<span>    reference <span class="op">=</span> <span class="va">iris</span>,</span>
<span>    reference_graph <span class="op">=</span> <span class="va">iris_search_graph</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_nnd</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span></span>
<span>  <span class="op">)</span></span>
<span><span class="co"># Compare </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_nnd</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_query_nn</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span></code></pre></div>
<p>For more on hubness and nearest neighbors, see for example <a href="https://www.jmlr.org/papers/v11/radovanovic10a.html" class="external-link">Radovanović and co-workers, 2010</a> and <a href="https://doi.org/10.1142/S0218213019600029" class="external-link">Bratić and co-workers, 2019</a>.</p>
</div>
<div class="section level2">
<h2 id="supported-metrics">Supported Metrics<a class="anchor" aria-label="anchor" href="#supported-metrics"></a>
</h2>
<p>Euclidean, Manhattan, Cosine, Correlation (1 - the Pearson correlation, implemented as cosine distance on row-centered data) and Hamming. Note that these have been implemented in a simple fashion, so no clever (but non-portable) optimizations using AVX/SSE or specialized <code>popcount</code> routines are used.</p>
</div>
<div class="section level2">
<h2 id="citation">Citation<a class="anchor" aria-label="anchor" href="#citation"></a>
</h2>
<p>Dong, W., Moses, C., &amp; Li, K. (2011, March). Efficient k-nearest neighbor graph construction for generic similarity measures. In <em>Proceedings of the 20th international conference on World wide web</em> (pp. 577-586). ACM. <a href="https://doi.org/10.1145/1963405.1963487" class="external-link">doi.org/10.1145/1963405.1963487</a>.</p>
</div>
<div class="section level2">
<h2 id="license">License<a class="anchor" aria-label="anchor" href="#license"></a>
</h2>
<p>The R package as a whole is licensed under <a href="https://www.gnu.org/licenses/gpl-3.0.txt" class="external-link">GPLv3 or later</a>. The following files are licensed differently:</p>
<ul>
<li>
<code>inst/include/dqsample.h</code> is a modification of some sampling code from <a href="https://github.com/daqana/dqrng" class="external-link">dqrng</a> and is <a href="https://www.gnu.org/licenses/agpl-3.0.en.html" class="external-link">AGPLv3 or later</a>.</li>
<li>
<code>inst/include/RcppPerpendicular.h</code> is a modification of some code from from <a href="https://github.com/RcppCore/RcppParallel" class="external-link">RcppParallel</a> and is <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html" class="external-link">GPLv2 or later</a>
</li>
<li>The underlying nearest neighbor descent C++ library, which can be found under <code>inst/include/tdoann</code>, is licensed under the <a href="https://opensource.org/license/bsd-2-clause/" class="external-link">BSD 2-Clause</a>.</li>
</ul>
<p>As far as I know, these licenses are all compatible with re-licensing under GPLv3 or later.</p>
</div>
<div class="section level2">
<h2 id="missing-metrics">Missing Metrics<a class="anchor" aria-label="anchor" href="#missing-metrics"></a>
</h2>
<p>The following metrics are in PyNNDescent but are not supported in rnndescent:</p>
<ul>
<li>Circular Kantorovich</li>
<li>Haversine</li>
<li>Kantorovich</li>
<li>Mahalanobis</li>
<li>Minkowski</li>
<li>Sinkhorn</li>
<li>Standardised Euclidean</li>
<li>Wasserstein 1d</li>
<li>Weighted Minkowski</li>
</ul>
<p>These require passing extra information as part of the metric definition, which is not currently supported.</p>
</div>
<div class="section level2">
<h2 id="see-also">See Also<a class="anchor" aria-label="anchor" href="#see-also"></a>
</h2>
<ul>
<li>
<a href="https://github.com/lmcinnes/pynndescent" class="external-link">PyNNDescent</a>, the Python implementation.</li>
<li>
<a href="https://github.com/TatsuyaShirakawa/nndescent" class="external-link">nndescent</a>, a C++ implementation.</li>
<li>
<a href="https://github.com/dillondaudert/NearestNeighborDescent.jl" class="external-link">NearestNeighborDescent.jl</a>, a Julia implementation.</li>
<li>
<a href="https://github.com/AnabelSMRuggiero/NNDescent.cpp" class="external-link">NNDescent.cpp</a>, another C++ implementation.</li>
<li>
<a href="https://github.com/brj0/nndescent" class="external-link">nndescent</a>, another C++ implementation, with Python bindings.</li>
</ul>
</div>
<div class="section level2">
<h2 id="old-news">Old News<a class="anchor" aria-label="anchor" href="#old-news"></a>
</h2>
<p><em>13 November 2023</em>. I have added most of the metrics that don’t need extra parameters for both sparse and non-sparse data, e.g. <code>braycurtis</code>, <code>dice</code>, <code>jaccard</code>, <code>hellinger</code> etc. See the <code>Missing Metrics</code> section at the end of this README for those which are not implemented. There are a few breaking changes (mainly around the hamming metric, see <code>NEWS.md</code> for the exact details).</p>
<p><em>06 November 2023</em> Sparse data support has been added. You should be able to use e.g. a <code>dgCMatrix</code> with all the methods and currently supported metrics as easily as a dense matrix.</p>
<p><em>30 October 2023</em> At last, a workable random partition forest implementation has been added. This can be used standalone (e.g. <code>rpf_knn</code>, <code>rpf_build</code>, <code>rpt_knn_query</code>) or as initialization to nearest neighbor descent (<code>nnd_knn(init = "tree", ...)</code>). The forest itself can be serialized with <code>saveRDS</code> but you will pay a price for that convenience by having to pass it back and forth from the R to C++ layer when querying. For now there is no access to the underlying C++ class via R like in RcppHNSW and RcppAnnoy so it may not be suitable for some use cases.</p>
<p><em>19 October 2023</em> Inevitably 0.0.11 is here because of a bug in 0.0.10 where nearest neighbor descent was not correctly flagging new/old neighbors which reduced performance (but not the actual result).</p>
<p><em>18 October 2023</em> A long-postponed major internal refactoring means I might be able to make a bit of progress on this package. For now, the <code>cosine</code> and <code>correlation</code> metrics have migrated to not preprocessing their data (these versions are still available as <code>cosine-preprocess</code> and <code>correlation-preprocess</code> respectively). Also, I have exported the distance metrics as R functions (e.g. <code>cosine_distance</code>, <code>euclidean_distance</code>).</p>
<p><em>18 September 2021</em> The <code>"hamming"</code> metric now supports integer-valued (not just binary) inputs, thanks to a contribution from <a href="https://github.com/vspinu" class="external-link">Vitalie Spinu</a>. The older metric code path for binary data only is supported via <code>metric = "bhamming"</code>.</p>
<p><em>20 June 2021</em> A big step forward in usefulness with the addition of the <code>prepare_search_graph</code> function which creates and prunes an undirected search graph from the neighbor graph for use with the (now re-named) <code>graph_knn_query</code> function. The latter is now also capable of backtracking search and performs fairly well.</p>
<p><em>4 October 2020</em> Added <code>"correlation"</code> as a metric and the <code>k_occur</code> function to help diagnose potential <a href="https://www.jmlr.org/papers/v11/radovanovic10a.html" class="external-link">hubness</a> in a dataset.</p>
<p><em>23 November 2019</em> Added <code>merge_knn</code> and <code>merge_knnl</code> for combining multiple nn results.</p>
<p><em>15 November 2019</em> It is now possible to query a reference set of data to produce the approximate knn graph relative to the references (i.e. none of the queries will be selected as neighbors) via <code>nnd_knn_query</code> (and related <code>brute_force</code> and <code>random</code> variants).</p>
<p><em>27 October 2019</em> <code>rnndescent</code> creeps towards usability. A multi-threaded implementation (using <a href="https://cran.r-project.org/package=RcppParallel" class="external-link">RcppParallel</a>) has now been added.</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/jlmelville/rnndescent/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/jlmelville/rnndescent/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3)</small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing rnndescent</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>James Melville <br><small class="roles"> Author, maintainer, copyright holder </small>  </li>
<li><a href="authors.html">More about authors...</a></li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/jlmelville/rnndescent/actions" class="external-link"><img src="https://github.com/jlmelville/rnndescent/workflows/R-CMD-check/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://ci.appveyor.com/project/jlmelville/rnndescent" class="external-link"><img src="https://ci.appveyor.com/api/projects/status/github/jlmelville/rnndescent?branch=master&amp;svg=true" alt="AppVeyor Build Status"></a></li>
<li><a href="https://codecov.io/github/jlmelville/rnndescent?branch=master" class="external-link"><img src="https://img.shields.io/codecov/c/github/jlmelville/rnndescent/master.svg" alt="Coverage Status"></a></li>
<li><a href="https://github.com/jlmelville/rnndescent" class="external-link"><img src="https://img.shields.io/github/last-commit/jlmelville/rnndescent" alt="Last Commit"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by James Melville.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
