<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content='The Nearest Neighbor Descent method (Dong and co-workers, 
    &lt;doi:10.1145/1963405.1963487&gt;) for finding approximate nearest
    neighbors. This is a translation of the Python implementation, "pynndescent"
    (&lt;https://github.com/lmcinnes/pynndescent&gt;).'>
<title>Nearest Neighbor Descent Method for Approximate Nearest Neighbors • rnndescent</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Nearest Neighbor Descent Method for Approximate Nearest Neighbors">
<meta property="og:description" content='The Nearest Neighbor Descent method (Dong and co-workers, 
    &lt;doi:10.1145/1963405.1963487&gt;) for finding approximate nearest
    neighbors. This is a translation of the Python implementation, "pynndescent"
    (&lt;https://github.com/lmcinnes/pynndescent&gt;).'>
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="index.html">rnndescent</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.12</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="articles/rnndescent.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="rnndescent">rnndescent<a class="anchor" aria-label="anchor" href="#rnndescent"></a>
</h1></div>
<!-- badges: start -->

<p>An R package implementing the Nearest Neighbor Descent method (<a href="https://doi.org/10.1145/1963405.1963487" class="external-link">Dong et al., 2011</a>) for finding approximate nearest neighbors, based on the Python library <a href="https://github.com/lmcinnes/pynndescent" class="external-link">PyNNDescent</a>.</p>
<p>Lightly in development, but can be used for optimizing an initial set of nearest neighbors, e.g. those generated by <a href="https://cran.r-project.org/package=RcppAnnoy" class="external-link">RcppAnnoy</a> or <a href="https://cran.r-project.org/package=RcppHNSW" class="external-link">RcppHNSW</a>, and can do a good job even on its own initialized from a random starting point.</p>
<div class="section level2">
<h2 id="current-status">Current Status<a class="anchor" aria-label="anchor" href="#current-status"></a>
</h2>
<p>Compared to pynndescent, the major missing features are:</p>
<ul>
<li>more dense distance functions</li>
<li>any sparse distance functions</li>
</ul>
<p><em>30 October 2023</em> At last, a workable random partition forest implementation has been added. This can be used standalone (e.g. <code>rpf_knn</code>, <code>rpf_build</code>, <code>rpt_knn_query</code>) or as initialization to nearest neighbor descent (<code>nnd_knn(init = "tree", ...)</code>). The forest itself can be serialized with <code>saveRDS</code> but you will pay a price for that convenience by having to pass it back and forth from the R to C++ layer when querying. For now there is no access to the underlying C++ class via R like in RcppHNSW and RcppAnnoy so it may not be suitable for some use cases.</p>
<p><em>19 October 2023</em> Inevitably 0.0.11 is here because of a bug in 0.0.10 where nearest neighbor descent was not correctly flagging new/old neighbors which reduced performance (but not the actual result).</p>
<p><em>18 October 2023</em> A long-postponed major internal refactoring means I might be able to make a bit of progress on this package. For now, the <code>cosine</code> and <code>correlation</code> metrics have migrated to not preprocessing their data (these versions are still available as <code>cosine-preprocess</code> and <code>correlation-preprocess</code> respectively). Also, I have exported the distance metrics as R functions (e.g. <code>cosine_distance</code>, <code>euclidean_distance</code>).</p>
<p><em>18 September 2021</em> The <code>"hamming"</code> metric now supports integer-valued (not just binary) inputs, thanks to a contribution from <a href="https://github.com/vspinu" class="external-link">Vitalie Spinu</a>. The older metric code path for binary data only is supported via <code>metric = "bhamming"</code>.</p>
<p><em>20 June 2021</em> A big step forward in usefulness with the addition of the <code>prepare_search_graph</code> function which creates and prunes an undirected search graph from the neighbor graph for use with the (now re-named) <code>graph_knn_query</code> function. The latter is now also capable of backtracking search and performs fairly well.</p>
<p><em>4 October 2020</em> Added <code>"correlation"</code> as a metric and the <code>k_occur</code> function to help diagnose potential <a href="https://www.jmlr.org/papers/v11/radovanovic10a.html" class="external-link">hubness</a> in a dataset.</p>
<p><em>23 November 2019</em> Added <code>merge_knn</code> and <code>merge_knnl</code> for combining multiple nn results.</p>
<p><em>15 November 2019</em> It is now possible to query a reference set of data to produce the approximate knn graph relative to the references (i.e. none of the queries will be selected as neighbors) via <code>nnd_knn_query</code> (and related <code>brute_force</code> and <code>random</code> variants).</p>
<p><em>27 October 2019</em> <code>rnndescent</code> creeps towards usability. A multi-threaded implementation (using <a href="https://cran.r-project.org/package=RcppParallel" class="external-link">RcppParallel</a>) has now been added.</p>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"jlmelville/rnndescent"</span><span class="op">)</span></span></code></pre></div>
<p>This packages makes use of C++ code which must be compiled. You may have to carry out a few extra steps before being able to build:</p>
<p><strong>Windows</strong>: install <a href="https://cran.r-project.org/bin/windows/Rtools/" class="external-link">Rtools</a> and ensure <code>C:\Rtools\bin</code> is on your path.</p>
<p><strong>Mac OS X</strong>: using a custom <code>~/.R/Makevars</code> <a href="https://github.com/jlmelville/uwot/issues/1" class="external-link">may cause linking errors</a>. This sort of thing is a potential problem on all platforms but seems to bite Mac owners more. <a href="https://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#Installation-of-source-packages" class="external-link">The R for Mac OS X FAQ</a> may be helpful here to work out what you can get away with. To be on the safe side, I would advise building without a custom <code>Makevars</code>.</p>
</div>
<div class="section level2">
<h2 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h2>
<p>Optimizing an initial set of approximate nearest neighbors:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jlmelville.github.io/rnndescent/">rnndescent</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># both hnsw_knn and nnd_knn will remove non-numeric columns from data-frames</span></span>
<span><span class="co"># for you, but to avoid confusion, these examples will use a matrix</span></span>
<span><span class="va">irism</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="op">-</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Generate a Random Projection knn (set n_threads for parallel search):</span></span>
<span><span class="va">iris_rp_nn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/rpf_knn.html">rpf_knn</a></span><span class="op">(</span><span class="va">irism</span>, k <span class="op">=</span> <span class="fl">15</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># nn descent improves results: set verbose = TRUE and progress = "dist", to </span></span>
<span><span class="co"># track distance sum progress over iterations</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_rp_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    progress <span class="op">=</span> <span class="st">"dist"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># search can be multi-threaded</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span></span>
<span>    <span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_rp_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># a (potentially) faster version of the algorithm is available that avoids some </span></span>
<span><span class="co"># repeated distance calculations at the cost of using more memory. Currently off</span></span>
<span><span class="co"># by default.</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span></span>
<span>    <span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_rp_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    low_memory <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span><span class="co"># You can optimize results from other methods or packages too:  </span></span>
<span><span class="co"># install.packages("RcppHNSW")</span></span>
<span><span class="co"># Use settings that don't get perfect results straight away</span></span>
<span><span class="va">iris_hnsw_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">RcppHNSW</span><span class="fu">::</span><span class="fu">hnsw_knn</span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>    M <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    distance <span class="op">=</span> <span class="st">"euclidean"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span></span>
<span>    <span class="va">irism</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_hnsw_nn</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    low_memory <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>You can also search the neighbor graph with new query items:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 100 reference iris items</span></span>
<span><span class="va">iris_ref</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"setosa"</span>, <span class="st">"versicolor"</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># 50 query items</span></span>
<span><span class="va">iris_query</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"versicolor"</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># First, find the approximate 10-nearest neighbor graph for the references:</span></span>
<span><span class="va">iris_ref_knn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris_ref</span>, k <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># For each item in iris_query find the 10 nearest neighbors in iris_ref</span></span>
<span><span class="co"># You need to pass both the reference data and the knn graph.</span></span>
<span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/graph_knn_query.html">graph_knn_query</a></span><span class="op">(</span></span>
<span>    query <span class="op">=</span> <span class="va">iris_query</span>,</span>
<span>    reference <span class="op">=</span> <span class="va">iris_ref</span>,</span>
<span>    reference_graph <span class="op">=</span> <span class="va">iris_ref_knn</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Although the above example shows the basic procedure of building the graph and then querying new data with it, the raw nearest neighbor graph isn’t very efficient in general. It’s highly advisable to use a refined search graph based on the original data and neighbor graph:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_search_graph</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/prepare_search_graph.html">prepare_search_graph</a></span><span class="op">(</span><span class="va">iris_ref</span>, <span class="va">iris_ref_knn</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/graph_knn_query.html">graph_knn_query</a></span><span class="op">(</span></span>
<span>    query <span class="op">=</span> <span class="va">iris_query</span>,</span>
<span>    reference <span class="op">=</span> <span class="va">iris_ref</span>,</span>
<span>    reference_graph <span class="op">=</span> <span class="va">iris_search_graph</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>See the help text to <code>prepare_search_graph</code> for various parameters you can change to control the trade off between speed and search accuracy.</p>
</div>
<div class="section level2">
<h2 id="initialization">Initialization<a class="anchor" aria-label="anchor" href="#initialization"></a>
</h2>
<p>The default for <code>nnd_knn</code> is to initialize with random neighbors. Set <code>init = "tree"</code> to use the random partition tree initialization.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jlmelville.github.io/rnndescent/">rnndescent</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">irism</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="op">-</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># picks indices at random and then carries out nearest neighbor descent</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>  k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>  n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># if you want the random indices and their distances:</span></span>
<span><span class="va">iris_rand_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/random_knn.html">random_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span><span class="co"># use RP tree initialization</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>  k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>  n_threads <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  init <span class="op">=</span> <span class="st">"tree"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>For initializing a knn query, there is also <code>random_knn_query</code>.</p>
</div>
<div class="section level2">
<h2 id="brute-force">Brute Force<a class="anchor" aria-label="anchor" href="#brute-force"></a>
</h2>
<p>For comparison with exact results, there is also <code>brute_force_knn</code> and <code>brute_force_knn_query</code> functions, that will generate the exact nearest neighbors by the simple process of trying every possible pair in the dataset. Obviously this becomes a very time consuming process as your dataset grows in size, even with multithreading (although the <code>iris</code> dataset in the example below doesn’t present any issues).</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_exact_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/brute_force_knn.html">brute_force_knn</a></span><span class="op">(</span><span class="va">irism</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span>,</span>
<span>    metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>    n_threads <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="merging">Merging<a class="anchor" aria-label="anchor" href="#merging"></a>
</h2>
<p>Also available are two functions for merging multiple approximate nearest neighbor graphs, which will result in a new graph which is at least as good as the best graph provided. For merging pairs of graphs, use <code>merge_knn</code>:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1337</span><span class="op">)</span></span>
<span><span class="co"># Nearest neighbor descent with 15 neighbors for iris three times,</span></span>
<span><span class="co"># starting from a different random initialization each time</span></span>
<span><span class="va">iris_rnn1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_rnn2</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Merged results should be an improvement over either individual results</span></span>
<span><span class="va">iris_mnn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/merge_knn.html">merge_knn</a></span><span class="op">(</span><span class="va">iris_rnn1</span>, <span class="va">iris_rnn2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_mnn</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_rnn1</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_mnn</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_rnn2</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span></code></pre></div>
<p>If you have more than two graphs stored in memory, it’s more efficient to use the list-based version, <code>merge_knnl</code>:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1337</span><span class="op">)</span></span>
<span><span class="co"># Nearest neighbor descent with 15 neighbors for iris three times,</span></span>
<span><span class="co"># starting from a different random initialization each time</span></span>
<span><span class="va">iris_rnn1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_rnn2</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_rnn3</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">iris_mnn</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/merge_knnl.html">merge_knnl</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">iris_rnn1</span>, <span class="va">iris_rnn2</span>, <span class="va">iris_rnn3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="hubness-diagnostic">Hubness Diagnostic<a class="anchor" aria-label="anchor" href="#hubness-diagnostic"></a>
</h2>
<p>The <code>k_occur</code> function takes an <code>idx</code> matrix and returns a vector of the k-occurrences for each item in the dataset. This is just the number of times an item was found in the k-nearest neighbor list of another item. If you think of the <code>idx</code> matrix as representing a directed graph where the element <code>idx[i, j]</code> in the matrix is an edge from node <code>i</code> to node <code>idx[i, j]</code>, then the k_occurrences are calculated by reversing each edge and then counts the number of edges incident to each node. Alternatively, in the nomenclature of nearest neighbor descent, it’s the size of the “reverse neighbor” list for each node.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_nnd</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span><span class="op">)</span></span>
<span><span class="va">kos</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/k_occur.html">k_occur</a></span><span class="op">(</span><span class="va">iris_nnd</span><span class="op">$</span><span class="va">idx</span><span class="op">)</span></span></code></pre></div>
<p>The k-occurrence can take a value between 0 and <code>N</code> the number of items in the dataset. Values much larger than <code>k</code> indicate that the item is potentially a hub. The presence of hubs in a dataset can reduce the accuracy of the approximate nearest neighbors returned by nearest neighbor descent, but the presence of hubs as determined by the distribution of k-occurrences is quite robust even in the case of an approximate nearest neighbor graph of low accuracy. Therefore calculating the k-occurrences on the output of nearest neighbor descent is worth doing: if the maximum k-occurrence is a lot larger than <code>k</code> (I suggest <code>10 * k</code> as a danger sign), then the accuracy of the approximate nearest neighbors may be compromised. Items with low k-occurrences are most likely to be affected in this way. Increasing <code>k</code> or the <code>max_candidates</code> parameter can help in these situations. Alternatively, querying the data against itself with <code>graph_knn_query</code> can help:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Purposely don't do a very good job with NND so we have something to improve</span></span>
<span><span class="va">iris_nnd</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/nnd_knn.html">nnd_knn</a></span><span class="op">(</span><span class="va">iris</span>, k <span class="op">=</span> <span class="fl">15</span>, n_iters <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris_search_graph</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/prepare_search_graph.html">prepare_search_graph</a></span><span class="op">(</span><span class="va">iris</span>, <span class="va">iris_nnd</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># query and reference are the same</span></span>
<span><span class="va">iris_query_nn</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="reference/graph_knn_query.html">graph_knn_query</a></span><span class="op">(</span></span>
<span>    query <span class="op">=</span> <span class="va">iris</span>,</span>
<span>    reference <span class="op">=</span> <span class="va">iris</span>,</span>
<span>    reference_graph <span class="op">=</span> <span class="va">iris_search_graph</span>,</span>
<span>    init <span class="op">=</span> <span class="va">iris_nnd</span>,</span>
<span>    k <span class="op">=</span> <span class="fl">15</span></span>
<span>  <span class="op">)</span></span>
<span><span class="co"># Compare </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_nnd</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">iris_query_nn</span><span class="op">$</span><span class="va">dist</span><span class="op">)</span></span></code></pre></div>
<p>For more on hubness and nearest neighbors, see for example <a href="https://www.jmlr.org/papers/v11/radovanovic10a.html" class="external-link">Radovanović and co-workers, 2010</a> and <a href="https://doi.org/10.1142/S0218213019600029" class="external-link">Bratić and co-workers, 2019</a>.</p>
</div>
<div class="section level2">
<h2 id="supported-metrics">Supported Metrics<a class="anchor" aria-label="anchor" href="#supported-metrics"></a>
</h2>
<p>Euclidean, Manhattan, Cosine, Correlation (1 - the Pearson correlation, implemented as cosine distance on row-centered data) and Hamming. Note that these have been implemented in a simple fashion, so no clever (but non-portable) optimizations using AVX/SSE or specialized <code>popcount</code> routines are used.</p>
</div>
<div class="section level2">
<h2 id="citation">Citation<a class="anchor" aria-label="anchor" href="#citation"></a>
</h2>
<p>Dong, W., Moses, C., &amp; Li, K. (2011, March). Efficient k-nearest neighbor graph construction for generic similarity measures. In <em>Proceedings of the 20th international conference on World wide web</em> (pp. 577-586). ACM. <a href="https://doi.org/10.1145/1963405.1963487" class="external-link">doi.org/10.1145/1963405.1963487</a>.</p>
</div>
<div class="section level2">
<h2 id="license">License<a class="anchor" aria-label="anchor" href="#license"></a>
</h2>
<p>The R package as a whole is licensed under <a href="https://www.gnu.org/licenses/gpl-3.0.txt" class="external-link">GPLv3 or later</a>. The following files are licensed differently:</p>
<ul>
<li>
<code>inst/include/dqsample.h</code> is a modification of some sampling code from <a href="https://github.com/daqana/dqrng" class="external-link">dqrng</a> and is <a href="https://www.gnu.org/licenses/agpl-3.0.en.html" class="external-link">AGPLv3 or later</a>.</li>
<li>
<code>inst/include/RcppPerpendicular.h</code> is a modification of some code from from <a href="https://github.com/RcppCore/RcppParallel" class="external-link">RcppParallel</a> and is <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html" class="external-link">GPLv2 or later</a>
</li>
<li>The underlying nearest neighbor descent C++ library, which can be found under <code>inst/include/tdoann</code>, is licensed under the <a href="https://opensource.org/licenses/BSD-2-Clause" class="external-link">BSD 2-Clause</a>.</li>
</ul>
<p>As far as I know, these licenses are all compatible with re-licensing under GPLv3 or later.</p>
</div>
<div class="section level2">
<h2 id="see-also">See Also<a class="anchor" aria-label="anchor" href="#see-also"></a>
</h2>
<ul>
<li>
<a href="https://github.com/lmcinnes/pynndescent" class="external-link">PyNNDescent</a>, the Python implementation.</li>
<li>
<a href="https://github.com/TatsuyaShirakawa/nndescent" class="external-link">nndescent</a>, a C++ implementation.</li>
<li>
<a href="https://github.com/dillondaudert/NearestNeighborDescent.jl" class="external-link">NearestNeighborDescent.jl</a>, a Julia implementation.</li>
<li>
<a href="https://github.com/eskomski/nn_descent" class="external-link">nn_descent</a>, a C implementation.</li>
<li>
<a href="https://github.com/AnabelSMRuggiero/NNDescent.cpp" class="external-link">NNDescent.cpp</a>, another C++ implementation.</li>
<li>
<a href="https://github.com/brj0/nndescent" class="external-link">nndescent</a>, another C++ implementation, with Python bindings.</li>
</ul>
</div>
</div>
  </main><aside class="col-md-3"><div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3)</small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing rnndescent</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>James Melville <br><small class="roles"> Author, maintainer </small>  </li>
<li><a href="authors.html">More about authors...</a></li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/jlmelville/rnndescent/actions" class="external-link"><img src="https://github.com/jlmelville/rnndescent/workflows/R-CMD-check/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://ci.appveyor.com/project/jlmelville/rnndescent" class="external-link"><img src="https://ci.appveyor.com/api/projects/status/github/jlmelville/rnndescent?branch=master&amp;svg=true" alt="AppVeyor Build Status"></a></li>
<li><a href="https://codecov.io/github/jlmelville/rnndescent?branch=master" class="external-link"><img src="https://img.shields.io/codecov/c/github/jlmelville/rnndescent/master.svg" alt="Coverage Status"></a></li>
<li><a href="https://github.com/jlmelville/rnndescent" class="external-link"><img src="https://img.shields.io/github/last-commit/jlmelville/rnndescent" alt="Last Commit"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by James Melville.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
