% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rnndescent.R
\name{nnd_knn}
\alias{nnd_knn}
\title{Find Nearest Neighbors and Distances}
\usage{
nnd_knn(data, k = NULL, metric = "euclidean", init = NULL,
  n_iters = 10, max_candidates = 20, delta = 0.001, use_cpp = TRUE,
  low_memory = TRUE, fast_rand = FALSE, n_threads = 0,
  block_size = 16384, grain_size = 1, verbose = FALSE)
}
\arguments{
\item{data}{Matrix of \code{n} items to search.}

\item{k}{Number of nearest neighbors to return. Optional if \code{init} is
specified.}

\item{metric}{Type of distance calculation to use. One of \code{"euclidean"},
\code{"l2"} (squared Euclidean), \code{"cosine"}, \code{"manhattan"}
or \code{"hamming"}.}

\item{init}{Initial data to optimize. If not provided, \code{k} random
  neighbors are created. The input format should be the same as the return
  value: a list containing:
\itemize{
  \item \code{idx} an n by k matrix containing the nearest neighbor indices.
  \item \code{dist} an n by k matrix containing the nearest neighbor
  distances.
}
If \code{k} and \code{init} are provided then \code{k} must be equal to or
smaller than the number of neighbors provided in \code{init}. If smaller,
only the \code{k} closest value in \code{init} are retained.}

\item{n_iters}{Number of iterations of nearest neighbor descent to carry out.}

\item{max_candidates}{Maximum number of candidate neighbors to try for each
item in each iteration. Use relative to \code{k} to emulate the "rho"
sampling parameter in the nearest neighbor descent paper.}

\item{delta}{precision parameter. Routine will terminate early if
fewer than \eqn{\delta k N}{delta x k x n} updates are made to the nearest
neighbor list in a given iteration.}

\item{use_cpp}{If \code{TRUE}, use the faster C++ code path.}

\item{low_memory}{If \code{TRUE}, use a lower memory, but more
computationally expensive approach to index construction. Applies only if
\code{use_cpp = TRUE}. If set to \code{FALSE}, you should see a noticeable
speed improvement, especially when using a smaller number of threads, so
this is worth trying if you have the memory to spare.}

\item{fast_rand}{If \code{TRUE}, use a faster random number generator than
the R PRNG. Probably acceptable for the needs of the NN descent algorithm.
Applies only if \code{use_cpp = TRUE}.}

\item{n_threads}{Number of threads to use.}

\item{block_size}{Batch size for creating/applying local join updates. A
smaller value will apply the update more often, which may help reduce the
number of unnecessary distance calculations, at the cost of more overhead
associated with multi-threading code. Ignored if \code{n_threads < 1}.}

\item{grain_size}{Minimum batch size for multithreading. If the number of
items to process in a thread falls below this number, then no threads will
be used. Ignored if \code{n_threads < 1}.}

\item{verbose}{If \code{TRUE}, log information to the console.}
}
\value{
a list containing:
\itemize{
  \item \code{idx} an n by k matrix containing the nearest neighbor indices.
  \item \code{dist} an n by k matrix containing the nearest neighbor
   distances.
}
}
\description{
Find Nearest Neighbors and Distances
}
\examples{
# Find 4 (approximate) nearest neighbors using Euclidean distance
# If you pass a data frame, non-numeric columns are removed
iris_nn <- nnd_knn(iris, k = 4, metric = "euclidean")

# Manhattan (l1) distance
iris_nn <- nnd_knn(iris, k = 4, metric = "manhattan")

# Multi-threading: you can choose the number of threads to use: in real
# usage, you will want to set n_threads to at least 2
iris_nn <- nnd_knn(iris, k = 4, metric = "manhattan", n_threads = 1)

# Use verbose flag to see information about progress
iris_nn <- nnd_knn(iris, k = 4, metric = "euclidean", verbose = TRUE)

# Nearest neighbor descent uses random initialization, but you can pass any
# approximation using the init argument (as long as the metrics used to
# calculate the initialization are compatible with the metric options used
# by nnd_knn).
iris_nn <- random_knn(iris, k = 4, metric = "euclidean")
iris_nn <- nnd_knn(iris, init = iris_nn, metric = "euclidean", verbose = TRUE)

# Number of iterations controls how much optimization is attempted. A smaller
# value will run faster but give poorer results
iris_nn <- nnd_knn(iris, k = 4, metric = "euclidean", n_iters = 2)

# You can also control the amount of work done within an iteration by
# setting max_candidates
iris_nn <- nnd_knn(iris, k = 4, metric = "euclidean", max_candidates = 50)

# Optimization may also stop early if not much progress is being made. This
# convergence criterion can be controlled via delta. A larger value will
# stop progress earlier. The verbose flag will provide some information if
# convergence is occurring before all iterations are carried out.
set.seed(1337)
iris_nn <- nnd_knn(iris, k = 4, metric = "euclidean", n_iters = 5, delta = 0.5)

# To ensure that descent only stops if no improvements are made, set delta = 0
set.seed(1337)
iris_nn <- nnd_knn(iris, k = 4, metric = "euclidean", n_iters = 5, delta = 0)

# A faster version of the algorithm is available that avoids repeated
# distance calculations at the cost of using more RAM. Set low_memory to
# FALSE to try it.
set.seed(1337)
iris_nn <- nnd_knn(iris, k = 4, metric = "euclidean", low_memory = FALSE)
}
\references{
Dong, W., Moses, C., & Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In \emph{Proceedings of the 20th international conference on World Wide Web}
(pp. 577-586).
ACM.
\url{doi.org/10.1145/1963405.1963487}.
}
