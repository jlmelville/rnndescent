% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rnndescent.R
\name{prepare_search_graph}
\alias{prepare_search_graph}
\title{Nearest Neighbor Graph Refinement}
\usage{
prepare_search_graph(
  data,
  graph,
  metric = "euclidean",
  prune_probability = 1,
  pruning_degree_multiplier = 1.5,
  verbose = FALSE
)
}
\arguments{
\item{data}{Matrix of \code{n} items.}

\item{graph}{neighbor graph for \code{data}, a list containing:
\itemize{
\item \code{idx} an \code{n} by \code{k} matrix containing the nearest neighbor indices of
the data in \code{data}.
\item \code{dist} an \code{n} by \code{k} matrix containing the nearest neighbor distances.
}}

\item{metric}{Type of distance calculation to use. One of \code{"euclidean"},
\code{"l2sqr"} (squared Euclidean), \code{"cosine"}, \code{"manhattan"}, \code{"correlation"}
(1 minus the Pearson correlation), or \code{"hamming"}.}

\item{prune_probability}{Probability of a neighbor being removed if it is
found to be an "occlusion". A neighbor \code{p} of item \code{i} is an "occlusion" if
the distance between \code{p} and one of the other neighbors \code{q} is smaller than
the distance between \code{i} and \code{q}, i.e. it is likely that \code{q} will be in the
neighbor list of \code{p} so there is no need to retain it in the neighbor list
of \code{i}. Set to \code{NULL} to skip any occlusion pruning. Note that occlusion
pruning is carried out twice, one to the forward neighbors, and once to the
reverse neighbors.}

\item{pruning_degree_multiplier}{How strongly to truncate the final neighbor
list for each item. The neighbor list of each item will be truncated to
retain only the closest \code{d} neighbors, where
\code{d = k * pruning_degree_multiplier}, and \code{k} is the
original number of neighbors per item in \code{graph}. Roughly, values
larger than \code{1} will keep all the nearest neighbors of an item, plus
the given fraction of reverse neighbors (if they exist). For example,
setting this to \code{1.5} will keep all the forward neighbors and then
half as many of the reverse neighbors, although exactly which neighbors are
retained is also dependent on any occlusion pruning that occurs. Set this
to \code{NULL} to skip this step.}

\item{verbose}{If \code{TRUE}, log information to the console.}
}
\value{
a search graph for \code{data} based on \code{graph}, represented as a sparse
matrix, suitable for use with \code{\link[=graph_knn_query]{graph_knn_query()}}.
}
\description{
Create a graph using existing nearest neighbor data to balance search
speed and accuracy using the occlusion pruning and truncation strategies
of Harwood and Drummond (2016).
}
\details{
An approximate nearest neighbor graph is not very useful for querying via
\code{\link[=graph_knn_query]{graph_knn_query()}}, especially if the query data is initialized randomly:
some items in the data set may not be in the nearest neighbor list of any
other item and can therefore never be returned as a neighbor, no matter how
close they are to the query. Even those which do appear in at least one
neighbor list may not be reachable by expanding an arbitrary starting list if
the neighbor graph contains disconnected components.

Converting the directed graph represented by the neighbor graph to an
undirected graph by adding an edge from item \code{j} to \code{i} if
an edge exists from \code{i} to \code{j} (i.e. creating the mutual neighbor
graph) solves the problems above, but can result in inefficient searches.
Although the out-degree of each item is restricted to the number of neighbors
the in-degree has no such restrictions: a given item could be very "popular"
and in a large number of neighbors lists. Therefore mutualizing the neighbor
graph can result in some items with a large number of neighbors to search.
These usually have very similar neighborhoods so there is nothing to be
gained from searching all of them.

To balance accuracy and search time, the following procedure is carried out:
\enumerate{
\item The graph is "diversified" by occlusion pruning.
\item The reverse graph is formed by reversing the direction of all edges in
the pruned graph.
\item The reverse graph is diversified by occlusion pruning.
\item The pruned forward and pruned reverse graph are merged.
\item The outdegree of each node in the merged graph is truncated.
\item The truncated merged graph is returned as the prepared search graph.
}
}
\examples{
# 100 reference iris items
iris_ref <- iris[iris$Species \%in\% c("setosa", "versicolor"), ]

# 50 query items
iris_query <- iris[iris$Species == "versicolor", ]

# First, find the approximate 4-nearest neighbor graph for the references:
ref_ann_graph <- nnd_knn(iris_ref, k = 4)

# Create a graph for querying with
ref_search_graph <- prepare_search_graph(iris_ref, ref_ann_graph)

# Using the search graph rather than the ref_ann_graph directly may give
# more accurate or faster results
iris_query_nn <- graph_knn_query(
  query = iris_query, reference = iris_ref,
  reference_graph = ref_search_graph, k = 4, metric = "euclidean",
  verbose = TRUE
)
}
\references{
Harwood, B., & Drummond, T. (2016).
Fanng: Fast approximate nearest neighbour graphs.
In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}
(pp. 5713-5722).
}
