% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rnndescent.R
\name{nnd_knn_query}
\alias{nnd_knn_query}
\title{Find Nearest Neighbors and Distances}
\usage{
nnd_knn_query(
  reference,
  reference_idx,
  query,
  k = NULL,
  metric = "euclidean",
  init = NULL,
  n_iters = 10,
  max_candidates = 20,
  delta = 0.001,
  low_memory = TRUE,
  use_alt_metric = TRUE,
  n_threads = 0,
  block_size = 16384,
  grain_size = 1,
  verbose = FALSE,
  progress = "bar"
)
}
\arguments{
\item{reference}{Matrix of \code{m} reference items. The nearest neighbors to the
queries are calculated from this data.}

\item{reference_idx}{Matrix of \code{m} by \code{k} integer indices
representing the (possibly approximate) \code{k}-nearest neighbors graph
of the \code{reference} data.}

\item{query}{Matrix of \code{n} query items.}

\item{k}{Number of nearest neighbors to return. Optional if \code{init} is
specified.}

\item{metric}{Type of distance calculation to use. One of \code{"euclidean"},
\code{"l2sqr"} (squared Euclidean), \code{"cosine"}, \code{"manhattan"}
or \code{"hamming"}.}

\item{init}{Initial data to optimize. If not provided, \code{k} random
  neighbors are created. The input format should be the same as the return
  value: a list containing:
\itemize{
  \item \code{idx} an n by k matrix containing the nearest neighbor indices
  of the data in \code{reference}.
  \item \code{dist} an n by k matrix containing the nearest neighbor
  distances.
}
If \code{k} and \code{init} are provided then \code{k} must be equal to or
smaller than the number of neighbors provided in \code{init}. If smaller,
only the \code{k} closest value in \code{init} are retained. The input
distances may be ignored if \code{use_alt_metric = TRUE} and no
transformation from the distances to the internal distance representation
is available. In this case, the distances will be recalculated internally
and only the contents of \code{init$idx} will be used.}

\item{n_iters}{Number of iterations of nearest neighbor descent to carry out.}

\item{max_candidates}{Maximum number of candidate neighbors to try for each
item in each iteration. Use relative to \code{k} to emulate the "rho"
sampling parameter in the nearest neighbor descent paper.}

\item{delta}{precision parameter. Routine will terminate early if
fewer than \eqn{\delta k N}{delta x k x n} updates are made to the nearest
neighbor list in a given iteration.}

\item{low_memory}{If \code{TRUE}, use a lower memory, but more
computationally expensive approach to index construction. If set to
\code{FALSE}, you should see a noticeable speed improvement, especially
when using a smaller number of threads, so this is worth trying if you
have the memory to spare.}

\item{use_alt_metric}{If \code{TRUE}, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
\code{metric = "euclidean"}), then apply a correction at the end. Probably
the only reason to set this to \code{FALSE} is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.}

\item{n_threads}{Number of threads to use.}

\item{block_size}{Batch size for creating/applying local join updates. A
smaller value will apply the update more often, which may help reduce the
number of unnecessary distance calculations, at the cost of more overhead
associated with multi-threading code. Ignored if \code{n_threads < 1}.}

\item{grain_size}{Minimum batch size for multithreading. If the number of
items to process in a thread falls below this number, then no threads will
be used. Ignored if \code{n_threads < 1}.}

\item{verbose}{If \code{TRUE}, log information to the console.}

\item{progress}{Determines the type of progress information logged if
\code{verbose = TRUE}. Options are:
\itemize{
  \item \code{"bar"}: a simple text progress bar.
  \item \code{"dist"}: the sum of the distances in the approximate knn
  graph at the end of each iteration.
}}
}
\value{
a list containing:
\itemize{
  \item \code{idx} an n by k matrix containing the nearest neighbor indices.
  \item \code{dist} an n by k matrix containing the nearest neighbor
   distances.
}
}
\description{
Find Nearest Neighbors and Distances
}
\examples{
# 100 reference iris items
iris_ref <- iris[iris$Species \%in\% c("setosa", "versicolor"), ]

# 50 query items
iris_query <- iris[iris$Species == "versicolor", ]

# First, find the approximate 4-nearest neighbor graph for the references:
iris_ref_knn <- nnd_knn(iris_ref, k = 4)

# For each item in iris_query find the 4 nearest neighbors in iris_ref.
# You need to pass both the reference data and the knn graph indices (the
# 'idx' matrix in the return value of nnd_knn).
# If you pass a data frame, non-numeric columns are removed.
# set verbose = TRUE to get details on the progress being made
iris_query_nn <- nnd_knn_query(iris_ref, iris_ref_knn$idx, iris_query,
  k = 4, metric = "euclidean",
  verbose = TRUE
)
}
\references{
Dong, W., Moses, C., & Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In \emph{Proceedings of the 20th international conference on World Wide Web}
(pp. 577-586).
ACM.
\url{doi.org/10.1145/1963405.1963487}.
}
