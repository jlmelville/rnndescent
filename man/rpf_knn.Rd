% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rptree.R
\name{rpf_knn}
\alias{rpf_knn}
\title{Find Nearest Neighbors and Distances Using A Random Projection Forest}
\usage{
rpf_knn(
  data,
  k,
  metric = "euclidean",
  use_alt_metric = TRUE,
  n_trees = NULL,
  leaf_size = NULL,
  include_self = TRUE,
  n_threads = 0,
  verbose = FALSE,
  obs = "R"
)
}
\arguments{
\item{data}{Matrix of \code{n} items to generate neighbors for, with observations
in the rows and features in the columns. Optionally, input can be passed
with observations in the columns, by setting \code{obs = "C"}, which should be
more efficient.}

\item{k}{Number of nearest neighbors to return. Optional if \code{init} is
specified.}

\item{metric}{Type of distance calculation to use. One of:
\itemize{
\item \code{"euclidean"}.
\item \code{"l2sqr"} (squared Euclidean).
\item \code{"cosine"}.
\item \code{"cosine-preprocess"}: cosine with preprocessing: this trades memory for a
potential speed up during the distance calculation.It should give the
same results as \code{cosine}, give or take minor numerical changes. Be aware
that the distance between two identical items may not always give exactly
zero with this method.
\item \code{"manhattan"}.
\item \code{"correlation"} (1 minus the Pearson correlation).
\item \code{"correlation-preprocess"}: \code{correlation} with preprocessing. This trades
memory for a potential speed up during the distance calculation. It should
give the same results as \code{correlation}, give or take minor numerical
changes. Be aware that the distance between two identical items may not
always give exactly zero with this method.
\item \code{"hamming"}.
\item \code{"bhamming"} (hamming on binary data with bitset internal memory
optimization).
}}

\item{use_alt_metric}{If \code{TRUE}, use faster metrics that maintain the
ordering of distances internally (e.g. squared Euclidean distances if using
\code{metric = "euclidean"}), then apply a correction at the end. Probably
the only reason to set this to \code{FALSE} is if you suspect that some
sort of numeric issue is occurring with your data in the alternative code
path.}

\item{n_trees}{The number of trees to use in the RP forest. A larger number
will give more accurate results at the cost of a longer computation time.
The default of \code{NULL} means that the number is chosen based on the number
of observations in \code{data}.}

\item{leaf_size}{The maximum number of items that can appear in a leaf. The
default of \code{NULL} means that the number of leaves is chosen based on the
number of requested neighbors \code{k}.}

\item{include_self}{If \code{TRUE} (the default) then an item is considered to
be a neighbor of itself. Hence the first nearest neighbor in the results
will be the item itself. This is a convention that many nearest neighbor
methods and software adopt, so if you want to use the resulting knn graph
from this function in downstream applications or compare with other
methods, you should probably keep this set to \code{TRUE}. However, if you are
planning on using the result of this as initialization to another nearest
neighbor method (e.g. \code{\link[=nnd_knn]{nnd_knn()}}), then set this to \code{FALSE}.}

\item{n_threads}{Number of threads to use.}

\item{verbose}{If \code{TRUE}, log information to the console.}

\item{obs}{set to \code{"C"} to indicate that the input \code{data} orientation stores
each observation as a column. The default \code{"R"} means that observations are
stored in each row. Storing the data by row is usually more convenient, but
internally your data will be converted to column storage. Passing it
already column-oriented will save some memory and (a small amount of) CPU
usage.}
}
\value{
the approximate nearest neighbor graph as a list containing:
\itemize{
\item \code{idx} an n by k matrix containing the nearest neighbor indices.
\item \code{dist} an n by k matrix containing the nearest neighbor distances.
}

\code{k} neighbors per observation are not guaranteed to be found. Missing data
is represented with an index of \code{0} and a distance of \code{NA}.
}
\description{
Find approximate nearest neighbors using a "forest" of Random Projection
Trees (Dasgupta and Freund, 2008).
}
\examples{
# Find 4 (approximate) nearest neighbors using Euclidean distance
# If you pass a data frame, non-numeric columns are removed
iris_nn <- rpf_knn(iris, k = 4, metric = "euclidean", leaf_size = 3)

# If you want to initialize another method (e.g. nearest neighbor descent)
# with the result of the RP forest, then it's more efficient to skip
# evaluating whether an item is a neighbor of itself by setting
# `include_self = FALSE`:
iris_rp <- rpf_knn(iris, k = 4, n_trees = 3, include_self = FALSE)
# Use it with e.g. `nnd_knn` -- this should be better than a random start
iris_nnd <- nnd_knn(iris, k = 4, init = iris_rp)

}
\references{
Dasgupta, S., & Freund, Y. (2008, May).
Random projection trees and low dimensional manifolds.
In \emph{Proceedings of the fortieth annual ACM symposium on Theory of computing}
(pp. 537-546).
\url{https://doi.org/10.1145/1374376.1374452}.
}
