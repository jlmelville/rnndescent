[{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) 2019 James Melville  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. rnndescent Copyright (C) 2019 James Melville This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"this-is-not-a-vignette","dir":"Articles","previous_headings":"","what":"This is not a Vignette","title":"Fashion MNIST Example","text":"article doesn’t contain “live” documentation paste output. install packages github also brute force search user-friendly, run code know letting .","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"how-slow-is-this-going-to-be","dir":"Articles","previous_headings":"This is not a Vignette","what":"How Slow Is This Going to Be?","title":"Fashion MNIST Example","text":"8th gen Intel i7 laptop six cores, slowest part brute force search takes 10 minutes. reasonably modern machine shouldn’t tying computer hours end. made particular efforts super accurate timings. ran approximate nearest neighbor methods several times reporting nearest second seems fine. also shut programs computer running, also didn’t actively carry activities machine either.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Fashion MNIST Example","text":"packages need installing. get Fashion MNIST dataset, use snedata package CRAN. Therefore turn need install via another package, e.g.  devtools case pak: installing using uwot, want dimensionality reduction, want re-use convenient functions make running code lot easier (especially RcppAnnoy). internal functions, fortunately close personal friends uwot maintainer know won’t going away time soon (yet another reason isn’t vignette).","code":"install.packages(\"pak\") pak::pkg_install(\"jlmelville/snedata\") install.packages(c(\"RcppHNSW\", \"RcppAnnoy\", \"uwot\")) library(rnndescent) library(RcppHNSW) library(RcppAnnoy) library(uwot) library(snedata)"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"data-setup","dir":"Articles","previous_headings":"","what":"Data Setup","title":"Fashion MNIST Example","text":"Download Fashion MNIST data. format famous MNIST data, instead images handwritten images, ’s images different types clothing. typical split first 60,000 images training set, rest test set.","code":"fmnist <- snedata::download_fashion_mnist() fmnist_train <- head(fmnist, 60000) fmnist_test <- tail(fmnist, 10000)"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"k-Nearest Neighbors","title":"Fashion MNIST Example","text":"look finding k = 15 neighbors data point, ballpark evaluations, also default setting number neighbors find UMAP.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"exact-k-nearest-neighbors","dir":"Articles","previous_headings":"k-Nearest Neighbors","what":"Exact k-Nearest Neighbors","title":"Fashion MNIST Example","text":"see much faster approximate nearest neighbors approaches cost accuracy, need generate exact nearest neighbors: took nearly ten minutes get done. Plenty scope approximate nearest neighbor methods lot faster. also use neighbor_overlap compare overlap approximate nearest neighbor indices exact results.","code":"fmnist_train_bf <- brute_force_knn(   fmnist_train,   k = 15,   n_threads = 6, )"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"rnndescent","dir":"Articles","previous_headings":"k-Nearest Neighbors","what":"rnndescent","title":"Fashion MNIST Example","text":"methods can find k-nearest neighbors building index querying index original data, rnndescent can extract k-nearest neighbors directly index without separate query step. query step produce accurate result, ’s always worth trying initial knn graph. took 11 seconds. accurate results? Basically 99% accuracy. ’s pretty good, say . Now onto HNSW Annoy need build indices query separately (note use similar technique extracting knn graph index directly ’s currently available ).","code":"fmnist_train_rnnd <- rnnd_knn(fmnist_train, k = 15, n_threads = 6, verbose = TRUE) neighbor_overlap(fmnist_train_rnnd, fmnist_train_bf) 0.9871422"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"hnsw","dir":"Articles","previous_headings":"k-Nearest Neighbors","what":"HNSW","title":"Fashion MNIST Example","text":"HNSW functions take matrices, dataframes. x2m internal uwot function converts us. building HNSW parameters M = 16 ef_construction = 200. Index building took 17 seconds. searching, default search parameter ef = 15. impressive 3 seconds index searching. HNSW delivers 97% accuracy 20 seconds .","code":"hnsw_index <- hnsw_build(uwot:::x2m(fmnist_train), n_threads = 6) fmnist_train_hnsw <- hnsw_search(uwot:::x2m(fmnist_train), hnsw_index, k = 15, n_threads = 6) neighbor_overlap(fmnist_train_hnsw, fmnist_train_bf) [1] 0.9748644"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"annoy","dir":"Articles","previous_headings":"k-Nearest Neighbors","what":"Annoy","title":"Fashion MNIST Example","text":"RcppAnnoy rather bare-bones terms higher level functions searching batch data. Fortunately, uwot uses Annoy internally nearest neighbor search, can make use functions (need convert data matrix). ’m using uwot settings index search parameters. use n_trees = 50 build: Annoy index build took 24 seconds. search parameter search_k set 2 * k * n_trees: search took around 16 seconds. similar accuracy, Annoy bit slower HNSW rnndescent bit hobbled multi-threaded query search implemented uwot, index written disk C++ thread can read copy memory. Also, balance n_trees search_k may bit heavily skewed favor tree building. probably build fewer trees (slow) spend (relatively) time searching: Build time 15 seconds. Search time terribly affected: still takes 16 seconds. accuracy? can get time around 31 seconds without overly affecting accuracy. ’ll use smaller index next section .","code":"annoy_index <-   uwot:::annoy_build(uwot:::x2m(fmnist_train),     metric = \"euclidean\",     n_trees = 50   ) fmnist_train_annoy <-   uwot:::annoy_search(     uwot:::x2m(fmnist_train),     k = 15,     ann = annoy_index,     search_k = 2 * 15 * 50,     tmpdir = tempdir(),     n_threads = 6   ) neighbor_overlap(fmnist_train_annoy, fmnist_train_bf) [1] 0.9590367 annoy_index <-   uwot:::annoy_build(uwot:::x2m(fmnist_train),     metric = \"euclidean\",     n_trees = 30   ) fmnist_train_annoy <-   uwot:::annoy_search(     uwot:::x2m(fmnist_train),     k = 15,     ann = annoy_index,     search_k = 2 * 15 * 50,     tmpdir = tempdir(),     n_threads = 6   ) neighbor_overlap(fmnist_train_annoy, fmnist_train_bf) [1] 0.9502022"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"neighbor-search","dir":"Articles","previous_headings":"","what":"Neighbor Search","title":"Fashion MNIST Example","text":"Separate k-nearest neighbor creation querying existing search index new data. searching indexes already created FMNIST test data.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"exact-nearest-neighbors","dir":"Articles","previous_headings":"Neighbor Search","what":"Exact Nearest Neighbors","title":"Fashion MNIST Example","text":"need ground truth exact neighbors: took 90 seconds: much less data, brute force search reasonable. still like approximate methods lot faster .","code":"fmnist_test_bf <-   brute_force_knn_query(     query = fmnist_test,     reference = fmnist_train,     k = 15,     n_threads = 6,   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"rnndescent-1","dir":"Articles","previous_headings":"Neighbor Search","what":"rnndescent","title":"Fashion MNIST Example","text":"avoided build index rnndescent k-nearest neighbor tasks. Now must. Building index takes around 17 seconds. Like Annoy can probably afford building far fewer search trees, can save seconds index building without affecting downstream accuracy don’t need worry now. 15 neighbors, querying took 2 seconds. get 96% accuracy test set 19 seconds.","code":"rnnd_index <-   rnnd_build(     fmnist_train,     k = 15,     prepare = TRUE,     n_threads = 6   ) fmnist_test_rnnd <-   rnnd_query(     index = rnnd_index,     query = fmnist_test,     k = 15,     n_threads = 6   ) neighbor_overlap(fmnist_test_rnnd, fmnist_test_bf) [1] 0.95952"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"hnsw-1","dir":"Articles","previous_headings":"Neighbor Search","what":"HNSW","title":"Fashion MNIST Example","text":"can make use pre-existing HNSW index built looking k-nearest neighbor graph, need query data. Index searching fast. round 1 second, like 0.65 seconds. include index build time, HNSW delivers 96% accuracy 17 seconds .","code":"fmnist_test_hnsw <-   hnsw_search(uwot:::x2m(fmnist_test),     hnsw_index,     k = 15,     n_threads = 6   ) neighbor_overlap(fmnist_test_hnsw, fmnist_test_bf) [1] 0.9551133"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"annoy-1","dir":"Articles","previous_headings":"Neighbor Search","what":"Annoy","title":"Fashion MNIST Example","text":"Like HSNW, re-use Annoy index k-nearest neighbors graph building. one set n_trees = 30. search took around 3 seconds. 95% accuracy total time (including index building) 18 seconds.","code":"fmnist_test_annoy <-   uwot:::annoy_search(     uwot:::x2m(fmnist_test),     k = 15,     ann = annoy_index,     search_k = 3 * 15 * 30,     tmpdir = tempdir(),     n_threads = 6   ) neighbor_overlap(fmnist_test_annoy, fmnist_test_bf) [1] 0.9458733"},{"path":"https://jlmelville.github.io/rnndescent/articles/fmnist-example.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Fashion MNIST Example","text":"real world setting, approximate nearest neighbor methods tested perform comparably. rnndescent can produce k-nearest neighbor graph little bit faster two methods, querying scenario ’s pretty much . big caveat optimal hyperparameters really want tweaking anyway: can probably just brute force search done unless think going find set parameters using several index building querying tasks.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"comparing-low--and-high-dimensional-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Comparing Low- and High-Dimensional Nearest Neighbors","title":"Hubness","text":"Let’s look distribution nearest neighbor distances high low dimensions (easier comparison, normalized respect largest distance)   Compared low dimensional data, can see high dimensional distances distributed around higher distance well symmetric distribution. distribution neighbor distances high-dimensional case neighbors found NND:  Pretty much indistinguishable exact results, seems like isn’t obvious diagnostic distances . distribution errors NND results uniform items neighborhoods noticeably better predicted others? function calculate vector relative RMS error two sets neighbors terms distances: don’t include first nearest neighbor distances, invariably self distances leads uninteresting number zero error results. measure error bit less strict neighbor_overlap neighbor outside true kNN, comparable distance, penalized less harshly distant point. ’s histogram RRMS distance errors:  None relative errors actually large, care value kth nearest neighbor distances, even 1000D case, still get decent results case. can also see clear distribution errors, appreciable number items zero RRMS distance errors, items largest error. can also get overlap per-item basis. set ret_vec = TRUE, neighbor_overlap now return list individual overlaps overlaps vector (overall mean average overlap returned mean item). histograms accuracies:  shows similar pattern: items high accuracy noticeably worse accuracy average. completeness, relationship accuracy RRMSE:  Nothing surprising : ’s fairly consistent spread RRMSE given accuracy. whether use error distance accuracy measure well approximate nearest neighbors method working, least case, high dimensional dataset affects items others.","code":"hist(g2d_nnbf$dist[, -1] / max(g2d_nnbf$dist[, -1]), xlab = \"distances\", main = \"2D 15-NN\") hist(g1000d_nnbf$dist[, -1] / max(g1000d_nnbf$dist[, -1]), xlab = \"distances\", main = \"1000D 15-NN\") hist(g1000d_nnd$dist[, -1] / max(g1000d_nnd$dist[, -1]),   xlab = \"distances\",   main = \"1000D 15-NND\" ) nn_rrmsev <- function(nn, ref) {   n <- ncol(ref$dist) - 1   sqrt(apply((nn$dist[, -1] - ref$dist[, -1])^2 / n, 1, sum) /     apply(ref$dist[, -1]^2, 1, sum)) } g1000d_rrmse <- nn_rrmsev(g1000d_nnd, g1000d_nnbf) hist(g1000d_rrmse,   main = \"1000D distance error\",   xlab = \"Relative RMS error\" ) g1000d_nnd_acc <-   neighbor_overlap(g1000d_nnbf, g1000d_nnd, k = 15, ret_vec = TRUE)$overlaps hist(g1000d_nnd_acc,   main = \"1000D accuracy\",   xlab = \"accuracy\",   xlim = c(0, 1) ) plot(   g1000d_nnd_acc,   g1000d_rrmse,   main = \"RRMSE vs accuracy\",   xlab = \"accuracy\",   ylab = \"RRMSE\" )"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"detecting-hubness","dir":"Articles","previous_headings":"","what":"Detecting Hubness","title":"Hubness","text":"(Radovanovic, Nanopoulos, Ivanovic 2010) discusses technique detecting hubness: look items appear frequently k-nearest neighbor graph. k_occur function counts “k-occurrences” item dataset, .e. count number times item appears k-nearest neighbor graph. can also see reversing direction edges k-nearest neighbor graph counting -degree item. distribution neighbors entirely uniform expect see item appear \\(k\\) times. hubs k-occurrence get large size dataset, \\(N\\). item appears neighbor graph fewer \\(k\\) times termed “antihub”. definition neighbor item always includes item , expect minimum \\(k\\)-occurrence \\(1\\). Also, always \\(Nk\\) edges \\(k\\)-nearest neighbor graph, item appears expected amount implies items must -represented. Practically speaking, always going items larger \\(k\\)-occurrence expected hence lower \\(k\\)-occurrence, hubness anti-hubness case deciding cut-presence item lot neighbors starts causing problems, going dependent planning neighbor graph (probably number neighbors want).","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"k-occurrence-in-the-2d-case","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence in the 2D case","title":"Hubness","text":"First, let’s look 2D case using exact k-nearest neighbors: mean average \\(k\\)-occurrence never helpful: noted always \\(Nk\\) edges neighbor graph, mean \\(k\\)-occurrence always \\(k\\). However descriptions distribution informative. median \\(k\\)-occurrence also 15, good sign, values 25% 75% aren’t different . maximum \\(k\\)-occurrence less \\(2k\\). minimum value 1 means anti-hubs dataset, : one anti-hub dataset. ’s histogram k-occurrences:  unremarkable-looking distribution visual indication dataset without lot hubness anti-hubs lurking cause problems nearest neighbor descent.","code":"g2d_bfko <- k_occur(g2d_nnbf, k = 15) summary(g2d_bfko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1      13      15      15      17      24 sum(g2d_bfko == 1) #> [1] 1 hist(g2d_bfko, main = \"2D 15-NN\", xlab = \"k-occurrences\")"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"k-occurrence-in-the-1000d-case","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence in the 1000D case","title":"Hubness","text":"’s k-occurrence histogram looks like high dimensional case:  differences pretty stark. first thing notice x-axis. 2D case, maximum k-occurrence ~20. 1000D looking ~300. ’s hard see details, let’s zoom region 2D case clipping k-occurrence larger largest 2D k-occurrence:  ’s different distribution 2D case: large number anti-hubs noticeable number hubs. ’s certainly peak k-occurrence 15. Comparing numerical summary 2D case instructive: , ’s good reminder mean k-occurrence value. median k-occurrence immediately communicates difference 2D case. can also see maximum k-occurrence means one point considered close neighbor one third dataset. many anti-hubs ? quarter dataset appear neighbor point. serious implications using neighbor graph certain purposes: reach quarter dataset starting arbitrary point graph following neighbors. also might point nearest neighbor descent trouble high dimensional case: rely points turning neighbors points order introduce potential neighbors, fact many points dataset aren’t anyone’s actual neighbors suggest unlikely get involved local join procedure much points.","code":"g1000d_bfko <- k_occur(g1000d_nnbf$idx, k = 15) hist(g1000d_bfko, main = \"1000D 15-NN\", xlab = \"k-occurrences\") hist(pmin(g1000d_bfko, max(g2d_bfko)),   main = \"1000D 15-NN zoomed\",   xlab = \"k-occurrences\" ) summary(g1000d_bfko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    1.00    2.00    5.00   15.00   14.25  345.00 sum(g1000d_bfko == 1) #> [1] 221"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"k-occurrence-as-a-diagnostic-of-nnd-failure","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence as a diagnostic of NND failure","title":"Hubness","text":"now shown can use k-occurrences exact nearest neighbors low high dimensional data detect existence hubs, turn might lead us suspect approximate nearest neighbors found nearest neighbor descent may accurate. ’s useful diagnostic exact neighbors don’t need run NND first place. even approximate nearest neighbor graph produced NND isn’t highly accurate, still show similar characteristics hubness?  seems similar true results, zooming like exact results:  Visually looks lot like distribution exact results. Next, numerical summary: Quantitatively, also tracks exact results: median k-occurrence much smaller \\(k\\), hub large number neighbors (larger exact case similar degree) similar number anti-hubs. suggests way diagnose nearest neighbor descent routine may low accuracy: look distribution k-occurrences resulting approximate nearest neighbor graph (even just maximum value). value \\(\\gg k\\) may mean reduced accuracy. course, isn’t foolproof, even NND perfect job still get sorts values, ’s starting point. Taking distribution k-occurrences whole, approximate results seem track exact results fairly well, seen, errors approximate results uniformly distributed across data. let’s see well NND k-occurrences “predict” exact results:  overall relationship seems strong. line plot x=y, can see high values k-occurrence NND results tend -estimate k-occurrence, large values hardly matters, ambiguity nodes hub-like. Zooming lower values k-occurrence:  seems tendency -estimate k-occurrence. Anti-hubs also perfectly identified, true anti-hubs appear small number times approximate neighbor graph.","code":"g1000d_nndko <- k_occur(g1000d_nnd$idx, k = 15) hist(g1000d_nndko, main = \"1000D 15-NND\", xlab = \"k-occurrences\") hist(pmin(g1000d_nndko, max(g2d_bfko)),   main = \"1000D 15-NND zoomed\",   xlab = \"k-occurrences\" ) summary(g1000d_nndko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       4      15      12     406 sum(g1000d_nndko == 1) #> [1] 254 plot(g1000d_nndko, g1000d_bfko,   xlab = \"approximate\", ylab = \"exact\",   xlim = c(0, max(g1000d_nndko, g1000d_bfko)),   ylim = c(0, max(g1000d_nndko, g1000d_bfko)),   main = \"1000D k-occ\" ) abline(a = 0, b = 1) cor(g1000d_nndko, g1000d_bfko, method = \"pearson\") #> [1] 0.9939508 plot(g1000d_nndko, g1000d_bfko,   xlab = \"approximate\", ylab = \"exact\",   xlim = c(0, max(g2d_bfko)),   ylim = c(0, max(g2d_bfko)),   main = \"1000D low k-occ\" ) abline(a = 0, b = 1)"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"detecting-poorly-predicted-neighbors","dir":"Articles","previous_headings":"Detecting Hubness","what":"Detecting Poorly Predicted Neighbors","title":"Hubness","text":"’ve seen objects neighbors predicted better others. Based everything ’ve seen far k-occurrences NND, reasonable wonder: items dataset poorly predicted neighbors anti-hubs (predicted exact)? least give us way detecting items likely low accuracy neighborhoods: perhaps treated specially (algorithm). ’s plot accuracy k-occurrences NND neighbors:  answer question “really”, trend. empty space lower right plot indicates items large k-occurrence (hubs) well predicted. k-occurrence 150, guaranteed perfectly predict neighborhood item. However, end k-occurrence spectrum, can see lower bound predicted accuracy plummet k-occurrence reduced, anti-hubs actually neighborhoods accurately predicted . Unfortunately means k-occurrence bit rough use predict poorly-predicted items. Let’s say wanted get items neighborhood less 90% accurate: ’s already quite lot items: three-quarters entire dataset. largest k-occurrence item dataset accuracy threshold? , guarantee found items might poorly predicted, need filter every item k-occurrence smaller value, even though know well-predicted: ’s dataset. dropped threshold 80 accuracy, help? bit, ’s still substantial majority dataset. whatever decided items wouldn’t saving huge amount effort. much idea. suggests can’t improve results , just effort identifying individual points filter , treat differently merge back final neighbor graph means just reprocessing entire dataset different way likely competitive solution.","code":"plot(g1000d_nndko, g1000d_nnd_acc,   xlab = \"NND k-occ\", ylab = \"accuracy\",   xlim = c(0, max(g1000d_nndko, g1000d_bfko)),   main = \"1000D acc vs NND k-occ\" ) sum(g1000d_nnd_acc < 0.9) #> [1] 767 max(g1000d_nndko[g1000d_nnd_acc < 0.9]) #> [1] 48 sum(g1000d_nndko <= max(g1000d_nndko[g1000d_nnd_acc < 0.9])) #> [1] 929 sum(g1000d_nndko <= max(g1000d_nndko[g1000d_nnd_acc < 0.8])) #> [1] 860"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"detecting-problems-early","dir":"Articles","previous_headings":"Detecting Hubness","what":"Detecting Problems Early","title":"Hubness","text":"Back looking k-occurrence distribution whole: can see converged NND results, despite 100% accurate good job expressing hubness underlying data. converged results need ? think NND tool identifying hubness datasets whole rather accurate approximate nearest neighbor graphs? much less unconverged NND graph, obviously even less accurate, still correctly identify dataset hubs? test , let’s run NND method one iteration get k-occurrences result: accurate results? Ok, think can agree accurate neighbor graph. let’s take look k-occurrence distribution:  Looking familiar. Zooming …  distribution least similar converged version. Taking look numbers: Compared converged (exact) distribution, median k-occurrence low, object largest k-occurrence, large (\\(> 10k\\), seems like good threshold concerned presence hubs) large, fewer objects anti-hubs. least dataset, hubness can qualitatively detected even inaccurate neighbor graph. datasets don’t contain hubs? Let’s just check seeing artifact unconverged nearest neighbor descent, running procedure 2D dataset:  can see neighbor graph also accurate 1 iteration 2D case, distribution k-occurrences also qualitatively resembles exact result. time, compared exact results slightly anti-hubs maximum k-occurrence increased, trends slightly reversed compared 1000D data. least qualitative identification hubness, , one iteration nearest neighbor descent might enough.","code":"g1000d_nnd_iter1 <- nnd_knn(g1000d, k = 15, metric = \"euclidean\", n_iters = 1) g1000d_nndkoi1 <- k_occur(g1000d_nnd_iter1$idx, k = 15) neighbor_overlap(g1000d_nnbf, g1000d_nnd_iter1, k = 15) #> [1] 0.3202 hist(g1000d_nndkoi1, main = \"1000D 15-NND (1 iter)\", xlab = \"k-occurrences\") hist(pmin(g1000d_nndkoi1, max(g2d_bfko)),   main = \"1000D 15-NND (1 iter, zoomed)\",   xlab = \"k-occurrences\" ) summary(g1000d_nndkoi1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       2       7      15      17     212 sum(g1000d_nndkoi1 == 1) #> [1] 163 g2d_nnd_iter1 <- nnd_knn(g2d, k = 15, metric = \"euclidean\", n_iters = 1) g2d_nndkoi1 <- k_occur(g2d_nnd_iter1$idx, k = 15) hist(g2d_nndkoi1, main = \"2D 15-NND (1 iter)\", xlab = \"k-occurrences\") summary(g2d_nndkoi1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1      11      15      15      19      33 sum(g2d_nndkoi1 == 1) #> [1] 4 neighbor_overlap(g2d_nnbf, g2d_nnd_iter1, k = 15) #> [1] 0.3496667"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"improving-accuracy","dir":"Articles","previous_headings":"","what":"Improving accuracy","title":"Hubness","text":"know nearest neighbor descent (least typical settings) may give highly accurate results high dimensions. help k-occurrences, can even detect might happening. can ?","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"use-more-neighbors","dir":"Articles","previous_headings":"Improving accuracy","what":"Use More Neighbors","title":"Hubness","text":"One simple (slightly expensive) way keep neighbors calculation. example, double number neighbors 30, get top-15 accuracy: ’s big improvement, increasing k way can quite expensive terms run time.","code":"g1000d_nnd_k30 <- nnd_knn(g1000d, k = 30, metric = \"euclidean\") neighbor_overlap(g1000d_nnbf, g1000d_nnd_k30, k = 15) #> [1] 0.9746667"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"use-more-candidates","dir":"Articles","previous_headings":"Improving accuracy","what":"Use More Candidates","title":"Hubness","text":"Another way improve accuracy increase number candidates considered iteration. controlled max_candidates parameter. Let’s try increasing 30: good increasing k, also time-consuming still improvement default setting (set k). probably first choice improving accuracy.","code":"g1000d_nnd_mc30 <- nnd_knn(g1000d, k = 15, metric = \"euclidean\", max_candidates = 30) neighbor_overlap(g1000d_nnbf, g1000d_nnd_mc30) #> [1] 0.8752"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"decrease-the-convergence-tolerance","dir":"Articles","previous_headings":"Improving accuracy","what":"Decrease the convergence tolerance","title":"Hubness","text":"set delta lower default 0.001 even 0 force algorithm run convergence: can see, ’s unlikely achieve much. default parameters pretty good job balancing accuracy speed.","code":"g1000d_nnd_tol0 <- nnd_knn(g1000d, k = 15, metric = \"euclidean\", delta = 0) neighbor_overlap(g1000d_nnbf, g1000d_nnd_tol0) #> [1] 0.7881333"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"merging-multiple-independent-results","dir":"Articles","previous_headings":"Improving accuracy","what":"Merging Multiple Independent Results","title":"Hubness","text":"taking advantage stochastic nature algorithm? results sufficiently diverse runs NND, generate two graphs two separate runs, merge results. Let’s repeat NND see accuracy new result like. ’s similar first run. ’s re-assuring sense variance accuracy doesn’t seem high one run next. hopefully doesn’t also mean NND producing similar neighbor graph time, case merging won’t helpful. Time find : ’s big improvement. seem like diversity results.  Despite similar overall accuracies, ’s quite large variance runs terms items accurate neighborhoods. might scope improving results merging different runs, especially can run individual NND routines parallel.","code":"g1000d_nnd_rep <- nnd_knn(g1000d, k = 15, metric = \"euclidean\") neighbor_overlap(g1000d_nnbf, g1000d_nnd_rep, k = 15) #> [1] 0.7776 g1000d_nnd_merge <- merge_knn(list(g1000d_nnd, g1000d_nnd_rep)) neighbor_overlap(g1000d_nnbf, g1000d_nnd_merge, k = 15) #> [1] 0.9058 g1000d_nnd_rep_acc <-   neighbor_overlap(g1000d_nnbf, g1000d_nnd_rep, k = 15, ret_vec = TRUE)$overlaps plot(   g1000d_nnd_acc,   g1000d_nnd_rep_acc,   main = \"1000D NND accuracy comparison\",   xlab = \"accuracy run 1\",   ylab = \"accuracy run 2\" ) cor(g1000d_nnd_acc, g1000d_nnd_rep_acc) #> [1] 0.5680036"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"using-a-search-graph","dir":"Articles","previous_headings":"Improving accuracy","what":"Using a Search Graph","title":"Hubness","text":"Practically, simplest way improve results rnndescent convert neighbor graph search graph, query original data. First, preparation step: augments neighbor graph reversed edges neighbor graph, \\(\\) one nearest neighbors \\(j\\), guarantee \\(j\\) also considered near neighbor \\(\\). ameliorates issue anti-hubs \\(k\\) neighbors anti-hub now neighbor list. downside including reversed edges neighbor graph neighbor list hub now going large consists \\(k\\) nearest neighbors hub items consider hub near neighbor, definition lot. can make search graph inefficient, disproportionate amount time spent searching neighbors hub. diversify_prob pruning_degree_multiplier parameters used reduce back -degree node (number -going edges). results objects varying number neighbors, case maximum 22. 50% larger k = 15 account introduction reverse edges. Anti-hubs can reintroduced due edge reduction, hopefully distribution edges bit equitable. summary histogram k-occurrences search graph:  quite skewed neighbor graph, still lot room improvement. rate, search graph hand, can now search using original data query: results improved? Yes, accuracy now nearly perfect. disadvantages search graph approach building neighbor graph less efficient NND: graph_knn_query must assume query data entirely different reference data. advantage can make use reverse edges , importantly, back-tracking (controlled via epsilon parameter), seems make difference example. procedure recommended practice using graph_knn_query search graph generated neighbor graph. required use search graph argument reference_graph parameter. back-tracking search using neighbor graph directly everything else : Accuracies nearly good. can save even time turning back-tracking (epsilon = 0): accuracies now noticeably less improved. Using search graph without back-tracking gives slightly better accuracies: seems like sort back-tracking recommended approach. downside searching large proportion dataset, case save time using brute_force_query. conjunction epsilon, can also use max_search_fraction terminate search fraction dataset searched exceeds specified value. Set value 0 1, e.g. max_search_fraction = 0.1 don’t want 10% reference data searched. default 1, epsilon entirely controls search termination. similar parameter used (Harwood Drummond 2016). Accuracies reduced.","code":"g1000d_search_graph <-   prepare_search_graph(     data = g1000d,     graph = g1000d_nnd,     metric = \"euclidean\",     diversify_prob = 1,     pruning_degree_multiplier = 1.5   ) g1000d_sgko <- k_occur(g1000d_search_graph) hist(g1000d_sgko, main = \"search graph k-occurrences\", xlab = \"k-occurrences\") summary(g1000d_sgko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   1.000   4.000   5.000   7.186   9.000  22.000 sum(g1000d_sgko == 1) #> [1] 24 g1000d_search <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_search_graph,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0.1   ) neighbor_overlap(g1000d_nnbf, g1000d_search, k = 15) #> [1] 0.9978 g1000d_nnd_search <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_nnd,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0.1   ) neighbor_overlap(g1000d_nnbf, g1000d_nnd_search, k = 15) #> [1] 0.9845333 g1000d_nnd_search0 <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_nnd,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0   ) neighbor_overlap(g1000d_nnbf, g1000d_nnd_search0, k = 15) #> [1] 0.8286667 g1000d_search0 <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_search_graph,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0   ) neighbor_overlap(g1000d_nnbf, g1000d_search0, k = 15) #> [1] 0.8467333 g1000d_nnd_search_max <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_nnd,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0.1,     max_search_fraction = 0.1   ) neighbor_overlap(g1000d_nnbf, g1000d_nnd_search_max, k = 15) #> [1] 0.8228667"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Hubness","text":"High dimensional data leads hubs. “hubness” item dataset can measured k-occurrence corresponding nearest neighbor graph. higher k-occurrence, hub . existence hubs implies existence “anti-hubs”, .e. items low k-occurrence. small number hubs can create disproportionately larger number anti-hubs, larger value k-occurrence creating anti-hubs. accuracy nearest neighbor descent reduced presence hubs: specifically, lower k-occurrence item, greater probability low accuracy nearest neighbors. Accuracy nearest neighbor descent can improved searching larger number neighbors truncating result desired size, cost longer run-time memory usage. Alternatively, can run nearest neighbor descent multiple times different random starting points merge results. accurate efficient results obtained converting nearest neighbor descent results search graph querying graph original data, using nearest neighbor results initialization back-tracking search. concerned potential hubs interfering accuracy neighbor graph, suggest following steps: Generate neighbor graph nnd_knn default parameters, even just 1 iteration. Evaluate hubness graph k_occur. maximum k-occurrence exceeds threshold (maybe 10 * k good starting point), try following: Restart nnd_knn (may well use output first step initialization) max_candidates set larger value. probably don’t want set larger 60. Repeat step 4 get second graph. Merge graphs steps 4 5 merge_knn. Use merged graph prepare_search_graph, run graph_knn_query back-tracking search (set epsilon > 0) refine results . 8 step 3, maximum k-occurrence less threshold, probably ok run nnd_knn defaults. provide robust approach producing accurate approximate nearest neighbors without spending time unnecessary graph search results probably already quite good. effect hubness nearest neighbors, advanced attempts fix problem, see work Flexer co-workers (Schnitzer et al. 2012; Flexer 2016; Feldbauer Flexer 2019; Feldbauer, Rattei, Flexer 2019) Radovanović co-workers (Radovanovic, Nanopoulos, Ivanovic 2010; Bratić et al. 2019).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/metrics.html","id":"specialized-binary-metrics","dir":"Articles","previous_headings":"","what":"Specialized Binary Metrics","title":"Metrics","text":"metrics intended use binary data. means : numeric data consist two distinct values, typically 0 1. get unpredictable results otherwise. provide data logical matrix, much faster implementation used. metrics can use binary data : \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" ’s example using binary data stored 0s 1s \"hamming\" metric: Now let’s convert logical matrix: results : real-world dataset, logical version much faster.","code":"set.seed(42) binary_data <- matrix(sample(c(0, 1), 100, replace = TRUE), ncol = 10) head(binary_data) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #> [1,]    0    0    0    1    0    1    0    1    1     0 #> [2,]    0    1    0    1    0    0    1    0    0     1 #> [3,]    0    0    0    1    1    1    1    1    1     0 #> [4,]    0    1    0    1    1    1    1    1    0     1 #> [5,]    1    0    0    0    1    1    1    1    0     1 #> [6,]    1    0    1    1    1    1    1    1    1     1 nn <- brute_force_knn(binary_data, k = 4, metric = \"hamming\") logical_data <- binary_data == 1 head(logical_data) #>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] #> [1,] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE #> [2,] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE #> [3,] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE #> [4,] FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE #> [5,]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE #> [6,]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE logical_nn <- brute_force_knn(logical_data, k = 4, metric = \"hamming\") all.equal(nn, logical_nn) #> [1] TRUE"},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"local-join","dir":"Articles","previous_headings":"","what":"Local Join","title":"Nearest Neighbor Descent","text":"process described involves lot looping repeated fetching neighbor vectors, NND actually uses concept “local join”. One way think consider item fielding requests nearest neighbors. repeatedly asked item considers neighbor. work start iteration know items consider neighbor, can generate candidates neighbor pairs involved . need iterate items graph. need work finding considers neighbor also requires loop graph also. clear, amount work needs done, different order, everything bit efficient terms needs fetched memory. -shot using local join approach rather iterating graph one item time, end list pairs items (, j) update graph whole . dealing kNN graph pair (, j) also (j, ) potential update, cost one distance calculation. challenges terms parallel implementation also makes caching distances bit harder ’s still better naive approach explicitly looping neighbors--neighbors.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"other-heuristics","dir":"Articles","previous_headings":"","what":"Other Heuristics","title":"Nearest Neighbor Descent","text":"Additionally, two heuristics used reduce amount work done. first candidate neighbors split “new” “old” candidates. “new” candidate neighbor neighbor entered graph previous iteration. “Old” neighbors everything else. local join, possible pairs “new” neighbors used updating graph, “old” neighbors ever paired “new” neighbors, “old” neighbors. referred “incremental search” NND paper. Also, tolerance \\(\\delta\\) used determine early stopping criterion. total number items graph \\(kN\\) \\(k\\) number neighbors \\(N\\) number items. iteration, counter incremented every time graph successfully updated. end iteration number updates less \\(\\delta kN\\) iteration stops.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"pynndescent-modifications","dir":"Articles","previous_headings":"","what":"PyNNDescent Modifications","title":"Nearest Neighbor Descent","text":"one minor change PyNNDescent works versus description NND paper, rnndescent also uses, sampling candidates works. local join, need know just neighbors , items consider neighbor, call “reverse neighbors” . always \\(k\\) “forward” neighbors graph, don’t control neighbor , neighbor many (even ) items dataset. Thus, building reverse list can bit challenging need prepared item \\(N\\) neighbors. NND paper, avoided defining sample rate \\(\\rho\\), used sample k-nearest neighbors, reverse neighbor list built sampled items. subsequent -sampling applied reverse neighbor list forward reverse neighbor list contain \\(\\rho k\\) items. Instead sample rate, rnndescent defines max_candidates parameter determines size forward reverse neighbor lists per item. candidates max_candidates value, retained candidates chosen randomly works like random sampling. Finally, instead random initialization, PyNNDescent uses k-nearest neighbors graph random projection forest. entire vignette explaining RP forest works. also option rnndescent.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Nearest Neighbor Descent","text":"’s easy enough run NND dataset. ’s example using iris dataset: contents iris_knn list two elements, \\(N\\) \\(k\\) matrices \\(N\\) number items dataset \\(k\\) number neighbors: idx contains indices neighbors: dist contains distances: Apart k, parameters may want modify: metric distance metric use. default Euclidean distance. several metrics can use. See documentation nnd_knn full list. init initialization method. default \"rand\" initializes neighbors randomly. may wish use \"tree\" uses random projection forest initialize neighbors, similar rpf_build. control tree building, can pass sort parameters rpf_build via init_args parameter. See vignette RP forest details. can also pass neighbor graph directly. format output nnd_knn, .e. list two matrices size \\(N\\) \\(k\\). NND can used refine existing graph generated methods, e.g. RcppAnnoy RcppHNSW. n_iters number iterations NND carry . default choose based \\(N\\), number items dataset. amount work done per iteration decreases quite rapidly, sticking default usually sensible, especially don’t change convergence criterion delta (see ), often causes algorithm stop early anyway. delta controls early stopping must value 0 1. given iteration, number changes neighbor graph less delta * k * N algorithm stops. default 0.001 can interpret roughly neighbor graph needs changed 0.1% avoid early stopping. max_candidates controls size forward reverse neighbor lists. default set whatever smaller, k 60. n_threads controls number threads use. default run single thread. slow part approximate nearest neighbor algorithm distance calculation using multiple threads usually good idea. ret_forest TRUE, set init = \"tree\", random projection forest used initialize neighbor graph returned well. want generate new neighbors based original data want . verbose set TRUE get information progress NND. progress affects progress NND displayed verbose = TRUE. default bar shows textual progress bar. can also set progress = \"dist\" show current value convergence criterion sum distances iteration. can help bit determine iterations different convergence criterion help. Note NND uses random number generation determine order processing candidates, reproducible results set random number seed explicitly. Also, way parallelism implemented means reproducibility possible different settings n_threads even consistent seed, e.g. going n_threads = 0 n_threads = 4 give different results, even set.seed fixed seed beforehand.","code":"iris_knn <- nnd_knn(iris, k = 15) iris_knn$idx[1:2, 1:5] #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1   18   29    5   28 #> [2,]    2   13   46   35   10 iris_knn$dist[1:2, 1:5] #>      [,1]      [,2]      [,3]      [,4]      [,5] #> [1,]    0 0.1000000 0.1414212 0.1414212 0.1414213 #> [2,]    0 0.1414213 0.1414213 0.1414213 0.1732050"},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Nearest Neighbor Descent","text":"reason believe aren’t getting results NND sufficiently accurate, probably best thing increase max_candidates. Reducing delta increasing n_iters usually less effect. Restarting nnd_knn init set output previous run usually also helps, time-efficient way improve matters. (lightly edited) sample output running tells dataset size iris, 7 iterations run. 1 / 7, 2 / 7 logged end iteration. Following sum distances neighbors heap, number updates neighbor graph convergence criterion. num_updates falls tol algorithm stops. case, third iteration updates , algorithm stopped early. case, almost certainly NND found exact nearest neighbors, wouldn’t worried modifying parameters. inclined, output shows little point increasing n_iters reducing delta. really leaves max_candidates option. vignette dealing hubness (can issue) goes bit detail use different functions rnndescent deal sort problem.","code":"iris_knn <- nnd_knn(iris, k = 15, verbose = TRUE, progress = \"dist\") Running nearest neighbor descent for 7 iterations 1 / 7 heap sum = 647.85 num_updates = 3356 tol = 2.25 2 / 7 heap sum = 599.9 num_updates = 216 tol = 2.25 3 / 7 heap sum = 599.9 num_updates = 0 tol = 2.25 Convergence: c = 0 tol = 2.25"},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"querying-new-data","dir":"Articles","previous_headings":"","what":"Querying New Data","title":"Nearest Neighbor Descent","text":"can’t. NND can produce k-nearest neighbors graph provided data. doesn’t produce “index” kind can query. value NND local join really makes sense can take advantage fact calculating distance \\(d_{ij}\\) lets update neighbor list \\(\\) \\(j\\) . try apply concepts NND querying new data quickly end method looks lot like greedy graph-based searches. , look graph_knn_query(), although noted can also use random projection forest used initialize neighbor graph init = \"tree\". also probably want augment neighbor graph make amenable searching using prepare_search_graph().","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"brute-force","dir":"Articles","previous_headings":"","what":"Brute Force","title":"Querying Data","text":"dataset small enough, can just use brute force find neighbors. index build, worry approximate results : format brute_nbrs usual k-nearest neighbors graph format, list two matrices, dimension (nrow(iris_odd), k). first matrix, idx contains indices nearest neighbors, second matrix, dist contains distances neighbors (’ll just show first five results per row):","code":"brute_nbrs <- brute_force_knn_query(   query = iris_odd,   reference = iris_even,   k = 15 ) lapply(brute_nbrs, function(m) {   head(m[, 1:5]) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    9   20   14    4   25 #> [2,]   24    2   23   15    1 #> [3,]   19    9    4   20   14 #> [4,]   24    6   15    2   19 #> [5,]    2    7   24   23   15 #> [6,]   14   10    3   16   11 #>  #> $dist #>           [,1]      [,2]      [,3]      [,4]      [,5] #> [1,] 0.1000000 0.1414213 0.1414213 0.1732050 0.2236068 #> [2,] 0.1414213 0.2449490 0.2645753 0.3000001 0.3000002 #> [3,] 0.1414213 0.1732050 0.2236066 0.2449488 0.2449488 #> [4,] 0.2236068 0.3000002 0.3162278 0.3316627 0.4123106 #> [5,] 0.2999998 0.3464101 0.3605550 0.4242641 0.4690414 #> [6,] 0.2828429 0.3316626 0.3464102 0.3605551 0.3605553"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"random-projection-forests","dir":"Articles","previous_headings":"","what":"Random Projection Forests","title":"Querying Data","text":"build random projection forest rpf_build, can query rpf_knn_query: See Random Partition Forests vignette .","code":"rpf_index <- rpf_build(iris_even) rpf_nbrs <- rpf_knn_query(   query = iris_odd,   reference = iris_even,   forest = rpf_index,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"graph-search","dir":"Articles","previous_headings":"","what":"Graph Search","title":"Querying Data","text":"See (Dobson et al. 2023) overview graph search algorithms, can described greedy beam search graph: find nearest neighbors, start candidate graph, find distance candidate query point, update neighbor list query accordingly. candidate made neighbor list query, seems like promising direction go , add candidate’s neighbors list candidates explore. Repeat time run candidates. may want explore neighbors candidate even doesn’t make onto current neighbor list, distance sufficiently small. much tolerance controls much back-tracking hence much exploration amount time spend search. graph_knn_query implements search. least must provide reference_graph search, reference data built reference_graph (can calculate distances), k number neighbors want, course query data: aren’t using metric = \"euclidean\", also provide metric used build reference_graph. default metric always \"euclidean\" function rnndescent ’s provided examples . parameters want tweak real world case merit deeper discussion.","code":"graph_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = rpf_nbrs,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"n_threads","dir":"Articles","previous_headings":"Graph Search","what":"n_threads","title":"Querying Data","text":"n_threads controls many threads use search. aware graph_knn_query designed batch parallelism, thread responsible searching subset query points. means streaming context, queries search likely arrive one time, won’t get speed using multiple threads.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"epsilon","dir":"Articles","previous_headings":"Graph Search","what":"epsilon","title":"Querying Data","text":"epsilon controls much exploration neighbors candidate , suggested (Iwasaki Miyazaki 2018). default value 0.1, also default NGT library. larger value, back-tracking permitted. exact meaning value related large distance considered “close enough” current neighbor list query worth exploring. epsilon = 0.1 means query-candidate distance allowed 10% larger largest distance neighbor list. set epsilon = 0.2, example, query-candidate distance allowed 20% higher largest distance neighbor list . set epsilon = 0 get pure greedy search. ’s hard give general rule value set, ’s highly dependent distribution distances dataset determined distance metric dimensionality data . recommend leaving default, modifying find search unreasonably slow (case make epsilon smaller) unreasonably inaccurate (case make epsilon larger). Yes, helpful know. benchmarking done (Dobson et al. 2023) using similar back-tracking method, epsilon = 0.25 maximum value used (Wang et al. 2021) epsilon = 0.1 used.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"init","dir":"Articles","previous_headings":"Graph Search","what":"init","title":"Querying Data","text":"controls search initialized. don’t provide , k random neighbors per item query generated .","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"neighbor-graph-input","dir":"Articles","previous_headings":"Graph Search > init","what":"Neighbor Graph Input","title":"Querying Data","text":"may provide input . neighbor graph format, .e. list two matrices, idx dist, described . Make sure dist matrix contains distances using metric use search.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"neighbor-indices-only","dir":"Articles","previous_headings":"Graph Search > init","what":"Neighbor Indices Only","title":"Querying Data","text":"fact, dist matrix optional. provide idx matrix, dist matrix calculated . dist matrix already available generated rnndescent reason use , neighbors come : another nearest neighbor package reason don’t distance indices different metric nonetheless believe good guess “real” metric. case might worth experimenting can cheaply binarize input data, .e. convert 0/1 FALSE/TRUE: use hamming metric another binary-specialized metric input data. Even brute force search can fast data. good way get good guess real data. contrived example iris, let’s anyway: brute force search binarized data: pass indices brute force search graph_knn_query, generate Euclidean distances : Whether worth depends whether time taken binarize data followed initial search binary data (doesn’t brute force) gives good enough guess save time “real” search graph_knn_query.","code":"numeric_iris <- iris[, sapply(iris, is.numeric)] logical_iris <- sweep(numeric_iris, 2, colMeans(numeric_iris), \">\") logical_iris_even <- logical_iris[seq_len(nrow(logical_iris)) %% 2 == 0, ] logical_iris_odd <- logical_iris[seq_len(nrow(logical_iris)) %% 2 == 1, ] head(logical_iris_even) #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]        FALSE       FALSE        FALSE       FALSE #> [2,]        FALSE        TRUE        FALSE       FALSE #> [3,]        FALSE        TRUE        FALSE       FALSE #> [4,]        FALSE        TRUE        FALSE       FALSE #> [5,]        FALSE        TRUE        FALSE       FALSE #> [6,]        FALSE        TRUE        FALSE       FALSE iris_logical_brute_nbrs <- brute_force_knn_query(   query = logical_iris_odd,   reference = logical_iris_even,   k = 15,   metric = \"hamming\" ) graph_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = rpf_nbrs,   init = iris_logical_brute_nbrs$idx,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"forest-initialization","dir":"Articles","previous_headings":"Graph Search > init","what":"Forest initialization","title":"Querying Data","text":"previously built RP Forest data may also use initialize query. can re-use rpf_index . general, RP forest initialization likely better initial guess random, terms speed/accuracy trade-, using large forest may best choice. may want use rpf_filter reduce size forest using initial guess. PyNNDescent Python package rnndescent based , one tree used initializing query results.","code":"forest_init_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = rpf_nbrs,   init = rpf_index,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"preparing-the-search-graph","dir":"Articles","previous_headings":"","what":"Preparing the Search Graph","title":"Querying Data","text":"examples far, used k-nearest neighbors graph reference_graph input graph_knn_query. actually good idea? Probably ! guarantee items original dataset can actually reached via k-nearest neighbors graph. nodes just aren’t popular may neighbor list item. means can never reach via k-nearest neighbors graph, matter thoroughly search . can solve problem reversing edges graph adding graph. can get item item j, can now get item j item . solves one problem adds just like items unpopular, items might popular appear often neighbor list items. large number edges graph can make search slow. therefore need prune edges. prepare_search_graph function take k-nearest neighbor graph add edges make useful search. procedure based process described (Harwood Drummond 2016) consists : Reversing edges graph. “Diversifying” graph “occlusion pruning”. considers triplets points, removes long edges probably redundant. item \\(\\) neighbors \\(p\\) \\(q\\) distances \\(d_{pq} \\lt d_{ip}\\) .e. neighbors closer \\(\\), said \\(p\\) occludes \\(q\\) don’t need edges \\(\\rightarrow p\\) \\(\\rightarrow q\\) – ’s likely \\(q\\) neighbor list \\(p\\) vice versa, ’s unlikely harm getting rid \\(\\rightarrow p\\). occlusion pruning, item still excessive number edges, longest edges removed number edges threshold. control pruning following parameters available:","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"diversification-probability","dir":"Articles","previous_headings":"Preparing the Search Graph","what":"Diversification Probability","title":"Querying Data","text":"diversify_prob probability neighbor removed found “occlusion”. take value 0 (diversification) 1 (remove many edges possible). default 1.0. DiskAnn/Vamana method’s pruning algorithm almost identical instead probability, uses related parameter called alpha, acts opposite direction: increasing alpha increases density graph. telling ? pbbsbench implementation PyNNDescent uses alpha instead diversify_prob accompanying paper (Dobson et al. 2023) mention use alpha yields “modest improvements” – context seems mean relative using diversify_prob = 1.0. can’t give exact mapping two values unfortunately.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"degree-pruning","dir":"Articles","previous_headings":"Preparing the Search Graph","what":"Degree Pruning","title":"Querying Data","text":"pruning_degree_multiplier controls many edges remove occlusion pruning relative number neighbors original nearest neighbor graph. default 1.5 means allow many 50% edge original graph. input graph k = 15, item search graph 15 * 1.5 = 22 edges. Let’s see works iris neighbors: returned search graph can contain different number edges per item, neighbor graph format isn’t suitable. Instead get back sparse matrix, specifically dgCMatrix. ’s histogram edges distributed:  items around k = 15 edges just like nearest neighbor graph. maximum number edges 10 edges.","code":"set.seed(42) iris_search_graph <- prepare_search_graph(   data = iris_even,   graph = rpf_nbrs,   diversify_prob = 0.1,   pruning_degree_multiplier = 1.5 ) search_graph_edges <- diff(iris_search_graph@p) hist(search_graph_edges,   main = \"Distribution of search graph edges\", xlab = \"# edges\" ) range(search_graph_edges) #> [1] 10 22 search_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = iris_search_graph,   init = rpf_index,   k = 15 )"},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"random-partition-forests","dir":"Articles","previous_headings":"","what":"Random Partition Forests","title":"Random Partition Forests","text":"’s basic introduction random partition forests work.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"building-a-space-partitioning-tree","dir":"Articles","previous_headings":"Random Partition Forests","what":"Building a Space-Partitioning Tree","title":"Random Partition Forests","text":"First, consider recipe building space-partitioning tree: Select dimension. Select split point along dimension. Split data two child nodes based split point. Repeat steps 1-3 two groups. number items group less threshold, node now leaf, stop splitting. Variations steps 1 2 determines vast majority differences various tree-based methods.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"building-a-random-partition-tree","dir":"Articles","previous_headings":"Random Partition Forests","what":"Building a Random Partition Tree","title":"Random Partition Forests","text":"random partition tree : Select two points random. Calculate mid-point two points. enough define hyperplane data. exactly algorithm described (Dasgupta Freund 2008), ’s done similar method Annoy. Step 3 involves calculating side hyperplane point assigning data child nodes basis.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"from-trees-to-forests","dir":"Articles","previous_headings":"Random Partition Forests","what":"From Trees to Forests","title":"Random Partition Forests","text":"random partition forest just collection random partition trees. random nature trees, different.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"build-a-forest","dir":"Articles","previous_headings":"","what":"Build a Forest","title":"Random Partition Forests","text":"build forest rnndescent, use rpf_build function. ’ll use iris dataset example, goal finding 15-nearest neighbors item dataset. options disposal: metric: type distance calculation use. default euclidean, lot choose . See help text metric parameter rpf_build()` details. n_trees: number trees build. default choose based size data provided, maximum 32: eventually get diminishing returns number trees forest. leaf_size: number items leaf. splitting procedure stops fewer number items node. default 10 want leaf size scale number neighbors look , increased 15 example. bigger value accurate search , cost lot distance calculations carry . Conversely, make small compared number neighbors, may end items finding k neighbors. max_tree_depth: maximum depth tree. tree reaches depth even current node size exceeds value leaf_size, stop splitting. point splitting tree size leaf rapidly decrease go tree, ideal case decrease factor two level, ideally can process datasets vary many orders magnitude depth tree increases levels. default max_tree_depth 200, trigger limit, answer may increase depth. ’s likely something distribution data prevents splitting well. case, ’s different metric try still relevance data, ’s worth try, possibly best solution abandon tree-based approach (example initialize nearest neighbor descent random neighbors). set verbose = TRUE get warning maximum leaf size larger leaf_size. margin: makes slight modification assignment data sides hyperplane calculated. ’ll discuss . forest returned just R list, can save load saveRDS readRDS without issue. ’s something want inspect definitely don’t modify . ’s mainly useful passing functions, like one talk next.","code":"iris_forest <- rpf_build(iris, leaf_size = 15)"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"finding-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Finding Nearest Neighbors","title":"Random Partition Forests","text":"use find nearest neighbors, query point traverse tree root leaf, calculating side hyperplane encounters. items leaf ends candidates nearest neighbors. query forest just build, use rpf_knn_query function. Apart forest , also need data want query (query) data used build forest (reference), forest doesn’t store information. thus case, looking k-nearest neighbors iris, query reference , don’t . point, must also specify number neighbors want. iris_query returned list two matrices: idx contains row indices k-nearest neighbors, dist contains distances.","code":"iris_query <-   rpf_knn_query(     query = iris,     reference = iris,     forest = iris_forest,     k = 15   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"a-small-optimization-for-the-k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"A Small Optimization for the k-Nearest Neighbors","title":"Random Partition Forests","text":"use querying approach mentioned finding k-nearest neighbors data used building tree. However, data already partitioned want k-nearest neighbor data, ’s efficient way : leaf, k-nearest neighbors point leaf members leaf. usually distance calculations take time looking neighbors, avoid make tree traversals associated hyperplane distance calculations. give result running rpf_build followed rpf_knn_query (apart vagaries random number generator), lot convenient bit faster. access parameters forest building rpf_build, e.g. leaf_size, n_trees, max_tree_depth etc. Additionally, want k-nearest neighbors also want forest future querying, set ret_forest = TRUE, return value now also contain forest forest item list. example build forest (get 15-nearest neighbors) first 50 iris items query remaining 100:","code":"iris_knn <- rpf_knn(iris, k = 15) iris_knn_with_forest <-   rpf_knn(iris[1:50, ], k = 15, ret_forest = TRUE) iris_query_virginica <-   rpf_knn_query(     query = iris[51:150, ],     reference = iris[1:50, ],     forest = iris_knn_with_forest$forest,     k = 15   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"margin","dir":"Articles","previous_headings":"","what":"Margin","title":"Random Partition Forests","text":"margin parameter determines calculate side hyperplane item split belongs . usual method (margin = \"explicit\") thing PyNNDescent: way hyperplane defined use vector defined two points \\(\\) \\(b\\) normal vector plane, point midway point plane. calculate margin point \\(x\\) (effectively signed distance plane \\(x\\)) : \\[ \\text{margin}(\\mathbf{x}) = ((\\mathbf{b} - \\mathbf{}) \\cdot (\\mathbf{x} - \\frac{\\mathbf{} + \\mathbf{b}}{2})) \\] Taking dot products vectors finding mid points totally unexceptional using Euclidean metric. monotonic relationship cosine distances Euclidean distance normalization vectors, can define “angular” version calculation works normalized vectors. datasets bit weird un-natural. Imagine dataset binary vectors applying e.g. Hamming metric. mid-point two binary vectors binary vector, make sense think geometric relationship implied dot product. alternative calculating margin via explicit creation hyperplane, instead think distance \\(x\\) \\(\\), \\(d_{xa}\\) compares distance \\(x\\) \\(b\\), \\(d_{xb}\\) significance margin . Remember vector defined \\(\\) \\(b\\) normal vector hyperplane, can think line connecting \\(\\) \\(b\\), hyperplane splitting line two equal halves. Now imagine \\(x\\) somewhere line. \\(x\\) closer \\(\\) \\(b\\) must side hyperplane \\(\\), vice versa. Therefore can calculate margin comparing \\(d_{xa}\\) \\(d_{xb}\\) seeing value smaller. don’t explicitly create hyperplane, call “implicit” margin method can choose generate splits way setting margin = \"implicit\". ’ll use random binary data example. Note .logical call: rnndescent detects binary data format specify metric appropriate binary data (e.g. Hamming), use margin = \"implicit\" specialized function called much faster functions written generic floating point data mind. following give results large datasets likely noticeably slower: implicit margin method faster (makes sense metrics) ever want use explicit method? Well, implicit method faster binary data specialized metrics. downside implicit method determining side hyperplane requires two distance calculations per point, whereas explicit method requires dot product calculation, likely costly single distance calculation. floating point data, explicit method likely twice fast. ’s lot think default setting margin \"auto\", tries right thing: using binary data suitable metric, use implicit method, otherwise use explicit method normalize vectors give “angular” approach metrics put emphasis angle versus magnitude.","code":"binary_data <- matrix(as.logical(rbinom(1000, 1, 0.5)), ncol = 10) bin_knn_imp <-   rpf_knn(binary_data,     k = 15,     metric = \"hamming\",     margin = \"implicit\"   ) bin_knn_exp <-   rpf_knn(binary_data,     k = 15,     metric = \"hamming\",     margin = \"explicit\"   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"filtering-a-forest","dir":"Articles","previous_headings":"","what":"Filtering a Forest","title":"Random Partition Forests","text":"mentioned beginning vignette, rnndescent ’s expected use random partition forests initialization nearest neighbor descent. case, keeping entire forest querying new data probably unnecessary: can keep “best” trees. PyNNDescent keeps one tree purpose. determining tree “best”, mean tree reproduces k-nearest neighbor graph effectively. can comparing existing k-nearest neighbor graph produced single tree. rpf_filter function : n_trees number trees keep. Feel free keep like, although extra diversification step ensure trees retained good reproducing k-nearest neighbor graph diverse (perhaps reproduce different parts neighbor graph well?). higher quality k-nearest-neighbor graph , better filtering work although example uses graph forest, might get better results using graph run nearest neighbor descent forest result input.","code":"iris_filtered <-   rpf_filter(     nn = iris_query,     forest = iris_forest,     n_trees = 1   )"},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"find-the-k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Find the k-nearest neighbors","title":"rnndescent","text":"just want k-nearest neighbors data, use rnnd_knn:","code":"iris_knn <- rnnd_knn(data = iris, k = 5)"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"the-neighbor-graph-format","dir":"Articles","previous_headings":"Find the k-nearest neighbors","what":"The Neighbor Graph Format","title":"rnndescent","text":"nearest neighbor graph format returned functions package list two matrices: idx – matrix indices nearest neighbors. usual R, 1-indexed. dist – equivalent distances.","code":"lapply(iris_knn, function(x) {   head(x, 3) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1   18    5   29   28 #> [2,]    2   13   46   35   10 #> [3,]    3   48    4    7   13 #>  #> $dist #>      [,1]      [,2]      [,3]      [,4]      [,5] #> [1,]    0 0.1000000 0.1414212 0.1414212 0.1414213 #> [2,]    0 0.1414213 0.1414213 0.1414213 0.1732050 #> [3,]    0 0.1414213 0.2449490 0.2645751 0.2645753"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"build-an-index","dir":"Articles","previous_headings":"","what":"Build an Index","title":"rnndescent","text":"rnnd_knn returns k-nearest neighbors, return “index” can use query new data. , use rnnd_build. Normally query index different used build index, let’s split iris : index also list lot components (none intended manual examination), apart neighbor graph can found graph component format return value rnnd_knn: aware large high-dimensional data, returned index can get large, especially set n_search_trees large value.","code":"iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_index <- rnnd_build(iris_even, k = 5) lapply(iris_index$graph, function(x) {   head(x, 3) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1   23    5   13   18 #> [2,]    2   24   15   23    5 #> [3,]    3   10   11   17   14 #>  #> $dist #>      [,1]      [,2]      [,3]      [,4]      [,5] #> [1,]    0 0.1414213 0.1732050 0.2236068 0.3000000 #> [2,]    0 0.1414215 0.1732051 0.2645753 0.3162279 #> [3,]    0 0.3872986 0.4123107 0.4795830 0.5291505"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"querying-data","dir":"Articles","previous_headings":"","what":"Querying Data","title":"rnndescent","text":"query new data, use rnnd_query: don’t need keep data used build index around, internally, index stores (’s one reasons index can get large). Another use rnnd_query improve quality k-nearest neighbor graph. using query data used build iris_index specifying via init parameter knn graph already generated: k-nearest neighbor graph index$graph isn’t sufficiently high quality, result running rnnd_query data improvement. Exactly much better hard say, can always compare sum distances: case, initial knn improved, hardly surprising due size dataset. Another function might use neighbor_overlap function see many neighbors shared two graphs: change graph, overlap 100%. details can found hubness vignette ambitious dataset covered FMNIST article.","code":"iris_odd_nn <- rnnd_query(   index = iris_index,   query = iris_odd,   k = 5 ) lapply(iris_odd_nn, function(x) {   head(x, 3) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    9   14   20    4   25 #> [2,]   24    2   23   15    1 #> [3,]   19    9    4   14   20 #>  #> $dist #>           [,1]      [,2]      [,3]      [,4]      [,5] #> [1,] 0.1000000 0.1414213 0.1414213 0.1732050 0.2236068 #> [2,] 0.1414213 0.2449490 0.2645753 0.3000001 0.3000002 #> [3,] 0.1414213 0.1732050 0.2236066 0.2449488 0.2449488 iris_knn_improved <- rnnd_query(   index = iris_index,   query = iris_even,   init = iris_index$graph,   k = 5 ) c(   sum(iris_index$graph$dist),   sum(iris_knn_improved$dist) ) #> [1] 124.3317 124.3317 neighbor_overlap(iris_index$graph, iris_knn_improved) #> [1] 1"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"parallelism","dir":"Articles","previous_headings":"","what":"Parallelism","title":"rnndescent","text":"rnndescent multi-threaded, default single-threaded. Set n_threads set number threads want use:","code":"iris_index <- rnnd_build(data = iris_even, k = 5, n_threads = 2)"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"available-metrics","dir":"Articles","previous_headings":"","what":"Available Metrics","title":"rnndescent","text":"Several different distances available rnndescent beyond typically-supported Euclidean Cosine-based distances nearest neighbor packages. See metrics vignette details.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"supported-data-types","dir":"Articles","previous_headings":"","what":"Supported Data Types","title":"rnndescent","text":"Dense matrices data frames. Sparse matrices, dgCMatrix. distances supported dense matrices. Additionally, dense binary data, supply logical matrix, certain distances intended binary data, specialized functions used speed computation.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"parameters","dir":"Articles","previous_headings":"","what":"Parameters","title":"rnndescent","text":"several options rnnd_build rnnd_query expose can modified change behavior different stages algorithm. See documentation functions (e.g. ?rnnd_build) Random Partition Forests, Nearest Neighbor Descent Querying Data vignettes details.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"this-is-not-a-vignette","dir":"Articles","previous_headings":"","what":"This is not a Vignette","title":"Sparse TF-IDF Example","text":"need install load lot packages vignette won’t show output unless ’s interesting relevant task hand, get similar results install packages run code.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Sparse TF-IDF Example","text":"Let’s install packages: load :","code":"install.packages(c(   \"tm\",   \"slam\",   \"janeaustenr\",   \"dplyr\",   \"uwot\",   \"Matrix\",   \"RSpectra\",   \"RColorBrewer\",   \"ggplot2\" )) library(tm) library(slam) library(janeaustenr) library(dplyr) library(rnndescent) library(uwot) library(Matrix) library(RSpectra) library(RColorBrewer) library(ggplot2)"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"preprocessing-the-text-data","dir":"Articles","previous_headings":"","what":"Preprocessing the Text Data","title":"Sparse TF-IDF Example","text":"Let’s start getting data. austen_books() function returns tibble text Jane Austen’s six novels, 70 characters worth text per row. book factor column tells us book line text text contains text . Ok, ’s lot text rows lot pre-amble. Let’s take look end (spoiler alerts end Persuasion): Alright, can expect lot blank rows. ’d like bit text per entry (also reduces number rows need process neighbor search). Let’s group rows chunks 10 collapse . want experiment , can change n_chunks value .","code":"data <- austen_books() dim(data) [1] 73422     2 head(data, 10) # A tibble: 10 × 2    text                    book                   <chr>                   <fct>                1 \"SENSE AND SENSIBILITY\" Sense & Sensibility  2 \"\"                      Sense & Sensibility  3 \"by Jane Austen\"        Sense & Sensibility  4 \"\"                      Sense & Sensibility  5 \"(1811)\"                Sense & Sensibility  6 \"\"                      Sense & Sensibility  7 \"\"                      Sense & Sensibility  8 \"\"                      Sense & Sensibility  9 \"\"                      Sense & Sensibility 10 \"CHAPTER 1\"             Sense & Sensibility tail(data, 10) # A tibble: 10 × 2    text                                                                      book          <chr>                                                                     <fct>       1 \"affection.  His profession was all that could ever make her friends\"     Persuasion  2 \"wish that tenderness less, the dread of a future war all that could dim\" Persuasion  3 \"her sunshine.  She gloried in being a sailor's wife, but she must pay\"   Persuasion  4 \"the tax of quick alarm for belonging to that profession which is, if\"    Persuasion  5 \"possible, more distinguished in its domestic virtues than in its\"        Persuasion  6 \"national importance.\"                                                    Persuasion  7 \"\"                                                                        Persuasion  8 \"\"                                                                        Persuasion  9 \"\"                                                                        Persuasion 10 \"Finis\"                                                                   Persuasion n_chunks <- 10 data_processed <- data |>   group_by(`book`) |>   mutate(`row_group` = ceiling(row_number() / n_chunks)) |>   group_by(`book`, `row_group`) |>   summarise(text = paste(`text`, collapse = \" \")) |>   ungroup() dim(data_processed) [1] 7344    3 head(data_processed) # A tibble: 6 × 3   book                row_group text                                                                                                                                 <fct>                   <dbl> <chr>                                                                                                                              1 Sense & Sensibility         1 \"SENSE AND SENSIBILITY  by Jane Austen  (1811)     CHAPTER 1\"                                                                      2 Sense & Sensibility         2 \"  The family of Dashwood had long been settled in Sussex.  Their estate was large, and their residence was at Norland Park, in t… 3 Sense & Sensibility         3 \"alteration in his home; for to supply her loss, he invited and received into his house the family of his nephew Mr. Henry Dashwo… 4 Sense & Sensibility         4 \" By a former marriage, Mr. Henry Dashwood had one son: by his present lady, three daughters.  The son, a steady respectable youn… 5 Sense & Sensibility         5 \"father only seven thousand pounds in his own disposal; for the remaining moiety of his first wife's fortune was also secured to … 6 Sense & Sensibility         6 \"son's son, a child of four years old, it was secured, in such a way, as to leave to himself no power of providing for those who …"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"text-analysis","dir":"Articles","previous_headings":"","what":"Text Analysis","title":"Sparse TF-IDF Example","text":"Next, must make decisions text pre-processing. shouldn’t take decisions gold standard text processing. good news can experiment different approaches visualizing results UMAP provide feedback. Now can create TF-IDF matrix: nearly 20,000 columns. ’s lot dimensions. lot downstream analysis dimensionality reduction typical workflow involve: Potentially converting dense representation. 7,000 rows huge memory burden. decided use original data, now matrix 10 times size. linear dimensionality reduction dense representation, e.g. Truncated SVD hundred () dimensions. maybe random projection? lot cases, ways sparse input wouldn’t necessarily need densify initial matrix. Nonetheless, involves making decision number dimensions keep, need track sort information loss involved decision, spend time dimensionality reduction, ’s data keep track . rnndescent helps able generate neighbor graph sparse representation directly.","code":"corpus <-   Corpus(VectorSource(data_processed$text)) |>   tm_map(content_transformer(tolower)) |>   tm_map(removePunctuation) |>   tm_map(removeNumbers) |>   tm_map(removeWords, stopwords(\"english\")) |>   tm_map(stripWhitespace) tfidf <- weightTfIdf(DocumentTermMatrix(corpus)) dim(tfidf) [1]  7344 18853"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"some-more-preprocessing","dir":"Articles","previous_headings":"","what":"Some More Preprocessing","title":"Sparse TF-IDF Example","text":"Just go , rnndescent works dgCMatrix need convert tf-idf matrix: normalize rows. Sadly, apply doesn’t work well sparse matrices, bit creative. L2 normalization looks like: ’m going use divergence-based distance metric assumes row probability distribution L1 normalization requires even creativity. gist privefl guide :","code":"tfidf_sp <-   sparseMatrix(     i = tfidf$i,     j = tfidf$j,     x = tfidf$v,     dims = dim(tfidf)   ) tfidf_spl2 <-   Diagonal(x = 1 / sqrt(rowSums(tfidf_sp^2))) %*% tfidf_sp l1_normalize <- function(X) {   res <- rep(0, nrow(X))   dgt <- as(X, \"TsparseMatrix\")   tmp <- tapply(dgt@x, dgt@i, function(x) {     sum(abs(x))   })   res[as.integer(names(tmp)) + 1] <- tmp   Diagonal(x = 1 / res) %*% X } tfidf_spl1 <- l1_normalize(tfidf_sp) head(rowSums(tfidf_spl1)) [1] 1 1 1 1 1 1"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"nearest-neighbor-search","dir":"Articles","previous_headings":"","what":"Nearest Neighbor Search","title":"Sparse TF-IDF Example","text":"7000 rows, can brute force search get exact nearest neighbors. takes just minute 6 threads 8th-gen Intel i7 laptop longer cutting-edge.","code":"tfidfl1_js_bf <-   brute_force_knn(     tfidf_spl1,     k = 15,     metric = \"jensenshannon\",     n_threads = 6,     verbose = TRUE   ) 10:19:28 Calculating brute force k-nearest neighbors with k = 15 using 6 threads 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** 10:20:52 Finished"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"hubness","dir":"Articles","previous_headings":"","what":"Hubness","title":"Sparse TF-IDF Example","text":"always strongly recommend looking distribution edges throughout graph, specifically looking hubness. First generate k-occurrences: telling us popularity items dataset, .e. every item 15 neighbors, items tend show neighbor list items average? item k-occurrence higher 15 ’s popular average, vice versa. item k-occurrence 1 called anti-hub, item never neighbor item (one edge ). Conversely, can see items popular, k-occurrence ten times higher average. definitely considered hubs. ’s k-occurrences top 6 popular items: many anti-hubs : 129 pieces text corpus considered top-15 neighbors piece text. quite 2% dataset. Note values get hubness highly dependent things like dataset scaled, distance use obviously number neighbors. case, ’re pretty ok. Using Euclidean distance high-dimensional data tends produce large numbers hubs, can really distort downstream analysis ’s important check .","code":"k_occurrences <- k_occur(tfidfl1_js_bf) summary(k_occurrences) Min. 1st Qu.  Median    Mean 3rd Qu.    Max.        1       6      11      15      19     167 tail(sort(k_occurrences)) [1] 129 137 138 152 158 167 sum(k_occurrences == 1) [1] 129"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"hub-and-hubbability","dir":"Articles","previous_headings":"","what":"Hub and Hubbability","title":"Sparse TF-IDF Example","text":"Perhaps can learn something dataset taking closer look hubs anti-hubs. hubs represent items seen similar lots items, imagine items represent Jane Austen-like pieces text dataset. Meanwhile anti-hubs must less usual prose way. Let’s look popular text first.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"popular-text","dir":"Articles","previous_headings":"Hub and Hubbability","what":"Popular Text","title":"Sparse TF-IDF Example","text":"’s full text popular item: dialog contain word “love” lot . without looking text, ’s hard say distinguishing characteristic text. let’s now look unpopular items.","code":"popular_indices <- head(order(k_occurrences, decreasing = TRUE)) data_processed[popular_indices, c(\"book\", \"text\")] # A tibble: 6 × 2   book              text                                                                                             <fct>             <chr>                                                                                          1 Mansfield Park    \"You will be what you ought to be to her. I hope it does not distress you very much, Fanny?\\\"… 2 Emma              \"know any body was coming. 'It is only Mrs. Cole,' said I, 'depend upon it. Nobody else would… 3 Pride & Prejudice \"Jane? Shall you like to have such a brother?\\\"  \\\"Very, very much. Nothing could give either… 4 Emma              \" \\\"Perhaps she might; but it is not every man's fate to marry the woman who loves him best. … 5 Northanger Abbey  \" \\\"Henry!\\\" she replied with a smile. \\\"Yes, he does dance very well.\\\"  \\\"He must have thou… 6 Pride & Prejudice \" \\\"I wish you joy. If you love Mr. Darcy half as well as I do my dear Wickham, you must be v… data_processed[popular_indices[1], ]$text [1] \"You will be what you ought to be to her. I hope it does not distress you very much, Fanny?\\\"  \\\"Indeed it does: I cannot like it. I love this house and everything in it: I shall love nothing there. You know how uncomfortable I feel with her.\\\"  \\\"I can say nothing for her manner to you as a child; but it was the same with us all, or nearly so. She never knew how to be pleasant to children. But you are now of an age to be treated better; I think she is\""},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"unpopular-text","dir":"Articles","previous_headings":"Hub and Hubbability","what":"Unpopular Text","title":"Sparse TF-IDF Example","text":"terms anti-hubs, large number , let’s sample 6 random: ’s full text randomly selected item: ’s definitely different flavor text anti-hubs see compared hubs, small fraction anti-hubs. least isn’t just pulling passages Northanger Abbey.","code":"unpopular_indices <- which(k_occurrences == 1) data_processed[sample(unpopular_indices, 6), c(\"book\", \"text\")] # A tibble: 6 × 2   book                text                                                                                                                                     <fct>               <chr>                                                                                                                                  1 Emma                \"a letter, among the thousands that are constantly passing about the kingdom, is even carried wrong--and not one in a million, I supp… 2 Northanger Abbey    \"Catherine heard neither the particulars nor the result. Her companion's discourse now sunk from its hitherto animated pitch to nothi… 3 Mansfield Park      \"whose distress she thought of most. Julia's elopement could affect her comparatively but little; she was amazed and shocked; but it … 4 Persuasion          \"tried, more fixed in a knowledge of each other's character, truth, and attachment; more equal to act, more justified in acting.  And… 5 Sense & Sensibility \"but not on picturesque principles.  I do not like crooked, twisted, blasted trees.  I admire them much more if they are tall, straig… 6 Sense & Sensibility \"and though a sigh sometimes escaped her, it never passed away without the atonement of a smile.  After dinner she would try her pian… data_processed[sample(unpopular_indices, 1),]$text [1] \"unwelcome, most ill-timed, most appalling! Mr. Yates might consider it only as a vexatious interruption for the evening, and Mr. Rushworth might imagine it a blessing; but every other heart was sinking under some degree of self-condemnation or undefined alarm, every other heart was suggesting, \\\"What will become of us? what is to be done now?\\\" It was a terrible pause; and terrible to every ear were the corroborating sounds of opening doors and passing footsteps.  Julia was the first to move and speak again. Jealousy and bitterness had been suspended: selfishness was lost in the common cause; but at the\""},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"popular-and-unpopular-words","dir":"Articles","previous_headings":"Hub and Hubbability","what":"Popular and Unpopular Words","title":"Sparse TF-IDF Example","text":"’s entirely obvious difference popular unpopular texts. Maybe can look average tf-idf value words least popular texts? check makes sense , let’s just see popular words Pride & Prejudice method: actually correponds ok results tidytext vignette: ’s mainly character names (, er, 'said'). apply dataset whole: Now generic words, although said still . Way go, said. popular words look like (said ?): ’s love right top. said seen, lot overlap average words. Onto unpopular text. bit overlap average words , Tolstoy said unhappy families? wouldn’t necessarily expect theme emerge chunks text nothing common anything else. scientific proof want sound like Jane Austen, write “love”. “think” “feel”. don’t want sound like classic Jane Austen, sprinkle “every” “long”. Also “Harriet”. Good luck.","code":"pap <- tfidf_sp[data_processed$book == \"Pride & Prejudice\", ] mean_pap <- colMeans(pap) names(mean_pap) <- colnames(tfidf) mean_pap[mean_pap > 0] |>   sort(decreasing = TRUE) |>   head(10) elizabeth      darcy     bennet    bingley       jane       will       said    wickham    collins      lydia  0.04253865 0.03290232 0.02795931 0.02545364 0.01991761 0.01847970 0.01713893 0.01687501 0.01687119 0.01438015 mean_tfidf <- colMeans(tfidf_sp) names(mean_tfidf) <- colnames(tfidf) mean_tfidf[mean_tfidf > 0] |>   sort(decreasing = TRUE) |>   head(10) will        mrs       said       must       miss       much        one      think       know      never  0.01630534 0.01513324 0.01446493 0.01388111 0.01355313 0.01307697 0.01266730 0.01200490 0.01169455 0.01152472 mean_popular_tfidf <- colMeans(tfidf_sp[popular_indices, ]) names(mean_popular_tfidf) <- colnames(tfidf) mean_popular_tfidf[mean_popular_tfidf > 0] %>%   sort(decreasing = TRUE) %>%   head(10) love      quite       well       feel      think       like      shall       sure    nothing        yes  0.08188918 0.07007464 0.06825873 0.06808318 0.06540969 0.06223084 0.06055058 0.05710714 0.05624428 0.05375587 mean_unpopular_tfidf <- colMeans(tfidf_sp[unpopular_indices, ]) names(mean_unpopular_tfidf) <- colnames(tfidf) mean_unpopular_tfidf[mean_unpopular_tfidf > 0] |>   sort(decreasing = TRUE) |>   head(10) every        much        mind       might        must     harriet     without         one        long       first  0.014918830 0.011596126 0.011479893 0.011270905 0.011188771 0.010718795 0.010047623 0.009764715 0.009650010 0.009499682"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"umap","dir":"Articles","previous_headings":"","what":"UMAP","title":"Sparse TF-IDF Example","text":"can also visualize neighbor graph carrying dimensionality reduction using UMAP via uwot package. attempt place nearest neighbors close together 2D scatter plot. uwotdoesn’t need input information k-nearest neighbor graph, pass nn_method parameter. also requires explicitly setting usual data input parameter X NULL. also setting density scaling 0.5, attempts re-add relative density clusters data back embedding using LEOPOLD approximation densMAP. UMAP uses random number generator part contrastive learning style optimization, purposely setting random seed get different results . way avoid -interpreting layout clusters. recommend running algorithm times seeing changes.","code":"js_umap <-   umap(     X = NULL,     nn_method = tfidfl1_js_bf,     n_sgd_threads = 6,     n_epochs = 1000,     batch = TRUE,     init_sdev = \"range\",     dens_scale = 0.5,     verbose = TRUE   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"plot","dir":"Articles","previous_headings":"","what":"Plot","title":"Sparse TF-IDF Example","text":"Let’s plot got: UMAP plot Hey, looks pretty good! can see texts book mostly clustered together. clusters “Persuasion” “Northanger Abbey” smaller books. terms relative position clusters, must take care -interpret . re-run UMAP multiple times clusters move , “Pride & Prejudice” “Sense & Sensibility” usually end together.","code":"ggplot(   data.frame(js_umap, Book = data_processed$book),   aes(x = X1, y = X2, color = Book) ) +   geom_point(alpha = 0.6, size = 1) +   scale_color_brewer(palette = \"Set2\") +   theme_minimal() +   labs(     title = \"Jensen-Shannon Divergence UMAP\",     x = \"\",     y = \"\",     color = \"Book\"   ) +   theme(legend.position = \"bottom\") +   guides(color = guide_legend(override.aes = list(size = 5)))"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"neighbor-preservation","dir":"Articles","previous_headings":"Plot","what":"Neighbor Preservation","title":"Sparse TF-IDF Example","text":"get quantitative sense well reduction preserved high-dimensional structure, can calculate nearest neighbors 2D space compare nearest neighbors high-dimensional space. two dimensions, brute force search fast. UMAP uses Euclidean distance output embedding match similarities input space, use metric = \"euclidean\" rather \"jensenshannon\": Now can measure overlap two neighbor graphs: average 16% neighbor list retained embedding 2D. Although doesn’t sound like much, actually pretty standard UMAP (maybe bit better normal). numbers going vary bit , hopefully much.","code":"umap_nbrs <-   brute_force_knn(js_umap,     k = 15,     n_threads = 6,     metric = \"euclidean\"   ) neighbor_overlap(umap_nbrs, tfidfl1_js_bf) 0.1604303"},{"path":"https://jlmelville.github.io/rnndescent/articles/sparse-example.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Sparse TF-IDF Example","text":"Whatever think text analysis skills, point rnndescent can used handle sparse data easily dense data, makes working lot simpler.","code":""},{"path":"https://jlmelville.github.io/rnndescent/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"James Melville. Author, maintainer, copyright holder. Vitalie Spinu. Contributor.","code":""},{"path":"https://jlmelville.github.io/rnndescent/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Melville J (2023). rnndescent: Nearest Neighbor Descent Method Approximate Nearest Neighbors. R package version 0.1.3,  https://github.com/jlmelville/rnndescent, https://jlmelville.github.io/rnndescent/.","code":"@Manual{,   title = {rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors},   author = {James Melville},   year = {2023},   note = {R package version 0.1.3,  https://github.com/jlmelville/rnndescent},   url = {https://jlmelville.github.io/rnndescent/}, }"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"rnndescent","dir":"","previous_headings":"","what":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"R package finding approximate nearest neighbors, translated Python package PyNNDescent written great Leland McInnes. name suggests, uses Nearest Neighbor Descent method (Dong et al., 2011), also makes use Random Partition Trees (Dasgupta Freund, 2008) well ideas FANNG NGT. can use rnndescent : optimizing initial set nearest neighbors, e.g. generated RcppAnnoy RcppHNSW. using package nearest neighbor search … … including finding nearest neighbors sparse data, packages R ecosystem . much larger number metrics packages.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"See Get Started article basics. vignettes go detail.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"current-status","dir":"","previous_headings":"","what":"Current Status","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"08 December 2023: Version 0.1.3 released. deals UBSAN ASAN problems missing data present k-nearest neighbors graph.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"cran","dir":"","previous_headings":"Installation","what":"CRAN","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"","code":"install.packages(\"rnndescent\")"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"development-version","dir":"","previous_headings":"Installation","what":"Development Version","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"packages makes use C++ code must compiled. may carry extra steps able build: Windows: install Rtools ensure C:\\Rtools\\bin path. Mac OS X: using custom ~/.R/Makevars may cause linking errors. sort thing potential problem platforms seems bite Mac owners . R Mac OS X FAQ may helpful work can get away . safe side, advise building without custom Makevars. rnndescent uses C++17. shouldn’t big problem R platforms support (sorry affects ).","code":"# install.packages(\"pak\") pak::pkg_install(\"jlmelville/rnndescent\")"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"details, please see documentation.","code":"library(rnndescent)  # Find 15-knn iris_knn <- rnnd_knn(iris, k = 15)  # Build an index iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] # Specify the number of neighbors you are likely to want to query for iris_index <- rnnd_build(iris_index, k = 15)  # Query then index iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_query_nn <- rnnd_query(index = iris_index, query = iris_odd, k = 15)"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"supported-metrics","dir":"","previous_headings":"","what":"Supported Metrics","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Many. See metrics article list.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"missing-features","dir":"","previous_headings":"","what":"Missing Features","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Compared PyNNDescent, rnndescent currently lacking, decreasing order likelihood implementation: parallel batch queries currently supported. means trying stream queries, querying one item time, get parallelism. index always passed C++ R layers building index querying. useful portability easy serialize index (can use saveRDS like R data example), ’s efficient. Keeping index R-wrapped C++ class downsides fix . index can also get large large (high-dimensional) datasets. distance metrics. large number currently supported though. See Missing Metrics currently available. Custom metrics. just isn’t feasible C++ implementation. issues around index serialization parallel behavior make rnndescent currently unsuitable streaming applications querying one item time. batch queries, querying multiple items , rnndescent fine: example, generating nearest neighbors UMAP (maybe use uwot). Dimensionality reduction personal use case nearest neighbors calculation like get rnndescent onto CRAN useful--something state. result targeting initial release support streaming case. like fix subsequent release. Also specialized distance code take advantage AVX etc., rnndescent going run slower packages. wouldn’t allowed CRAN anyway, might nice--installing github.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World wide web (pp. 577-586). ACM. doi.org/10.1145/1963405.1963487.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"R package whole licensed GPLv3 later. following files licensed differently: inst/include/dqsample.h modification sampling code dqrng AGPLv3 later. inst/include/RcppPerpendicular.h modification code RcppParallel GPLv2 later underlying nearest neighbor descent C++ library, can found inst/include/tdoann, licensed BSD 2-Clause. far know, licenses compatible re-licensing GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"missing-metrics","dir":"","previous_headings":"","what":"Missing Metrics","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"following metrics PyNNDescent supported rnndescent: Circular Kantorovich Haversine Kantorovich Mahalanobis Minkowski Sinkhorn Standardised Euclidean Wasserstein 1d Weighted Minkowski require passing extra information part metric definition, currently supported.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"see-also","dir":"","previous_headings":"","what":"See Also","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"PyNNDescent, Python implementation. nndescent, C++ implementation. NearestNeighborDescent.jl, Julia implementation. NNDescent.cpp, another C++ implementation. nndescent, another C++ implementation, Python bindings.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find exact nearest neighbors by brute force — brute_force_knn","title":"Find exact nearest neighbors by brute force — brute_force_knn","text":"Returns exact nearest neighbors dataset. brute force search carried : possible pairs points compared, nearest neighbors returned.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find exact nearest neighbors by brute force — brute_force_knn","text":"","code":"brute_force_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find exact nearest neighbors by brute force — brute_force_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find exact nearest neighbors by brute force — brute_force_knn","text":"nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find exact nearest neighbors by brute force — brute_force_knn","text":"method accurate scales poorly dataset size, use caution larger datasets. exact neighbors ground truth compare approximate results useful benchmarking determining parameter settings approximate methods.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find exact nearest neighbors by brute force — brute_force_knn","text":"","code":"# Find the 4 nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- brute_force_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- brute_force_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- brute_force_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- brute_force_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 06:45:26 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:26 Calculating brute force k-nearest neighbors with k = 4 #> 06:45:26 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query exact nearest neighbors by brute force — brute_force_knn_query","title":"Query exact nearest neighbors by brute force — brute_force_knn_query","text":"Returns exact nearest neighbors query data reference data. brute force search carried : possible pairs reference query points compared, nearest neighbors returned.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query exact nearest neighbors by brute force — brute_force_knn_query","text":"","code":"brute_force_knn_query(   query,   reference,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query exact nearest neighbors by brute force — brute_force_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed format orientation reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query exact nearest neighbors by brute force — brute_force_knn_query","text":"nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices reference. dist n k matrix containing nearest neighbor distances items reference.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query exact nearest neighbors by brute force — brute_force_knn_query","text":"accurate scales poorly dataset size, use caution larger datasets. exact neighbors ground truth compare approximate results useful benchmarking determining parameter settings approximate methods.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query exact nearest neighbors by brute force — brute_force_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # For each item in iris_query find the 4 nearest neighbors in iris_ref # If you pass a data frame, non-numeric columns are removed # set verbose = TRUE to get details on the progress being made iris_query_nn <- brute_force_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 06:45:27 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:27 Calculating brute force k-nearest neighbors from reference with k = 4 #> 06:45:27 Finished  # Manhattan (l1) distance iris_query_nn <- brute_force_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"manhattan\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a search graph for nearest neighbors — graph_knn_query","title":"Query a search graph for nearest neighbors — graph_knn_query","text":"Run queries search graph, return nearest neighbors taken reference data used build graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a search graph for nearest neighbors — graph_knn_query","text":"","code":"graph_knn_query(   query,   reference,   reference_graph,   k = NULL,   metric = \"euclidean\",   init = NULL,   epsilon = 0.1,   max_search_fraction = 1,   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a search graph for nearest neighbors — graph_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed format orientation reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. reference_graph Search graph reference data. neighbor graph, output nnd_knn() can used, preferably suitably prepared sparse search graph used, output prepare_search_graph(). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" init Initial query neighbor graph optimize. provided, k random neighbors created. provided, input format must one : list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. input distances omitted, calculated . random projection forest, returned rpf_build() rpf_knn() ret_forest = TRUE. epsilon Controls trade-accuracy search cost, described Iwasaki Miyazaki (2018), specifying distance tolerance whether explore neighbors candidate points. larger value, neighbors searched. value 0.1 allows query-candidate distances 10% larger current -distant neighbor query point, 0.2 means 20%, . Suggested values 0-0.5, although value highly dependent distribution distances dataset (higher dimensional data choose smaller cutoff). large value epsilon result query search approaching brute force comparison. Use parameter conjunction max_search_fraction prepare_search_graph() prevent excessive run time. Default 0.1. set verbose = TRUE, statistics number distance calculations logged can help tune epsilon. max_search_fraction Maximum fraction reference data search. value 0 (search none reference data) 1 (search data necessary). works conjunction epsilon terminate search early specified fraction reference data searched. Default 1. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. search forest used initialization via init parameter, metric fetched setting ignored. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query a search graph for nearest neighbors — graph_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices specifying row neighbor reference. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query a search graph for nearest neighbors — graph_knn_query","text":"greedy beam search used query graph, combining two search pruning strategies. first, due Iwasaki Miyazaki (2018), considers new candidates within relative distance current furthest neighbor query's graph. second, due Harwood Drummond (2016), puts limit absolute number distance calculations carry . See epsilon max_search_fraction parameters respectively.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Query a search graph for nearest neighbors — graph_knn_query","text":"Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722). Iwasaki, M., & Miyazaki, D. (2018). Optimization indexing based k-nearest neighbor graph proximity search high-dimensional data. arXiv preprint arXiv:1810.07355.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query a search graph for nearest neighbors — graph_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # First, find the approximate 4-nearest neighbor graph for the references: iris_ref_graph <- nnd_knn(iris_ref, k = 4)  # For each item in iris_query find the 4 nearest neighbors in iris_ref. # You need to pass both the reference data and the reference graph. # If you pass a data frame, non-numeric columns are removed. # set verbose = TRUE to get details on the progress being made iris_query_nn <- graph_knn_query(iris_query, iris_ref, iris_ref_graph,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 06:45:27 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:27 Initializing from random neighbors #> 06:45:27 Generating random k-nearest neighbor graph from reference with k = 4 #> 06:45:27 Searching nearest neighbor graph with epsilon = 0.1 and max_search_fraction = 1 #> 06:45:27 Finished  # A more complete example, converting the initial knn into a search graph # and using a filtered random projection forest to initialize the search # create initial knn and forest iris_ref_graph <- nnd_knn(iris_ref, k = 4, init = \"tree\", ret_forest = TRUE) # keep the best tree in the forest forest <- rpf_filter(iris_ref_graph, n_trees = 1) # expand the knn into a search graph iris_ref_search_graph <- prepare_search_graph(iris_ref, iris_ref_graph) # run the query with the improved graph and initialization iris_query_nn <- graph_knn_query(iris_query, iris_ref, iris_ref_search_graph,   init = forest, k = 4 )"},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantify hubness of a nearest neighbor graph — k_occur","title":"Quantify hubness of a nearest neighbor graph — k_occur","text":"k_occur returns vector k-occurrences nearest neighbor graph defined  Radovanovic co-workers (2010). k-occurrence object number times occurs among k-nearest neighbors objects dataset.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantify hubness of a nearest neighbor graph — k_occur","text":"","code":"k_occur(idx, k = NULL, include_self = TRUE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantify hubness of a nearest neighbor graph — k_occur","text":"idx integer matrix containing nearest neighbor indices, integers labeled starting 1. Note integer labels refer rows idx, example nearest neighbor result querying one set objects respect another (instance running graph_knn_query()). may also pass nearest neighbor graph object (e.g. output running nnd_knn()), indices extracted , sparse matrix format returned prepare_search_graph(). k number closest neighbors use. Must 1 number columns idx. default, columns idx used. Ignored idx sparse. include_self logical indicating whether label idx considered valid neighbor found row . default TRUE. can set FALSE labels idx refer row indices idx, case results nnd_knn(). case may want consider trivial case object neighbor . cases leave set TRUE.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quantify hubness of a nearest neighbor graph — k_occur","text":"vector length max(idx), containing number times object idx found nearest neighbor list objects represented row indices idx.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantify hubness of a nearest neighbor graph — k_occur","text":"k-occurrence can take values 0 size dataset. larger k-occurrence object, \"popular\" . large values k-occurrence (much larger k) indicates object \"hub\" also implies existence \"anti-hubs\": objects never appear k-nearest neighbors objects. presence hubs can reduce accuracy nearest-neighbor descent approximate nearest neighbor algorithms terms retrieving exact k-nearest neighbors. However appearance hubs can still detected approximate results, calculating k-occurrences output nearest neighbor descent useful diagnostic step.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Quantify hubness of a nearest neighbor graph — k_occur","text":"Radovanovic, M., Nanopoulos, ., & Ivanovic, M. (2010). Hubs space: Popular nearest neighbors high-dimensional data. Journal Machine Learning Research, 11, 2487-2531. https://www.jmlr.org/papers/v11/radovanovic10a.html Bratic, B., Houle, M. E., Kurbalija, V., Oria, V., & Radovanovic, M. (2019). Influence Hubness NN-Descent. International Journal Artificial Intelligence Tools, 28(06), 1960002. doi:10.1142/S0218213019600029","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantify hubness of a nearest neighbor graph — k_occur","text":"","code":"iris_nbrs <- brute_force_knn(iris, k = 15) iris_ko <- k_occur(iris_nbrs$idx) # items 42 and 107 are not in 15 nearest neighbors of any other members of # iris which(iris_ko == 1) # they are only their own nearest neighbor #> [1]  42 107 max(iris_ko) # most \"popular\" item appears on 29 15-nearest neighbor lists #> [1] 29 which(iris_ko == max(iris_ko)) # it's iris item 64 #> [1] 64 # with k = 15, a maximum k-occurrence = 29 ~= 1.9 * k, which is not a cause # for concern"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge multiple approximate nearest neighbors graphs — merge_knn","title":"Merge multiple approximate nearest neighbors graphs — merge_knn","text":"merge_knn takes list nearest neighbor graphs merges single graph, number neighbors first graph. useful combine results multiple different nearest neighbor searches: output least accurate accurate two input graphs, ideally accurate either.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge multiple approximate nearest neighbors graphs — merge_knn","text":"","code":"merge_knn(graphs, is_query = FALSE, n_threads = 0, verbose = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge multiple approximate nearest neighbors graphs — merge_knn","text":"graphs list nearest neighbor graphs merge. item list consist sub-list containing: idx n k matrix containing k nearest neighbor indices. dist n k matrix containing k nearest neighbor distances. number neighbors can differ graphs, merged result number neighbors first graph list. is_query TRUE graphs treated result knn query, knn building process. : graph bipartite? set TRUE nn_graphs results using e.g. graph_knn_query() random_knn_query(), set FALSE results nnd_knn() random_knn(). difference is_query = FALSE, index p found nn_graph1[, ], .e. p neighbor distance d, assumed neighbor p distance. is_query = TRUE, p indexes two different datasets symmetry hold. sure case applies , safe (potentially inefficient) set is_query = TRUE. n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge multiple approximate nearest neighbors graphs — merge_knn","text":"list containing: idx n k matrix containing merged nearest neighbor indices. dist n k matrix containing merged nearest neighbor distances. size k output graph first item nn_graphs.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge multiple approximate nearest neighbors graphs — merge_knn","text":"","code":"set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn3 <- nnd_knn(iris, k = 15, n_iters = 1)  # Merged results should be an improvement over individual results iris_mnn <- merge_knn(list(iris_rnn1, iris_rnn2, iris_rnn3)) sum(iris_mnn$dist) < sum(iris_rnn1$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn2$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn3$dist) #> [1] TRUE"},{"path":"https://jlmelville.github.io/rnndescent/reference/neighbor_overlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Overlap between the indices of two nearest neighbor graphs — neighbor_overlap","title":"Overlap between the indices of two nearest neighbor graphs — neighbor_overlap","text":"Calculates mean average number neighbors common two graphs. per-item overlap can also returned. function can useful measure accuracy approximation algorithms, exact nearest neighbors known, measure diversity two different approximate graphs.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/neighbor_overlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Overlap between the indices of two nearest neighbor graphs — neighbor_overlap","text":"","code":"neighbor_overlap(idx1, idx2, k = NULL, ret_vec = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/neighbor_overlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Overlap between the indices of two nearest neighbor graphs — neighbor_overlap","text":"idx1 Indices nearest neighbor graph, .e. matrix nearest neighbor indices. Can also list containing idx element. idx2 Indices nearest neighbor graph, .e. matrix nearest neighbor indices. Can also list containing idx element. considered ground truth. k Number neighbors consider. NULL, minimum number neighbors idx1 idx2 used. ret_vec TRUE, also return vector containing per-item overlap.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/neighbor_overlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Overlap between the indices of two nearest neighbor graphs — neighbor_overlap","text":"mean overlap idx1 idx2. ret_vec = TRUE, list containing mean overlap overlap item returned names mean overlaps, respectively.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/neighbor_overlap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Overlap between the indices of two nearest neighbor graphs — neighbor_overlap","text":"graph format returned e.g. nnd_knn() dimensions n k, n number points k number neighbors. pass neighbor graph directly, index matrix extracted present. two graphs different numbers neighbors, smaller number neighbors used.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/neighbor_overlap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Overlap between the indices of two nearest neighbor graphs — neighbor_overlap","text":"","code":"set.seed(1337) # Generate two random neighbor graphs for iris iris_rnn1 <- random_knn(iris, k = 15) iris_rnn2 <- random_knn(iris, k = 15)  # Overlap between the two graphs mean_overlap <- neighbor_overlap(iris_rnn1, iris_rnn2)  # Also get a vector of per-item overlap overlap_res <- neighbor_overlap(iris_rnn1, iris_rnn2, ret_vec = TRUE) summary(overlap_res$overlaps) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.0000  0.1333  0.2000  0.1871  0.2667  0.4000"},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find nearest neighbors using nearest neighbor descent — nnd_knn","title":"Find nearest neighbors using nearest neighbor descent — nnd_knn","text":"Uses Nearest Neighbor Descent method due Dong co-workers (2011) optimize approximate nearest neighbor graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find nearest neighbors using nearest neighbor descent — nnd_knn","text":"","code":"nnd_knn(   data,   k = NULL,   metric = \"euclidean\",   init = \"rand\",   init_args = NULL,   n_iters = NULL,   max_candidates = NULL,   delta = 0.001,   low_memory = TRUE,   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   progress = \"bar\",   obs = \"R\",   ret_forest = FALSE )"},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find nearest neighbors using nearest neighbor descent — nnd_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" init Name initialization strategy initial data neighbor graph optimize. One : \"rand\" random initialization (default). \"tree\" use random projection tree method Dasgupta Freund (2008). pre-calculated neighbor graph. list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. input distances omitted, calculated .' k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. init_args list containing arguments pass random partition forest initialization. See rpf_knn() possible arguments. avoid inconsistences tree calculation subsequent nearest neighbor descent optimization, attempt provide metric use_alt_metric option list ignored. n_iters Number iterations nearest neighbor descent carry . default, chosen based number observations data. max_candidates Maximum number candidate neighbors try item iteration. Use relative k emulate \"rho\" sampling parameter nearest neighbor descent paper. default, set k 60, whichever smaller. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1% neighbor graph must updated iteration. low_memory TRUE, use lower memory, computationally expensive approach index construction. set FALSE, see noticeable speed improvement, especially using smaller number threads, worth trying memory spare. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. progress Determines type progress information logged verbose = TRUE. Options : \"bar\": simple text progress bar. \"dist\": sum distances approximate knn graph end iteration. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage. ret_forest TRUE init = \"tree\" RP forest used initialize nearest neighbors returned nearest neighbor data. See Value section details. returned forest can used part initializing search new data: see rpf_knn_query() rpf_filter() details.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find nearest neighbors using nearest neighbor descent — nnd_knn","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. forest (init = \"tree\" ret_forest = TRUE ): RP forest used initialize neighbor data.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find nearest neighbors using nearest neighbor descent — nnd_knn","text":"initial graph provided, random graph generated, may also specify use graph generated forest random projection trees, using method Dasgupta Freund (2008).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find nearest neighbors using nearest neighbor descent — nnd_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). doi:10.1145/1374376.1374452 . Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 .","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find nearest neighbors using nearest neighbor descent — nnd_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- nnd_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- nnd_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 06:45:28 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:28 Initializing neighbors using 'rand' method #> 06:45:28 Generating random k-nearest neighbor graph with k = 4 #> 06:45:28 Running nearest neighbor descent for 7 iterations #> 06:45:28 Finished  # Nearest neighbor descent uses random initialization, but you can pass any # approximation using the init argument (as long as the metrics used to # calculate the initialization are compatible with the metric options used # by nnd_knn). iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\") iris_nn <- nnd_knn(iris, init = iris_nn, metric = \"euclidean\", verbose = TRUE) #> 06:45:28 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:28 Initializing from user-supplied graph #> 06:45:28 Applying metric correction to initial distances from 'euclidean' to 'sqeuclidean' #> 06:45:28 Running nearest neighbor descent for 7 iterations #> 06:45:28 Finished  # Number of iterations controls how much optimization is attempted. A smaller # value will run faster but give poorer results iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 2)  # You can also control the amount of work done within an iteration by # setting max_candidates iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", max_candidates = 50)  # Optimization may also stop early if not much progress is being made. This # convergence criterion can be controlled via delta. A larger value will # stop progress earlier. The verbose flag will provide some information if # convergence is occurring before all iterations are carried out. set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 5, delta = 0.5)  # To ensure that descent only stops if no improvements are made, set delta = 0 set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 5, delta = 0)  # A faster version of the algorithm is available that avoids repeated # distance calculations at the cost of using more RAM. Set low_memory to # FALSE to try it. set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", low_memory = FALSE)  # Using init = \"tree\" is usually more efficient than random initialization. # arguments to the tree initialization method can be passed via the init_args # list set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, init = \"tree\", init_args = list(n_trees = 5))"},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","title":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","text":"Create graph using existing nearest neighbor data balance search speed accuracy using occlusion pruning truncation strategies Harwood Drummond (2016). resulting search graph efficient querying new data original nearest neighbor graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","text":"","code":"prepare_search_graph(   data,   graph,   metric = \"euclidean\",   diversify_prob = 1,   pruning_degree_multiplier = 1.5,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","text":"data Matrix n items, observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). graph neighbor graph data, list containing: idx n k matrix containing nearest neighbor indices data data. dist n k matrix containing nearest neighbor distances. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" diversify_prob degree diversification search graph removing unnecessary edges occlusion pruning. take value 0 (diversification) 1 (remove many edges possible) treated probability neighbor removed found \"occlusion\". item p q, two members neighbor list item , closer , nearer neighbor p said \"occlude\" q. likely q neighbor list p need retain neighbor list . may also set NULL skip occlusion pruning. Note occlusion pruning carried twice, forward neighbors, reverse neighbors. Reducing value result dense graph. similar increasing \"alpha\" parameter used DiskAnn pruning method Subramanya co-workers (2014). pruning_degree_multiplier strongly truncate final neighbor list item. neighbor list item truncated retain closest d neighbors, d = k * pruning_degree_multiplier, k original number neighbors per item graph. Roughly, values larger 1 keep nearest neighbors item, plus given fraction reverse neighbors (exist). example, setting 1.5 keep forward neighbors half many reverse neighbors, although exactly neighbors retained also dependent occlusion pruning occurs. Set NULL skip step. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","text":"search graph data based graph, represented sparse matrix, suitable use graph_knn_query().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","text":"approximate nearest neighbor graph useful querying via graph_knn_query(), especially query data initialized randomly: items data set may nearest neighbor list item can therefore never returned neighbor, matter close query. Even appear least one neighbor list may reachable expanding arbitrary starting list neighbor graph contains disconnected components. Converting directed graph represented neighbor graph undirected graph adding edge item j edge exists j (.e. creating mutual neighbor graph) solves problems , can result inefficient searches. Although -degree item restricted number neighbors -degree restrictions: given item \"popular\" large number neighbors lists. Therefore mutualizing neighbor graph can result items large number neighbors search. usually similar neighborhoods nothing gained searching . balance accuracy search time, following procedure carried : graph \"diversified\" occlusion pruning. reverse graph formed reversing direction edges pruned graph. reverse graph diversified occlusion pruning. pruned forward pruned reverse graph merged. outdegree node merged graph truncated. truncated merged graph returned prepared search graph. Explicit zero distances graph converted small positive number avoid dropped sparse representation. one exception \"self\" distance, .e. edge graph links node (diagonal sparse distance matrix). trivial edges useful search purposes always dropped.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","text":"Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722). Jayaram Subramanya, S., Devvrit, F., Simhadri, H. V., Krishnawamy, R., & Kadekodi, R. (2019). Diskann: Fast accurate billion-point nearest neighbor search single node. Advances Neural Information Processing Systems, 32.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a nearest neighbor graph into a search graph — prepare_search_graph","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # First, find the approximate 4-nearest neighbor graph for the references: ref_ann_graph <- nnd_knn(iris_ref, k = 4)  # Create a graph for querying with ref_search_graph <- prepare_search_graph(iris_ref, ref_ann_graph)  # Using the search graph rather than the ref_ann_graph directly may give # more accurate or faster results iris_query_nn <- graph_knn_query(   query = iris_query, reference = iris_ref,   reference_graph = ref_search_graph, k = 4, metric = \"euclidean\",   verbose = TRUE ) #> 06:45:28 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:28 Initializing from random neighbors #> 06:45:28 Generating random k-nearest neighbor graph from reference with k = 4 #> 06:45:28 Searching nearest neighbor graph with epsilon = 0.1 and max_search_fraction = 1 #> 06:45:28 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find nearest neighbors by random selection — random_knn","title":"Find nearest neighbors by random selection — random_knn","text":"Create neighbor graph randomly selecting neighbors. useful nearest neighbor method , can used methods require initialization, nnd_knn().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find nearest neighbors by random selection — random_knn","text":"","code":"random_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   order_by_distance = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find nearest neighbors by random selection — random_knn","text":"data Matrix n items generate random neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. order_by_distance TRUE (default), results item returned increasing distance. need results sorted, e.g. going pass results initialization another routine like nnd_knn(), set FALSE save small amount computational time. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find nearest neighbors by random selection — random_knn","text":"random neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find nearest neighbors by random selection — random_knn","text":"","code":"# Find 4 random neighbors and calculate their Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- random_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- random_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 06:45:29 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:29 Generating random k-nearest neighbor graph with k = 4 #> 06:45:29 Finished  # These results can be improved by nearest neighbors descent. You don't need # to specify k here because this is worked out from the initial input iris_nn <- nnd_knn(iris, init = iris_nn, metric = \"euclidean\", verbose = TRUE) #> 06:45:29 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:29 Initializing from user-supplied graph #> 06:45:29 Applying metric correction to initial distances from 'euclidean' to 'sqeuclidean' #> 06:45:29 Running nearest neighbor descent for 7 iterations #> 06:45:29 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query nearest neighbors by random selection — random_knn_query","title":"Query nearest neighbors by random selection — random_knn_query","text":"Run queries reference data return randomly selected neighbors. useful query method , can used methods require initialization.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query nearest neighbors by random selection — random_knn_query","text":"","code":"random_knn_query(   query,   reference,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   order_by_distance = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query nearest neighbors by random selection — random_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries randomly selected data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed orientation format reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. order_by_distance TRUE (default), results item returned increasing distance. need results sorted, e.g. going pass results initialization another routine like graph_knn_query(), set FALSE save small amount computational time. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query nearest neighbors by random selection — random_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query nearest neighbors by random selection — random_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # For each item in iris_query find 4 random neighbors in iris_ref # If you pass a data frame, non-numeric columns are removed # set verbose = TRUE to get details on the progress being made iris_query_random_nbrs <- random_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 06:45:29 Using alt metric 'sqeuclidean' for 'euclidean' #> 06:45:29 Generating random k-nearest neighbor graph from reference with k = 4 #> 06:45:29 Finished  # Manhattan (l1) distance iris_query_random_nbrs <- random_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"manhattan\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":null,"dir":"Reference","previous_headings":"","what":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","title":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","text":"function builds approximate nearest neighbors graph convenient defaults, prepares index querying new data, later use rnnd_query(). control process, please see functions package.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","text":"","code":"rnnd_build(   data,   k = 30,   metric = \"euclidean\",   use_alt_metric = TRUE,   init = \"tree\",   n_trees = NULL,   leaf_size = NULL,   max_tree_depth = 200,   margin = \"auto\",   n_iters = NULL,   delta = 0.001,   max_candidates = NULL,   low_memory = TRUE,   n_search_trees = 1,   pruning_degree_multiplier = 1.5,   diversify_prob = 1,   n_threads = 0,   verbose = FALSE,   progress = \"bar\",   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors build index . can specify different number running rnnd_query, index calibrated using value recommended set k according likely number neighbors want retrieve. Optional init specified, case k can inferred init data. , specified version k take precedence. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. init Name initialization strategy initial data neighbor graph optimize. One : \"rand\" random initialization (default). \"tree\" use random projection tree method Dasgupta Freund (2008). pre-calculated neighbor graph. list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. input distances omitted, calculated .' k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. used init = \"tree\". leaf_size maximum number items can appear leaf. value chosen match expected number neighbors want retrieve running queries (e.g. want find 50 nearest neighbors set leaf_size = 50) set value smaller 10. used init = \"tree\". max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. used init = \"tree\". margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. used init = \"tree\". n_iters Number iterations nearest neighbor descent carry . default, chosen based number observations data. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1% neighbor graph must updated iteration. max_candidates Maximum number candidate neighbors try item iteration. Use relative k emulate \"rho\" sampling parameter nearest neighbor descent paper. default, set k 60, whichever smaller. low_memory TRUE, use lower memory, computationally expensive approach index construction. set FALSE, see noticeable speed improvement, especially using smaller number threads, worth trying memory spare. n_search_trees, number trees keep search forest part index preparation. default 1. pruning_degree_multiplier strongly truncate final neighbor list item. neighbor list item truncated retain closest d neighbors, d = k * pruning_degree_multiplier, k original number neighbors per item graph. Roughly, values larger 1 keep nearest neighbors item, plus given fraction reverse neighbors (exist). example, setting 1.5 keep forward neighbors half many reverse neighbors, although exactly neighbors retained also dependent occlusion pruning occurs. Set NULL skip step. diversify_prob degree diversification search graph removing unnecessary edges occlusion pruning. take value 0 (diversification) 1 (remove many edges possible) treated probability neighbor removed found \"occlusion\". item p q, two members neighbor list item , closer , nearer neighbor p said \"occlude\" q. likely q neighbor list p need retain neighbor list . may also set NULL skip occlusion pruning. Note occlusion pruning carried twice, forward neighbors, reverse neighbors. n_threads Number threads use. verbose TRUE, log information console. progress Determines type progress information logged nearest neighbor descent stage verbose = TRUE. Options : \"bar\": simple text progress bar. \"dist\": sum distances approximate knn graph end iteration. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","text":"approximate nearest neighbor index, list containing: graph k-nearest neighbor graph, list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. list items intended internal use functions rnnd_query().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","text":"process k-nearest neighbor graph construction using Random Projection Forests (Dasgupta Freund, 2008) initialization Nearest Neighbor Descent (Dong co-workers, 2011) refinement. Index preparation, uses graph diversification method Harwood Drummond (2016).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). doi:10.1145/1374376.1374452 . Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 . Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build approximate nearest neighbors index and neighbor graph — rnnd_build","text":"","code":"iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ]  # Find 4 (approximate) nearest neighbors using Euclidean distance iris_even_index <- rnnd_build(iris_even, k = 4) iris_odd_nbrs <- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find approximate nearest neighbors — rnnd_knn","title":"Find approximate nearest neighbors — rnnd_knn","text":"function builds approximate nearest neighbors graph provided data using convenient defaults. return index later querying, speed graph construction reduce size complexity return value.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find approximate nearest neighbors — rnnd_knn","text":"","code":"rnnd_knn(   data,   k = 30,   metric = \"euclidean\",   use_alt_metric = TRUE,   init = \"tree\",   n_trees = NULL,   leaf_size = NULL,   max_tree_depth = 200,   margin = \"auto\",   n_iters = NULL,   delta = 0.001,   max_candidates = NULL,   low_memory = TRUE,   n_threads = 0,   verbose = FALSE,   progress = \"bar\",   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find approximate nearest neighbors — rnnd_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. init Name initialization strategy initial data neighbor graph optimize. One : \"rand\" random initialization (default). \"tree\" use random projection tree method Dasgupta Freund (2008). pre-calculated neighbor graph. list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. input distances omitted, calculated .' k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. used init = \"tree\". leaf_size maximum number items can appear leaf. value chosen match expected number neighbors want retrieve running queries (e.g. want find 50 nearest neighbors set leaf_size = 50) set value smaller 10. used init = \"tree\". max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. used init = \"tree\". margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. used init = \"tree\". n_iters Number iterations nearest neighbor descent carry . default, chosen based number observations data. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1% neighbor graph must updated iteration. max_candidates Maximum number candidate neighbors try item iteration. Use relative k emulate \"rho\" sampling parameter nearest neighbor descent paper. default, set k 60, whichever smaller. low_memory TRUE, use lower memory, computationally expensive approach index construction. set FALSE, see noticeable speed improvement, especially using smaller number threads, worth trying memory spare. n_threads Number threads use. verbose TRUE, log information console. progress Determines type progress information logged nearest neighbor descent stage verbose = TRUE. Options : \"bar\": simple text progress bar. \"dist\": sum distances approximate knn graph end iteration. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find approximate nearest neighbors — rnnd_knn","text":"approximate nearest neighbor index, list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find approximate nearest neighbors — rnnd_knn","text":"process k-nearest neighbor graph construction using Random Projection Forests (Dasgupta Freund, 2008) initialization Nearest Neighbor Descent (Dong co-workers, 2011) refinement. sure want query new data compared rnnd_build() function advantage storing index, can large.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find approximate nearest neighbors — rnnd_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). doi:10.1145/1374376.1374452 . Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find approximate nearest neighbors — rnnd_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance iris_knn <- rnnd_knn(iris, k = 4)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query an index for approximate nearest neighbors — rnnd_query","title":"Query an index for approximate nearest neighbors — rnnd_query","text":"Takes nearest neighbor index produced rnnd_build() uses find nearest neighbors query set observations, using back-tracking search search size determined method Iwasaki Miyazaki (2018). control search effort, total number distance calculations can also bounded, similar method Harwood Drummond (2016).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query an index for approximate nearest neighbors — rnnd_query","text":"","code":"rnnd_query(   index,   query,   k = 30,   epsilon = 0.1,   max_search_fraction = 1,   init = NULL,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query an index for approximate nearest neighbors — rnnd_query","text":"index nearest neighbor index produced rnnd_build(). query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). Sparse non-sparse data mixed, data used build index sparse, query data must also sparse. vice versa. k Number nearest neighbors return. epsilon Controls trade-accuracy search cost, described Iwasaki Miyazaki (2018). Setting epsilon positive value specifies distance tolerance whether explore neighbors candidate points. larger value, neighbors searched. value 0.1 allows query-candidate distances 10% larger current -distant neighbor query point, 0.2 means 20%, . Suggested values 0-0.5, although value highly dependent distribution distances dataset (higher dimensional data choose smaller cutoff). large value epsilon result query search approaching brute force comparison. Use parameter conjunction max_search_fraction prevent excessive run time. Default 0.1. set verbose = TRUE, statistics number distance calculations logged can help tune epsilon. max_search_fraction Maximum fraction reference data search. value 0 (search none reference data) 1 (search data necessary). works conjunction epsilon terminate search early specified fraction reference data searched. Default 1. init optional matrix k initial nearest neighbors query point. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query an index for approximate nearest neighbors — rnnd_query","text":"approximate nearest neighbor index, list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Query an index for approximate nearest neighbors — rnnd_query","text":"Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722). Iwasaki, M., & Miyazaki, D. (2018). Optimization indexing based k-nearest neighbor graph proximity search high-dimensional data. arXiv preprint arXiv:1810.07355. https://arxiv.org/abs/1810.07355","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query an index for approximate nearest neighbors — rnnd_query","text":"","code":"iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ]  iris_even_index <- rnnd_build(iris_even, k = 4) iris_odd_nbrs <- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":null,"dir":"Reference","previous_headings":"","what":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"Nearest Neighbor Descent method finding approximate nearest neighbors Dong co-workers (2010) doi:10.1145/1963405.1963487 . Based 'Python' package 'PyNNDescent' https://github.com/lmcinnes/pynndescent.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"rnndescent package provides functions create approximate nearest neighbors using Nearest Neighbor Descent (Dong co-workers, 2010) Random Partition Tree (Dasgupta Freund, 2008) methods. comparison packages, offers metrics can used sparse matrices. querying new data, uses graph diversification methods (Harwood Drummond, 2016) back-tracking (Iwasakai Miyazaki, 2018) improve search performance. package also provides functions diagnose hubness nearest neighbor results (Radovanovic co-workers, 2010). library based heavily 'PyNNDescent' Python library. General resources: Website 'rnndescent' package: https://github.com/jlmelville/rnndescent Documentation 'rnndescent' package: https://jlmelville.github.io/rnndescent/ Website 'PyNNDescent' package: https://github.com/lmcinnes/pynndescent following functions provide main interface package, useful defaults: Find approximate nearest neighbors: rnnd_knn() Create search index query new neighbors: rnnd_build(). Query new neighbors (refine existing knn graph): rnnd_query(). diagnostic helper functions help explore structure graphs well approximation working: Find exact nearest neighbors: brute_force_knn(), brute_force_knn_query(). Merging graphs: merge_knn(). Hubness: k_occur(). Overlap/accuracy two neighbor graphs: neighbor_overlap(). lower-level functions also available want control rnnd_* functions provide: Find approximate nearest neighbors: rpf_knn(), nnd_knn(). Generating random neighbors: random_knn(), random_knn_query(). Building index: rpf_build(), rpf_filter(). Querying index new data: rpf_knn_query(), prepare_search_graph(), graph_knn_query().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). doi:10.1145/1374376.1374452 . Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 . Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722). Radovanovic, M., Nanopoulos, ., & Ivanovic, M. (2010). Hubs space: Popular nearest neighbors high-dimensional data. Journal Machine Learning Research, 11, 2487-2531. https://www.jmlr.org/papers/v11/radovanovic10a.html Iwasaki, M., & Miyazaki, D. (2018). Optimization indexing based k-nearest neighbor graph proximity search high-dimensional data. arXiv preprint arXiv:1810.07355. https://arxiv.org/abs/1810.07355","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"Maintainer: James Melville jlmelville@gmail.com [copyright holder] contributors: Vitalie Spinu [contributor]","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a random projection forest nearest neighbor index — rpf_build","title":"Create a random projection forest nearest neighbor index — rpf_build","text":"Builds \"forest\" Random Projection Trees (Dasgupta Freund, 2008), can later searched find approximate nearest neighbors.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a random projection forest nearest neighbor index — rpf_build","text":"","code":"rpf_build(   data,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_trees = NULL,   leaf_size = 10,   max_tree_depth = 200,   margin = \"auto\",   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a random projection forest nearest neighbor index — rpf_build","text":"data Matrix n items generate index , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" Note margin = \"explicit\", metric used determine whether \"angular\" \"Euclidean\" distance used measure distance split points tree. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"). Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. applies implicit margin method used. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. leaf_size maximum number items can appear leaf. value chosen match expected number neighbors want retrieve running queries (e.g. want find 50 nearest neighbors set leaf_size = 50) set value smaller 10. max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a random projection forest nearest neighbor index — rpf_build","text":"forest random projection trees list. tree forest list, intended examined manipulated user. normal R data type, can safely serialized deserialized base::saveRDS() base::readRDS(). use querying pass forest parameter rpf_knn_query(). forest store data passed build tree, going search forest, also need store data used build provide search.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a random projection forest nearest neighbor index — rpf_build","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). doi:10.1145/1374376.1374452 .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a random projection forest nearest neighbor index — rpf_build","text":"","code":"# Build a forest of 10 trees from the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_odd_forest <- rpf_build(iris_odd, n_trees = 10)  iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_even_nn <- rpf_knn_query(   query = iris_even, reference = iris_odd,   forest = iris_odd_forest, k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Keep the best trees in a random projection forest — rpf_filter","title":"Keep the best trees in a random projection forest — rpf_filter","text":"Reduce size random projection forest, scoring tree k-nearest neighbors graph. top N trees retained allows faster querying.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Keep the best trees in a random projection forest — rpf_filter","text":"","code":"rpf_filter(nn, forest = NULL, n_trees = 1, n_threads = 0, verbose = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Keep the best trees in a random projection forest — rpf_filter","text":"nn Nearest neighbor data dense list format. derived data used build forest. forest random partition forest, e.g. created rpf_build(), representing partitions underlying data reflected nn. convenient, parameter ignored nn list contains forest entry, e.g. running rpf_knn() nnd_knn() ret_forest = TRUE, forest value extracted nn. n_trees number trees retain. default best-scoring tree retained. n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Keep the best trees in a random projection forest — rpf_filter","text":"forest best scoring n_trees trees.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Keep the best trees in a random projection forest — rpf_filter","text":"Trees scored based well leaf reflects neighbors specified nearest neighbor data. best use accurate nearest neighbor data can need come directly searching forest: example, nearest neighbor data running nnd_knn() optimize neighbor data output RP Forest good choice. Rather rely RP Forest solely approximate nearest neighbor querying, probably cost-effective use small number trees initialize neighbor list use graph search via graph_knn_query().","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Keep the best trees in a random projection forest — rpf_filter","text":"","code":"# Build a knn with a forest of 10 trees using the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] # also return the forest with the knn rfknn <- rpf_knn(iris_odd, k = 15, n_trees = 10, ret_forest = TRUE)  # keep the best 2 trees: iris_odd_filtered_forest <- rpf_filter(rfknn)  # get some new data to search iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ]  # search with the filtered forest iris_even_nn <- rpf_knn_query(   query = iris_even, reference = iris_odd,   forest = iris_odd_filtered_forest, k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find nearest neighbors using a random projection forest — rpf_knn","title":"Find nearest neighbors using a random projection forest — rpf_knn","text":"Returns approximate k-nearest neighbor graph dataset searching multiple random projection trees, variant k-d trees originated Dasgupta Freund (2008).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find nearest neighbors using a random projection forest — rpf_knn","text":"","code":"rpf_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_trees = NULL,   leaf_size = NULL,   max_tree_depth = 200,   include_self = TRUE,   ret_forest = FALSE,   margin = \"auto\",   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find nearest neighbors using a random projection forest — rpf_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" Note margin = \"explicit\", metric used determine whether \"angular\" \"Euclidean\" distance used measure distance split points tree. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. leaf_size maximum number items can appear leaf. default NULL means number leaves chosen based number requested neighbors k. max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. include_self TRUE (default) item considered neighbor . Hence first nearest neighbor results item . convention many nearest neighbor methods software adopt, want use resulting knn graph function downstream applications compare methods, probably keep set TRUE. However, planning using result initialization another nearest neighbor method (e.g. nnd_knn()), set FALSE. ret_forest TRUE also return search forest can used future querying (via rpf_knn_query()) filtering (via rpf_filter()). default FALSE. Setting TRUE change output list nested (see Value section ). margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find nearest neighbors using a random projection forest — rpf_knn","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. forest (ret_forest = TRUE) RP forest generated neighbor graph, can used query new data. k neighbors per observation guaranteed found. Missing data represented index 0 distance NA.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find nearest neighbors using a random projection forest — rpf_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). doi:10.1145/1374376.1374452 .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find nearest neighbors using a random projection forest — rpf_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- rpf_knn(iris, k = 4, metric = \"euclidean\", leaf_size = 3)  # If you want to initialize another method (e.g. nearest neighbor descent) # with the result of the RP forest, then it's more efficient to skip # evaluating whether an item is a neighbor of itself by setting # `include_self = FALSE`: iris_rp <- rpf_knn(iris, k = 4, n_trees = 3, include_self = FALSE) # for future querying you may want to also return the RP forest: iris_rpf <- rpf_knn(iris,   k = 4, n_trees = 3, include_self = FALSE,   ret_forest = TRUE )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a random projection forest index for nearest neighbors — rpf_knn_query","title":"Query a random projection forest index for nearest neighbors — rpf_knn_query","text":"Run queries \"forest\" Random Projection Trees (Dasgupta Freund, 2008), return nearest neighbors taken reference data used build forest.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a random projection forest index for nearest neighbors — rpf_knn_query","text":"","code":"rpf_knn_query(   query,   reference,   forest,   k,   cache = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a random projection forest index for nearest neighbors — rpf_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data data used build forest. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed format orientation reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. forest random partition forest, created rpf_build(), representing partitions data reference. k Number nearest neighbors return. unlikely get good results choose value substantially larger value leaf_size used build forest. cache TRUE (default) candidate indices found leaves forest cached avoid recalculating distance repeatedly. incurs extra memory cost scales n_threads. Set FALSE disable distance caching. n_threads Number threads use. Note parallelism search done observations query trees forest. Thus single observation see speed-increasing n_threads. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query a random projection forest index for nearest neighbors — rpf_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. k neighbors per observation guaranteed found. Missing data represented index 0 distance NA.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Query a random projection forest index for nearest neighbors — rpf_knn_query","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). doi:10.1145/1374376.1374452 .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query a random projection forest index for nearest neighbors — rpf_knn_query","text":"","code":"# Build a forest of 10 trees from the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_odd_forest <- rpf_build(iris_odd, n_trees = 10)  iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_even_nn <- rpf_knn_query(   query = iris_even, reference = iris_odd,   forest = iris_odd_forest, k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-013","dir":"Changelog","previous_headings":"","what":"rnndescent 0.1.3","title":"rnndescent 0.1.3","text":"CRAN release: 2023-12-07 CRAN resubmission fix lingering UBSAN errors.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-2","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.1.2","text":"internal Address/Undefined Behavior Sanitizer fixes discovered CRAN checks 0.1.1 submission fixed.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-011","dir":"Changelog","previous_headings":"","what":"rnndescent 0.1.1","title":"rnndescent 0.1.1","text":"CRAN release: 2023-11-27 Initial CRAN submission.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"breaking-changes-0-0-16","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"rnndescent 0.0.16","text":"rnnd_build now always prepares search graph. rnnd_prepare function removed. option prepare search graph index building made sense interested k-nearest neighbor graph. Now rnnd_knn exists purpose (see ), logic index building substantially simplified. nn_to_sparse function removed. merge_knn function removed, merge_knnl renamed merge_knn. running e.g. merge_knn(nn1, nn2), must now use merge_knn(list(nn1, nn2)). Also parameter nn_graphs renamed graphs.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-16","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.16","text":"New function: rnnd_knn. Behaves lot like rnnd_build, returns knn graph index built. index can large size high dimensional large datasets, function useful care knn graph won’t ever want query new data. New function: neighbor_overlap. Measures overlap two knn graphs via shared indices. similar function used extensively vignettes may sufficient utility useful others. New parameter rnnd_query graph_knn_query: max_search_fraction. parameter controls maximum number nodes can searched querying. number nodes searched exceeds fraction total number nodes graph, search terminated. can used combination epsilon avoid excessive search times.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-16","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.16","text":"sparse spearmanr distance fixed. tree-building n_threads = 0, progress/interrupt monitoring occurring. can provide user-defined graph init parameter rnnd_query. rnnd_query: verbose = TRUE, summary min, max average number distance queries logged. can help tune epsilon max_search_fraction.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"breaking-changes-0-0-15","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"rnndescent 0.0.15","text":"Standalone distance functions removed. hadn’t expanded match distances available nearest neighbor functions, sparse support added. increase size package’s API even . may show another package. local_scale_nn removed, similar reasons removal standalone distance functions. remains localscale branch github repo. search graph returned prepare_search_graph now transposed. prevents repeatedly transpose inside every call graph_knn_query multiple queries made. need either regenerate saved search graphs transpose Matrix::t(search_graph).","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-15","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.15","text":"New functions: rnnd_build, rnnd_query rnnd_prepare. functions streamline process building k-nearest neighbor graph, preparing search graph querying .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"breaking-changes-0-0-14","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"rnndescent 0.0.14","text":"bhamming metric longer exists specialized metric. Instead, pass logical matrix data, reference query parameter (depending function) specify metric = \"hamming\" automatically get binary-specific version hamming metric. hamming bhamming metrics now normalized respect number features, consistent binary-style metrics (PyNNDescent). need old distances, multiply distance matrix number columns, e.g. something like: metric l2sqr renamed sqeuclidean consistent PyNNDescent.","code":"res <- brute_force_knn(X, metric = \"hamming\") res$dist <- res$dist * ncol(X)"},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-14","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.14","text":"Metrics? got ’em! metric parameter now accepts much larger number metrics. See rdoc full list supported metrics. Currently, metrics PyNNDescent don’t require extra parameters supported. number specialized binary metrics also expanded. New parameter rpf_knn rpf_build: max_tree_depth controls depth tree set 100 internally. default doubled 200 can now user-controlled. verbose = TRUE largest leaf forest exceeds leaf_size parameter, message warning logged indicates maximum tree depth exceeded. Increasing max_tree_depth may answer: ’s likely something unusual distribution distances dataset random initialization might better use time.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-13","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.13","text":"Sparse data now supported. Pass dgCMatrix data, reference query parameters usually use dense matrix data frame. cosine, euclidean, manhattan, hamming correlation available, alternative versions dense case, e.g. cosine-preprocess binary-specific bhamming dense data . new init option graph_knn_query: can now pass RP forest initialize , e.g. rpf_build, setting ret_forest = TRUE nnd_knn rpf_knn. may want cut size forest used initialization rpf_filter first, though (single tree may enough). also use metric data forest, setting metric (use_alt_metric) function ignored.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-13","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.13","text":"knn graph pass prepare_search_graph graph_knn_query contains missing data, longer cause error (still might best idea though).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-12","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.12","text":"New function: rpf_knn. Calculates approximate k-nearest neighbors using random partition forest. New function: rpf_build. Builds random partition forest. New function: rpf_knn_query. Queries random partition forest (built rpf_build find approximate nearest neighbors query points. New function: rpf_filter. Retains best “scoring” trees forest, tree scored based well reproduces given knn. New initialization method nnd_knn: init = \"tree\". Uses RP Forest initialization method. New parameter nnd_knn: ret_forest. Returns search forest used init = \"tree\" can used future searching filtering. New parameter nnd_knn: init_opts. Options can passed RP forest initialization (rpf_knn).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-11","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.11","text":"Progess report nnd_knn n_threads > 0 reporting double actual number iterations. made progress bar way optimistic. bug flagging neighbors 0.0.10 made nearest neighbor descent inefficient.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-10","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.10","text":"change metric: \"cosine\" \"correlation\" renamed \"cosine-preprocess\" \"correlation-preprocess\" respectively. reflects preprocessing data front make subsequent distance calculations faster. endeavored avoid unnecessary allocations copying preprocessing, still chance memory usage. cosine correlation metrics still available option, now use implementation doesn’t preprocessing. preprocessing non-preprocessing version give numerical results, give take minor numerical differences, distance zero, preprocessing versions may give values slightly different zero (e.g. 1e-7). New functions: correlation_distance, cosine_distance, euclidean_distance, hamming_distance, l2sqr_distance, manhattan_distance calculating distance two vectors, may useful arbitrary distance calculations nearest neighbor routines , although won’t efficient (call C++ code, though). cosine correlation calculations use non-preprocessing implementations. Generalize hamming metric standard definition. old implementation hamming metric worked binary data renamed bhamming. (contributed Vitalie Spinu) New parameter obs added functions: set obs = \"C\" can pass input data column-oriented format.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-10","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.10","text":"random_knn function used always return item neighbor, n_nbrs - 1 returned neighbors actually selected random. Even forgot doesn’t make lot sense, now really just get back n_nbrs random selections. providing pre-calculated neighbors init parameter nnd_knn graph_knn_query: previously, k specified larger number neighbors included init, gave error. Now, init augmented random neighbors reach desired k. useful way “restart” neighbor search better--random location k found small initially. Note random selection take account identities already chosen neighbors, duplicates may included augmented result, reduce effective size initialized number neighbors. Removed block_size grain_size parameters functions. related amount work done per thread, ’s obvious outside user set . long-running computations update progress indicators frequently (verbose = TRUE) respond user-requested cancellation.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-9","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.9 (20 June 2021)","text":"nnd_knn_query renamed graph_knn_query now closely follows current pynndescent graph search method (including backtracking search). New function: prepare_search_graph preparing search graph neighbor graph use graph_knn_query, using reverse nearest neighbors, occlusion pruning truncation. Sparse graphs supported input graph_knn_query.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-008-10-october-2020","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.8 (10 October 2020)","title":"rnndescent 0.0.8 (10 October 2020)","text":"major rewrite internal organization C++ less R-specific.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"minor-license-change-0-0-8","dir":"Changelog","previous_headings":"","what":"Minor License Change","title":"rnndescent 0.0.8 (10 October 2020)","text":"license rnndescent changed GPLv3 GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-8","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.8 (10 October 2020)","text":"New metric: \"correlation\". (1 minus) Pearson correlation. New function: k_occur counts k-occurrences item idx matrix, number times item appears k-nearest neighbor list dataset. distribution k-occurrences can used diagnose “hubness” dataset. Items large k-occurrence (>> k, e.g. 10k), may indicate low accuracy approximate nearest neighbor result.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-7","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.7 (1 March 2020)","text":"avoid undefined behavior issues, rnndescent now uses internal implementation RcppParallel’s parallelFor loop works std::thread load Intel’s TBB library.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-6","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.6 (29 November 2019)","text":"reason, thought ok use dqrng sample routines inside thread, despite clearly using R API extensively. ’s ok causes lots crashes. now re-implementation dqrng’s sample routines using plain std::vectors src/rnn_sample.h. file licensed AGPL (rnndescent whole remains GPL3).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-5","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.5 (23 November 2019)","text":"New function: merge_knn, combine two nearest neighbor graphs. Useful combining results multiple runs nnd_knn random_knn. Also, merge_knnl, operates list multiple neighbor graphs, can provide speed merge_knn don’t mind storing multiple graphs memory .","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-5","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.5 (23 November 2019)","text":"thread-locking issue converting R matrices internal heap data structure affected nnd_knn n_threads > 1 random_knn n_threads > 1 order_by_distance = TRUE. Potential minor speed improvement nnd_knn n_threads > 1 due use mutex pool.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-004-21-november-2019","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.4 (21 November 2019)","title":"rnndescent 0.0.4 (21 November 2019)","text":"Mainly internal clean-reduce duplication.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-4","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.4 (21 November 2019)","text":"default, nnd_knn nnd_knn_query use progress bar brute force random neighbor functions. Bring back old per-iteration logging also showed current distance sum knn progress = \"dist\" option. random_knn random_knn_query, order_by_distance = TRUE n_threads > 0, final sorting knn graph multi-threaded. Initialization nearest neighbor descent data structures also multi-threaded n_threads > 0. Progress bar updating cancellation now consistent less likely cause hanging crashing across different methods. Using Cosine Hamming distance may take less memory run bit faster.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-3","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.3 (15 November 2019)","text":"now “query” versions three functions: nnd_knn_query useful, brute_force_knn_query random_knn_query also available. allows query data search reference data, .e. returned indices distances relative reference data, member query. methods also available multi-threaded mode, nnd_knn_query low high memory version.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-3","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.3 (15 November 2019)","text":"Incremental search nearest neighbor descent didn’t work correctly, retained new neighbors marked new rather old. made search repeat distance calculations unnecessarily. Heap initialization ignored existing distances input distance matrix. l2 metric renamed l2sqr accurately reflect : square L2 (Euclidean) metric. New option use_alt_metric. Set FALSE don’t want alternative, faster metrics (keep distance ordering metric) used internal calculations. Currently applies metric = \"euclidean\", squared Euclidean distance used internally. worth setting FALSE think alternative causing numerical issues (bug, please report !). Random brute force methods make use alternative metrics. New option block_size parallel methods, determines amount work done parallel checking user interrupt request updating progress. random_knn now returns results sorted order. can turn order_distances = FALSE, don’t need sorting (e.g. using results input something else). Progress bars brute_force random methods now correct.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-2","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.2 (7 November 2019)","text":"Brute force nearest neighbor function renamed brute_force_knn. Random nearest neighbors renamed random_knn. Brute force random nearest neighbors now interruptible. Progress bar shown verbose = TRUE. fast_rand option removed, applied single-threading, negligible effect. Also, number changes inspired recent work https://github.com/lmcinnes/pynndescent: parallel nearest neighbor descent now faster. rho sampling parameter removed. size candidates (general neighbors) list now controlled entirely max_candidates. Default max_candidates reduced 20. use_set logical flag replaced low_memory, opposite meaning. now also works using multiple threads. follows pynndescent implementation, ’s still experimental, low_memory = TRUE default moment. low_memory = FALSE implementation n_threads = 0 (originally equivalent use_set = TRUE) faster. New parameter block_size, balances interleaving queuing updates versus applying current graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-2","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.2 (7 November 2019)","text":"incremental search, neighbors marked selected new candidate list even later removed due finite size candidates heap. Now, indices still retained candidate building marked new. Improved man pages (examples plus link nearest neighbor descent reference). Removed dependency Boost headers.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-001-27-october-2019","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.1 (27 October 2019)","title":"rnndescent 0.0.1 (27 October 2019)","text":"Initial release.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-1","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.1 (27 October 2019)","text":"Nearest Neighbor Descent following metrics: Euclidean, Cosine, Manhattan, Hamming. Support multi-threading RcppParallel. Initialization random neighbors. Brute force nearest neighbor calculation.","code":""}]
