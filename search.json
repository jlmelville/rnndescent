[{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) 2019 James Melville  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. rnndescent Copyright (C) 2019 James Melville This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"comparing-low--and-high-dimensional-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Comparing Low- and High-Dimensional Nearest Neighbors","title":"rnndescent","text":"Let’s look distribution nearest neighbor distances high low dimensions (easier comparison, normalized respect largest distance)   Compared low dimensional data, can see high dimensional distances distributed around higher distance well symmetric distribution. distribution neighbor distances high-dimensional case neighbors found NND:  Pretty much indistinguishable exact results, seems like isn’t obvious diagnostic distances . distribution errors NND results uniform items neighborhoods noticeably better predicted others? function calculate vector relative RMS error two sets neighbors terms distances: don’t include first nearest neighbor distances, invariably self distances leads uninteresting number zero error results. measure error bit less strict nn_accuracyv neighbor outside true kNN, comparable distance, penalized less harshly distant point. ’s histogram RRMS distance errors:  None relative errors actually large, care value kth nearest neighbor distances, even 1000D case, still get decent results case. can also see clear distribution errors, appreciable number items zero RRMS distance errors, items largest error. histograms accuracies:  shows similar pattern: items high accuracy noticeably worse accuracy average. completeness, relationship accuracy RRMSE:  Nothing surprising : ’s fairly consistent spread RRMSE given accuracy. whether use error distance accuracy measure well approximate nearest neighbors method working, least case, high dimensional dataset affects items others.","code":"hist(g2d_nnbf$dist[, -1] / max(g2d_nnbf$dist[, -1]), xlab = \"distances\", main = \"2D 15-NN\") hist(g1000d_nnbf$dist[, -1] / max(g1000d_nnbf$dist[, -1]), xlab = \"distances\", main = \"1000D 15-NN\") hist(g1000d_nnd$dist[, -1] / max(g1000d_nnd$dist[, -1]),   xlab = \"distances\",   main = \"1000D 15-NND\" ) nn_rrmsev <- function(nn, ref) {   n <- ncol(ref$dist) - 1   sqrt(apply((nn$dist[, -1] - ref$dist[, -1])^2 / n, 1, sum) /     apply(ref$dist[, -1]^2, 1, sum)) } g1000d_rrmse <- nn_rrmsev(g1000d_nnd, g1000d_nnbf) hist(g1000d_rrmse,   main = \"1000D distance error\",   xlab = \"Relative RMS error\" ) g1000d_nnd_acc <- nn_accuracyv(g1000d_nnbf, g1000d_nnd, k = 15) hist(g1000d_nnd_acc,   main = \"1000D accuracy\",   xlab = \"accuracy\",   xlim = c(0, 1) ) plot(   g1000d_nnd_acc,   g1000d_rrmse,   main = \"RRMSE vs accuracy\",   xlab = \"accuracy\",   ylab = \"RRMSE\" )"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"detecting-hubness","dir":"Articles","previous_headings":"","what":"Detecting Hubness","title":"rnndescent","text":"(Radovanovic, Nanopoulos, Ivanovic 2010) discusses technique detecting hubness: look items appear frequently k-nearest neighbor graph. k_occur function counts “k-occurrences” item dataset, .e. count number times item appears k-nearest neighbor graph. can also see reversing direction edges k-nearest neighbor graph counting -degree item. distribution neighbors entirely uniform expect see item appear \\(k\\) times. hubs k-occurrence get large size dataset, \\(N\\). item appears neighbor graph fewer \\(k\\) times termed “antihub”. definition neighbor item always includes item , expect minimum \\(k\\)-occurrence \\(1\\). Also, always \\(Nk\\) edges \\(k\\)-nearest neighbor graph, item appears expected amount implies items must -represented. Practically speaking, always going items larger \\(k\\)-occurrence expected hence lower \\(k\\)-occurrence, hubness anti-hubness case deciding cut-presence item lot neighbors starts causing problems, going dependent planning neighbor graph (probably number neighbors want).","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"k-occurrence-in-the-2d-case","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence in the 2D case","title":"rnndescent","text":"First, let’s look 2D case using exact k-nearest neighbors: mean average \\(k\\)-occurrence never helpful: noted always \\(Nk\\) edges neighbor graph, mean \\(k\\)-occurrence always \\(k\\). However descriptions distribution informative. median \\(k\\)-occurrence also 15, good sign, values 25% 75% aren’t different . maximum \\(k\\)-occurrence less \\(2k\\). minimum value 1 means anti-hubs dataset, : one anti-hub dataset. ’s histogram k-occurrences:  unremarkable-looking distribution visual indication dataset without lot hubness anti-hubs lurking cause problems nearest neighbor descent.","code":"g2d_bfko <- k_occur(g2d_nnbf, k = 15) summary(g2d_bfko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1      13      15      15      17      24 sum(g2d_bfko == 1) #> [1] 1 hist(g2d_bfko, main = \"2D 15-NN\", xlab = \"k-occurrences\")"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"k-occurrence-in-the-1000d-case","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence in the 1000D case","title":"rnndescent","text":"’s k-occurrence histogram looks like high dimensional case:  differences pretty stark. first thing notice x-axis. 2D case, maximum k-occurrence ~20. 1000D looking ~300. ’s hard see details, let’s zoom region 2D case clipping k-occurrence larger largest 2D k-occurrence:  ’s different distribution 2D case: large number anti-hubs noticeable number hubs. ’s certainly peak k-occurrence 15. Comparing numerical summary 2D case instructive: , ’s good reminder mean k-occurrence value. median k-occurrence immediately communicates difference 2D case. can also see maximum k-occurrence means one point considered close neighbor one third dataset. many anti-hubs ? quarter dataset appear neighbor point. serious implications using neighbor graph certain purposes: reach quarter dataset starting arbitrary point graph following neighbors. also might point nearest neighbor descent trouble high dimensional case: rely points turning neighbors points order introduce potential neighbors, fact many points dataset aren’t anyone’s actual neighbors suggest unlikely get involved local join procedure much points.","code":"g1000d_bfko <- k_occur(g1000d_nnbf$idx, k = 15) hist(g1000d_bfko, main = \"1000D 15-NN\", xlab = \"k-occurrences\") hist(pmin(g1000d_bfko, max(g2d_bfko)),   main = \"1000D 15-NN zoomed\",   xlab = \"k-occurrences\" ) summary(g1000d_bfko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    1.00    2.00    5.00   15.00   14.25  344.00 sum(g1000d_bfko == 1) #> [1] 220"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"k-occurrence-as-a-diagnostic-of-nnd-failure","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence as a diagnostic of NND failure","title":"rnndescent","text":"now shown can use k-occurrences exact nearest neighbors low high dimensional data detect existence hubs, turn might lead us suspect approximate nearest neighbors found nearest neighbor descent may accurate. ’s useful diagnostic exact neighbors don’t need run NND first place. even approximate nearest neighbor graph produced NND isn’t highly accurate, still show similar characteristics hubness?  seems similar true results, zooming like exact results:  Visually looks lot like distribution exact results. Next, numerical summary: Quantitatively, also tracks exact results: median k-occurrence much smaller \\(k\\), hub large number neighbors (larger exact case similar degree) similar number anti-hubs. suggests way diagnose nearest neighbor descent routine may low accuracy: look distribution k-occurrences resulting approximate nearest neighbor graph (even just maximum value). value \\(\\gg k\\) may mean reduced accuracy. course, isn’t foolproof, even NND perfect job still get sorts values, ’s starting point. Taking distribution k-occurrences whole, approximate results seem track exact results fairly well, seen, errors approximate results uniformly distributed across data. let’s see well NND k-occurrences “predict” exact results:  overall relationship seems strong. line plot x=y, can see high values k-occurrence NND results tend -estimate k-occurrence, large values hardly matters, ambiguity nodes hub-like. Zooming lower values k-occurrence:  seems tendency -estimate k-occurrence. Anti-hubs also perfectly identified, true anti-hubs appear small number times approximate neighbor graph.","code":"g1000d_nndko <- k_occur(g1000d_nnd$idx, k = 15) hist(g1000d_nndko, main = \"1000D 15-NND\", xlab = \"k-occurrences\") hist(pmin(g1000d_nndko, max(g2d_bfko)),   main = \"1000D 15-NND zoomed\",   xlab = \"k-occurrences\" ) summary(g1000d_nndko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       2       4      15      11     407 sum(g1000d_nndko == 1) #> [1] 247 plot(g1000d_nndko, g1000d_bfko,   xlab = \"approximate\", ylab = \"exact\",   xlim = c(0, max(g1000d_nndko, g1000d_bfko)),   ylim = c(0, max(g1000d_nndko, g1000d_bfko)),   main = \"1000D k-occ\" ) abline(a = 0, b = 1) cor(g1000d_nndko, g1000d_bfko, method = \"pearson\") #> [1] 0.9938052 plot(g1000d_nndko, g1000d_bfko,   xlab = \"approximate\", ylab = \"exact\",   xlim = c(0, max(g2d_bfko)),   ylim = c(0, max(g2d_bfko)),   main = \"1000D low k-occ\" ) abline(a = 0, b = 1)"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"detecting-poorly-predicted-neighbors","dir":"Articles","previous_headings":"Detecting Hubness","what":"Detecting Poorly Predicted Neighbors","title":"rnndescent","text":"’ve seen objects neighbors predicted better others. Based everything ’ve seen far k-occurrences NND, reasonable wonder: items dataset poorly predicted neighbors anti-hubs (predicted exact)? least give us way detecting items likely low accuracy neighborhoods: perhaps treated specially (algorithm). ’s plot accuracy k-occurrences NND neighbors:  answer question “really”, trend. empty space lower right plot indicates items large k-occurrence (hubs) well predicted. k-occurrence 150, guaranteed perfectly predict neighborhood item. However, end k-occurrence spectrum, can see lower bound predicted accuracy plummet k-occurrence reduced, anti-hubs actually neighborhoods accurately predicted . Unfortunately means k-occurrence bit rough use predict poorly-predicted items. Let’s say wanted get items neighborhood less 90% accurate: ’s already quite lot items: three-quarters entire dataset. largest k-occurrence item dataset accuracy threshold? , guarantee found items might poorly predicted, need filter every item k-occurrence smaller value, even though know well-predicted: ’s dataset. dropped threshold 80 accuracy, help? bit, ’s still substantial majority dataset. whatever decided items wouldn’t saving huge amount effort. much idea. suggests can’t improve results , just effort identifying individual points filter , treat differently merge back final neighbor graph means just reprocessing entire dataset different way likely competitive solution.","code":"plot(g1000d_nndko, g1000d_nnd_acc,   xlab = \"NND k-occ\", ylab = \"accuracy\",   xlim = c(0, max(g1000d_nndko, g1000d_bfko)),   main = \"1000D acc vs NND k-occ\" ) sum(g1000d_nnd_acc < 0.9) #> [1] 783 max(g1000d_nndko[g1000d_nnd_acc < 0.9]) #> [1] 136 sum(g1000d_nndko <= max(g1000d_nndko[g1000d_nnd_acc < 0.9])) #> [1] 981 sum(g1000d_nndko <= max(g1000d_nndko[g1000d_nnd_acc < 0.8])) #> [1] 872"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"detecting-problems-early","dir":"Articles","previous_headings":"Detecting Hubness","what":"Detecting Problems Early","title":"rnndescent","text":"Back looking k-occurrence distribution whole: can see converged NND results, despite 100% accurate good job expressing hubness underlying data. converged results need ? think NND tool identifying hubness datasets whole rather accurate approximate nearest neighbor graphs? much less unconverged NND graph, obviously even less accurate, still correctly identify dataset hubs? test , let’s run NND method one iteration get k-occurrences result: accurate results? Ok, think can agree accurate neighbor graph. let’s take look k-occurrence distribution:  Looking familiar. Zooming …  distribution least similar converged version. Taking look numbers: Compared converged (exact) distribution, median k-occurrence low, object largest k-occurrence, large (\\(> 10k\\), seems like good threshold concerned presence hubs) large, fewer objects anti-hubs. least dataset, hubness can qualitatively detected even inaccurate neighbor graph. datasets don’t contain hubs? Let’s just check seeing artifact unconverged nearest neighbor descent, running procedure 2D dataset:  can see neighbor graph also accurate 1 iteration 2D case, distribution k-occurrences also qualitatively resembles exact result. time, compared exact results slightly anti-hubs maximum k-occurrence increased, trends slightly reversed compared 1000D data. least qualitative identification hubness, , one iteration nearest neighbor descent might enough.","code":"g1000d_nnd_iter1 <- nnd_knn(g1000d, k = 15, metric = \"euclidean\", n_iters = 1) g1000d_nndkoi1 <- k_occur(g1000d_nnd_iter1$idx, k = 15) nn_accuracy(g1000d_nnbf, g1000d_nnd_iter1, k = 15) #> [1] 0.3188667 hist(g1000d_nndkoi1, main = \"1000D 15-NND (1 iter)\", xlab = \"k-occurrences\") hist(pmin(g1000d_nndkoi1, max(g2d_bfko)),   main = \"1000D 15-NND (1 iter, zoomed)\",   xlab = \"k-occurrences\" ) summary(g1000d_nndkoi1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       2       7      15      18     179 sum(g1000d_nndkoi1 == 1) #> [1] 154 g2d_nnd_iter1 <- nnd_knn(g2d, k = 15, metric = \"euclidean\", n_iters = 1) g2d_nndkoi1 <- k_occur(g2d_nnd_iter1$idx, k = 15) hist(g2d_nndkoi1, main = \"2D 15-NND (1 iter)\", xlab = \"k-occurrences\") summary(g2d_nndkoi1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1      12      15      15      19      32 sum(g2d_nndkoi1 == 1) #> [1] 4 nn_accuracy(g2d_nnbf, g2d_nnd_iter1, k = 15) #> [1] 0.3467333"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"improving-accuracy","dir":"Articles","previous_headings":"","what":"Improving accuracy","title":"rnndescent","text":"know nearest neighbor descent (least typical settings) may give highly accurate results high dimensions. help k-occurrences, can even detect might happening. can ?","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"use-more-neighbors","dir":"Articles","previous_headings":"Improving accuracy","what":"Use More Neighbors","title":"rnndescent","text":"One simple (slightly expensive) way keep neighbors calculation. example, double number neighbors 30, get top-15 accuracy: ’s big improvement, increasing k way can quite expensive terms run time.","code":"g1000d_nnd_k30 <- nnd_knn(g1000d, k = 30, metric = \"euclidean\") nn_accuracy(g1000d_nnbf, g1000d_nnd_k30, k = 15) #> [1] 0.9758667"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"merging-multiple-independent-results","dir":"Articles","previous_headings":"Improving accuracy","what":"Merging Multiple Independent Results","title":"rnndescent","text":"taking advantage stochastic nature algorithm? results sufficiently diverse runs NND, generate two graphs two separate runs, merge results. Let’s repeat NND see accuracy new result like. ’s similar first run. ’s re-assuring sense variance accuracy doesn’t seem high one run next. hopefully doesn’t also mean NND producing similar neighbor graph time, case merging won’t helpful. Time find : ’s big improvement. seem like diversity results.  Despite similar overall accuracies, ’s quite large variance runs terms items accurate neighborhoods. might scope improving results merging different runs, especially can run individual NND routines parallel.","code":"g1000d_nnd_rep <- nnd_knn(g1000d, k = 15, metric = \"euclidean\") nn_accuracy(g1000d_nnbf, g1000d_nnd_rep, k = 15) #> [1] 0.7776667 g1000d_nnd_merge <- merge_knn(g1000d_nnd, g1000d_nnd_rep) nn_accuracy(g1000d_nnbf, g1000d_nnd_merge, k = 15) #> [1] 0.9023333 g1000d_nnd_rep_acc <- nn_accuracyv(g1000d_nnbf, g1000d_nnd_rep, k = 15) plot(   g1000d_nnd_acc,   g1000d_nnd_rep_acc,   main = \"1000D NND accuracy comparison\",   xlab = \"accuracy run 1\",   ylab = \"accuracy run 2\" ) cor(g1000d_nnd_acc, g1000d_nnd_rep_acc) #> [1] 0.5370503"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"using-a-search-graph","dir":"Articles","previous_headings":"Improving accuracy","what":"Using a Search Graph","title":"rnndescent","text":"Practically, simplest way improve results rnndescent convert neighbor graph search graph, query original data. First, preparation step: augments neighbor graph reversed edges neighbor graph, \\(\\) one nearest neighbors \\(j\\), guarantee \\(j\\) also considered near neighbor \\(\\). ameliorates issue anti-hubs \\(k\\) neighbors anti-hub now neighbor list. downside including reversed edges neighbor graph neighbor list hub now going large consists \\(k\\) nearest neighbors hub items consider hub near neighbor, definition lot. can make search graph inefficient, disproportionate amount time spent searching neighbors hub. diversify_prob pruning_degree_multiplier parameters used reduce back -degree node (number -going edges). results objects varying number neighbors, case maximum 22. 50% larger k = 15 account introduction reverse edges. Anti-hubs can reintroduced due edge reduction, hopefully distribution edges bit equitable. summary histogram k-occurrences search graph:  quite skewed neighbor graph, still lot room improvement. rate, search graph hand, can now search using original data query: results improved? Yes, accuracy now nearly perfect. disadvantages search graph approach building neighbor graph less efficient NND: graph_knn_query must assume query data entirely different reference data. advantage can make use reverse edges , importantly, back-tracking (controlled via epsilon parameter), seems make difference example. procedure recommended practice using graph_knn_query search graph generated neighbor graph. required use search graph argument reference_graph parameter. back-tracking search using neighbor graph directly everything else : Accuracies nearly good. can save even time turning back-tracking (epsilon = 0): accuracies now noticeably less improved. Using search graph without back-tracking gives slightly better accuracies: seems like sort back-tracking recommended approach.","code":"g1000d_search_graph <-   prepare_search_graph(     data = g1000d,     graph = g1000d_nnd,     metric = \"euclidean\",     diversify_prob = 1,     pruning_degree_multiplier = 1.5   ) g1000d_sgko <- k_occur(g1000d_search_graph) hist(g1000d_sgko, main = \"search graph k-occurrences\", xlab = \"k-occurrences\") summary(g1000d_sgko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   1.000   2.000   5.000   7.517   8.000 111.000 sum(g1000d_sgko == 1) #> [1] 106 g1000d_search <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_search_graph,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0.1   ) nn_accuracy(g1000d_nnbf, g1000d_search, k = 15) #> [1] 0.9978 g1000d_nnd_search <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_nnd,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0.1   ) nn_accuracy(g1000d_nnbf, g1000d_nnd_search, k = 15) #> [1] 0.9876 g1000d_nnd_search0 <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_nnd,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0   ) nn_accuracy(g1000d_nnbf, g1000d_nnd_search0, k = 15) #> [1] 0.8298 g1000d_search0 <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_search_graph,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0   ) nn_accuracy(g1000d_nnbf, g1000d_search0, k = 15) #> [1] 0.8444"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"local-scaling","dir":"Articles","previous_headings":"Improving accuracy","what":"Local Scaling","title":"rnndescent","text":"context spectral clustering, (Zelnik-manor Perona 2005) suggested scaling nearest neighbor distances reflect local statistics neighborhoods. allow idea large short distance vary depending local density around point. scaling suggested : \\[\\hat{d}_{ij}^2 = \\frac{d_{ij}^2}{\\sigma_i\\sigma_j}\\] \\(\\hat{d}_{ij}\\) scaled distance points \\(\\) \\(j\\), \\(d_{ij}\\) original distance, \\(\\sigma_i\\) local scale associated point \\(\\). define \\(\\sigma_i\\), Zelnik-Manor Perona suggest using distance nearest neighbor. paper got good results using distance seventh-nearest neighbor. (Schnitzer et al. 2012) suggested using \\(\\hat{d}_{ij}\\) instead \\(d_{ij}\\) nearest neighbor calculations way reduce effect hubs. hubs neighbors, distance specific neighbor smaller relative non-hubs. Hence \\(\\sigma_i\\) smaller hubs, lead larger \\(\\hat{d}_{ij}\\) compared non-hubs. Therefore using \\(\\hat{d}_{ij}\\) acts “penalize” hubs competing non-hubs appear nearest neighbor list. modification suggested Schnitzer co-workers use average distance multiple nearest neighbors calculate \\(\\sigma_i\\), technique used (Jegou, Harzallah, Schmid 2007). field dimensionality reduction, generating nearest neighbor graph step many methods. (Wang et al. 2021) describe using local scaling post-processing step following recipe: Find \\(k+50\\)th nearest neighbors, \\(k\\) actual number neighbors want (e.g. want 15 neighbors per point, find 65 nearest neighbors). Use mean distance 4-6th nearest neighbors \\(\\sigma_i\\). Using range also advocated another dimensionality reduction method (Amid Warmuth 2019). \\(k+50\\) nearest neighbors, find \\(k\\) neighbors smallest locally scaled distances. \\(k\\) neighbors use downstream processing. 1000D example, first need generate (approximate) 65-nearest neighbors: noticeably slow step. Next, use local_scale_nn function extract 15-neighbor locally-scaled subset: look distances g1000d_nnd65ls15 original unscaled distances, found g1000d_nnd65 (\\(d_{ij}\\)). scaled distances \\(\\hat{d}_{ij}\\) used internally select 15 neighbors, returned local_scale_nn. local scaling affect hubness resulting neighbor graph?  massive reduction hubness 15NN graph, nearly good search graph generated 15NN graph. Can see improvements generate search graph locally scaled neighbors?  hubness properties improved using either local scaling creating search graph separately. promising, translate better search performance? accuracy results using locally scaled search graph without backtracking: Without back-tracking, can get 94% accuracy, nearly 10% improvement compared search graph generated 15NN graph directly. ’s worth noting initialized search using g1000d_nnd, 15NN results running NND k = 15. makes comparing performance search graph fairer, reality wouldn’t data hand, used k = 65. noted ‘Use Neighbors’ section , already know using NND increased value k going result big improvement without bothering local scaling: Almost perfect. also locally-scaled 15 nearest neighbor graph can use initialization: noticeably less good initializing using 15 NND results, ’s still best non-back-tracking result. set epsilon = 0.1, get 100% accuracy using neighbor graph search graph, non-locally scaled graphs getting close . value local scaling probably want find accurate approximation nearest neighbors: using back-tracking search search graph derived unscaled neighbor graph directly going less effort. However may cases increased cost building \\(k+50\\) nearest neighbor graph big problem, e.g. intend build graph search many times, search graph built locally scaled neighbor graph probably efficient traverse cases unscaled data suffers hubs. properties locally-scaled graph may attractive uses e.g. dimensionality reduction methods like described Wang co-workers.","code":"g1000d_nnd65 <- nnd_knn(g1000d, k = 65, metric = \"euclidean\") g1000d_nnd65ls15 <-   local_scale_nn(g1000d_nnd65, k = 15, k_scale = c(4, 6)) g1000d_lsko <- k_occur(g1000d_nnd65ls15) hist(g1000d_lsko, main = \"locally scaled k-occ\", xlab = \"k-occurrences\") summary(g1000d_lsko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       6      11      15      19     116 sum(g1000d_lsko == 1) #> [1] 31 g1000dls_search_graph <-   prepare_search_graph(     data = g1000d,     graph = g1000d_nnd65ls15,     metric = \"euclidean\",     diversify_prob = 1,     pruning_degree_multiplier = 1.5   ) g1000d_lssgko <- k_occur(g1000dls_search_graph) hist(g1000d_lssgko, main = \"locally scaled search k-occ\", xlab = \"k-occurrences\") summary(g1000d_lssgko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    1.00    6.00    9.00   10.38   13.00   83.00 sum(g1000d_lssgko == 1) #> [1] 20 g1000dls_search0nnd <-     graph_knn_query(         query = g1000d,         reference = g1000d,         reference_graph = g1000dls_search_graph,         k = 15,         metric = \"euclidean\",         init = g1000d_nnd,         epsilon = 0     ) nn_accuracy(g1000d_nnbf, g1000dls_search0nnd, k = 15) #> [1] 0.9392667 nn_accuracy(g1000d_nnbf, g1000d_nnd65, k = 15) #> [1] 0.9998667 g1000dls_search0 <-     graph_knn_query(         query = g1000d,         reference = g1000d,         reference_graph = g1000dls_search_graph,         k = 15,         metric = \"euclidean\",         init = g1000d_nnd65ls15,         epsilon = 0     ) nn_accuracy(g1000d_nnbf, g1000dls_search0, k = 15) #> [1] 0.8805333"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"rnndescent","text":"High dimensional data leads hubs. “hubness” item dataset can measured k-occurrence corresponding nearest neighbor graph. higher k-occurrence, hub . existence hubs implies existence “anti-hubs”, .e. items low k-occurrence. small number hubs can create disproportionately larger number anti-hubs, larger value k-occurrence creating anti-hubs. accuracy nearest neighbor descent reduced presence hubs: specifically, lower k-occurrence item, greater probability low accuracy nearest neighbors. Accuracy nearest neighbor descent can improved searching larger number neighbors truncating result desired size, cost longer run-time memory usage. Alternatively, can run nearest neighbor descent multiple times different random starting points merge results. accurate efficient results obtained converting nearest neighbor descent results search graph querying graph original data, using nearest neighbor results initialization back-tracking search. Local scaling can also reduce effect hubs combined search graph, cost larger computational effort. concerned potential hubs interfering accuracy neighbor graph, suggest following steps: Generate neighbor graph nnd_knn default parameters. Evaluate hubness graph k_occur. maximum k-occurrence exceeds threshold (maybe 10 * k good starting point), use prepare_search_graph graph_knn_query back-tracking search (set epsilon > 0) refine results . provide robust approach producing accurate approximate nearest neighbors without spending time unnecessary graph search results probably already quite good. effect hubness nearest neighbors, advanced attempts fix problem, see work Flexer co-workers (Schnitzer et al. 2012; Flexer 2016; Feldbauer Flexer 2019; Feldbauer, Rattei, Flexer 2019) Radovanović co-workers (Radovanovic, Nanopoulos, Ivanovic 2010; Bratić et al. 2019).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"James Melville. Author, maintainer. Vitalie Spinu. Contributor.","code":""},{"path":"https://jlmelville.github.io/rnndescent/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Melville J (2023). rnndescent: Nearest Neighbor Descent Method Approximate Nearest Neighbors. R package version 0.0.13, https://jlmelville.github.io/rnndescent/.","code":"@Manual{,   title = {rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors},   author = {James Melville},   year = {2023},   note = {R package version 0.0.13},   url = {https://jlmelville.github.io/rnndescent/}, }"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"rnndescent","dir":"","previous_headings":"","what":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"R package implementing Nearest Neighbor Descent method (Dong et al., 2011) finding approximate nearest neighbors, based Python library PyNNDescent. Lightly development, can used optimizing initial set nearest neighbors, e.g. generated RcppAnnoy RcppHNSW, can good job even initialized random starting point.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"current-status","dir":"","previous_headings":"","what":"Current Status","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Compared pynndescent, major missing features : dense distance functions sparse distance functions 30 October 2023 last, workable random partition forest implementation added. can used standalone (e.g. rpf_knn, rpf_build, rpt_knn_query) initialization nearest neighbor descent (nnd_knn(init = \"tree\", ...)). forest can serialized saveRDS pay price convenience pass back forth R C++ layer querying. now access underlying C++ class via R like RcppHNSW RcppAnnoy may suitable use cases. 19 October 2023 Inevitably 0.0.11 bug 0.0.10 nearest neighbor descent correctly flagging new/old neighbors reduced performance (actual result). 18 October 2023 long-postponed major internal refactoring means might able make bit progress package. now, cosine correlation metrics migrated preprocessing data (versions still available cosine-preprocess correlation-preprocess respectively). Also, exported distance metrics R functions (e.g. cosine_distance, euclidean_distance). 18 September 2021 \"hamming\" metric now supports integer-valued (just binary) inputs, thanks contribution Vitalie Spinu. older metric code path binary data supported via metric = \"bhamming\". 20 June 2021 big step forward usefulness addition prepare_search_graph function creates prunes undirected search graph neighbor graph use (now re-named) graph_knn_query function. latter now also capable backtracking search performs fairly well. 4 October 2020 Added \"correlation\" metric k_occur function help diagnose potential hubness dataset. 23 November 2019 Added merge_knn merge_knnl combining multiple nn results. 15 November 2019 now possible query reference set data produce approximate knn graph relative references (.e. none queries selected neighbors) via nnd_knn_query (related brute_force random variants). 27 October 2019 rnndescent creeps towards usability. multi-threaded implementation (using RcppParallel) now added.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"packages makes use C++ code must compiled. may carry extra steps able build: Windows: install Rtools ensure C:\\Rtools\\bin path. Mac OS X: using custom ~/.R/Makevars may cause linking errors. sort thing potential problem platforms seems bite Mac owners . R Mac OS X FAQ may helpful work can get away . safe side, advise building without custom Makevars.","code":"remotes::install_github(\"jlmelville/rnndescent\")"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Optimizing initial set approximate nearest neighbors: can also search neighbor graph new query items: Although example shows basic procedure building graph querying new data , raw nearest neighbor graph isn’t efficient general. ’s highly advisable use refined search graph based original data neighbor graph: See help text prepare_search_graph various parameters can change control trade speed search accuracy.","code":"library(rnndescent)  # both hnsw_knn and nnd_knn will remove non-numeric columns from data-frames # for you, but to avoid confusion, these examples will use a matrix irism <- as.matrix(iris[, -5])   # Generate a Random Projection knn (set n_threads for parallel search): iris_rp_nn <- rpf_knn(irism, k = 15)  # nn descent improves results: set verbose = TRUE and progress = \"dist\", to  # track distance sum progress over iterations res <-   nnd_knn(irism,     metric = \"euclidean\",     init = iris_rp_nn,     verbose = TRUE,     progress = \"dist\"   )  # search can be multi-threaded res <-   nnd_knn(     irism,     metric = \"euclidean\",     init = iris_rp_nn,     verbose = TRUE,     n_threads = 4   )  # a (potentially) faster version of the algorithm is available that avoids some  # repeated distance calculations at the cost of using more memory. Currently off # by default. res <-   nnd_knn(     irism,     metric = \"euclidean\",     init = iris_rp_nn,     verbose = TRUE,     n_threads = 4,     low_memory = FALSE   )    # You can optimize results from other methods or packages too:   # install.packages(\"RcppHNSW\") # Use settings that don't get perfect results straight away iris_hnsw_nn <-   RcppHNSW::hnsw_knn(irism,     k = 15,     M = 2,     distance = \"euclidean\"   )  res <-   nnd_knn(     irism,     metric = \"euclidean\",     init = iris_hnsw_nn,     verbose = TRUE,     n_threads = 4,     low_memory = FALSE   ) # 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # First, find the approximate 10-nearest neighbor graph for the references: iris_ref_knn <- nnd_knn(iris_ref, k = 10)  # For each item in iris_query find the 10 nearest neighbors in iris_ref # You need to pass both the reference data and the knn graph. iris_query_nn <-   graph_knn_query(     query = iris_query,     reference = iris_ref,     reference_graph = iris_ref_knn,     k = 4,     metric = \"euclidean\",     verbose = TRUE   ) iris_search_graph <-   prepare_search_graph(iris_ref, iris_ref_knn, verbose = TRUE) iris_query_nn <-   graph_knn_query(     query = iris_query,     reference = iris_ref,     reference_graph = iris_search_graph,     k = 4,     metric = \"euclidean\",     verbose = TRUE   )"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"initialization","dir":"","previous_headings":"","what":"Initialization","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"default nnd_knn initialize random neighbors. Set init = \"tree\" use random partition tree initialization. initializing knn query, also random_knn_query.","code":"library(rnndescent)  irism <- as.matrix(iris[, -5])  # picks indices at random and then carries out nearest neighbor descent res <- nnd_knn(irism,   k = 15,   metric = \"euclidean\",   n_threads = 4 )  # if you want the random indices and their distances: iris_rand_nn <-   random_knn(irism,     k = 15,     metric = \"euclidean\",     n_threads = 4   )    # use RP tree initialization res <- nnd_knn(irism,   k = 15,   metric = \"euclidean\",   n_threads = 4,   init = \"tree\" )"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"brute-force","dir":"","previous_headings":"","what":"Brute Force","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"comparison exact results, also brute_force_knn brute_force_knn_query functions, generate exact nearest neighbors simple process trying every possible pair dataset. Obviously becomes time consuming process dataset grows size, even multithreading (although iris dataset example doesn’t present issues).","code":"iris_exact_nn <-   brute_force_knn(irism,     k = 15,     metric = \"euclidean\",     n_threads = 4   )"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"merging","dir":"","previous_headings":"","what":"Merging","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Also available two functions merging multiple approximate nearest neighbor graphs, result new graph least good best graph provided. merging pairs graphs, use merge_knn: two graphs stored memory, ’s efficient use list-based version, merge_knnl:","code":"set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1)  # Merged results should be an improvement over either individual results iris_mnn <- merge_knn(iris_rnn1, iris_rnn2) sum(iris_mnn$dist) < sum(iris_rnn1$dist) sum(iris_mnn$dist) < sum(iris_rnn2$dist) set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn3 <- nnd_knn(iris, k = 15, n_iters = 1)  iris_mnn <- merge_knnl(list(iris_rnn1, iris_rnn2, iris_rnn3))"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"hubness-diagnostic","dir":"","previous_headings":"","what":"Hubness Diagnostic","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"k_occur function takes idx matrix returns vector k-occurrences item dataset. just number times item found k-nearest neighbor list another item. think idx matrix representing directed graph element idx[, j] matrix edge node node idx[, j], k_occurrences calculated reversing edge counts number edges incident node. Alternatively, nomenclature nearest neighbor descent, ’s size “reverse neighbor” list node. k-occurrence can take value 0 N number items dataset. Values much larger k indicate item potentially hub. presence hubs dataset can reduce accuracy approximate nearest neighbors returned nearest neighbor descent, presence hubs determined distribution k-occurrences quite robust even case approximate nearest neighbor graph low accuracy. Therefore calculating k-occurrences output nearest neighbor descent worth : maximum k-occurrence lot larger k (suggest 10 * k danger sign), accuracy approximate nearest neighbors may compromised. Items low k-occurrences likely affected way. Increasing k max_candidates parameter can help situations. Alternatively, querying data graph_knn_query can help: hubness nearest neighbors, see example Radovanović co-workers, 2010 Bratić co-workers, 2019.","code":"iris_nnd <- nnd_knn(iris, k = 15) kos <- k_occur(iris_nnd$idx) # Purposely don't do a very good job with NND so we have something to improve iris_nnd <- nnd_knn(iris, k = 15, n_iters = 1) iris_search_graph <-   prepare_search_graph(iris, iris_nnd)  # query and reference are the same iris_query_nn <-   graph_knn_query(     query = iris,     reference = iris,     reference_graph = iris_search_graph,     init = iris_nnd,     k = 15   ) # Compare  sum(iris_nnd$dist) sum(iris_query_nn$dist)"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"supported-metrics","dir":"","previous_headings":"","what":"Supported Metrics","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Euclidean, Manhattan, Cosine, Correlation (1 - Pearson correlation, implemented cosine distance row-centered data) Hamming. Note implemented simple fashion, clever (non-portable) optimizations using AVX/SSE specialized popcount routines used.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World wide web (pp. 577-586). ACM. doi.org/10.1145/1963405.1963487.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"R package whole licensed GPLv3 later. following files licensed differently: inst/include/dqsample.h modification sampling code dqrng AGPLv3 later. inst/include/RcppPerpendicular.h modification code RcppParallel GPLv2 later underlying nearest neighbor descent C++ library, can found inst/include/tdoann, licensed BSD 2-Clause. far know, licenses compatible re-licensing GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"see-also","dir":"","previous_headings":"","what":"See Also","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"PyNNDescent, Python implementation. nndescent, C++ implementation. NearestNeighborDescent.jl, Julia implementation. nn_descent, C implementation. NNDescent.cpp, another C++ implementation. nndescent, another C++ implementation, Python bindings.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"Calculate Exact Nearest Neighbors Brute Force","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"","code":"brute_force_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. k Number nearest neighbors return. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocessing\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". \"bhamming\" (hamming binary data bitset internal memory optimization). use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"","code":"# Find the 4 nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- brute_force_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- brute_force_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- brute_force_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- brute_force_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 22:28:14 Calculating brute force k-nearest neighbors with k = 4 #> 22:28:14 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"Query Exact Nearest Neighbors Brute Force","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"","code":"brute_force_knn_query(   query,   reference,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed orientation reference. k Number nearest neighbors return. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". \"bhamming\" (hamming binary data bitset internal memory optimization). use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices reference. dist n k matrix containing nearest neighbor distances items reference.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # For each item in iris_query find the 4 nearest neighbors in iris_ref # If you pass a data frame, non-numeric columns are removed # set verbose = TRUE to get details on the progress being made iris_query_nn <- brute_force_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 22:28:15 Calculating brute force k-nearest neighbors from reference with k = 4 #> 22:28:15 Finished  # Manhattan (l1) distance iris_query_nn <- brute_force_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"manhattan\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/correlation_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the correlation distance between two vectors — correlation_distance","title":"Find the correlation distance between two vectors — correlation_distance","text":"Find correlation distance two vectors","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/correlation_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the correlation distance between two vectors — correlation_distance","text":"","code":"correlation_distance(x, y)"},{"path":"https://jlmelville.github.io/rnndescent/reference/correlation_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the correlation distance between two vectors — correlation_distance","text":"x numeric vector. y numeric vector length x.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/cosine_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the cosine distance between two vectors — cosine_distance","title":"Find the cosine distance between two vectors — cosine_distance","text":"Find cosine distance two vectors","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/cosine_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the cosine distance between two vectors — cosine_distance","text":"","code":"cosine_distance(x, y)"},{"path":"https://jlmelville.github.io/rnndescent/reference/cosine_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the cosine distance between two vectors — cosine_distance","text":"x numeric vector. y numeric vector length x.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/euclidean_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the Euclidean (L2) distance between two vectors — euclidean_distance","title":"Find the Euclidean (L2) distance between two vectors — euclidean_distance","text":"Find Euclidean (L2) distance two vectors","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/euclidean_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the Euclidean (L2) distance between two vectors — euclidean_distance","text":"","code":"euclidean_distance(x, y)"},{"path":"https://jlmelville.github.io/rnndescent/reference/euclidean_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the Euclidean (L2) distance between two vectors — euclidean_distance","text":"x numeric vector. y numeric vector length x.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Nearest Neighbors and Distances — graph_knn_query","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"Find Nearest Neighbors Distances","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"","code":"graph_knn_query(   query,   reference,   reference_graph,   k = NULL,   metric = \"euclidean\",   init = NULL,   epsilon = 0.1,   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed orientation reference. reference_graph Search graph reference data. neighbor graph, output nnd_knn() can used, preferably suitably prepared sparse search graph used, output prepare_search_graph(). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One \"euclidean\", \"l2sqr\" (squared Euclidean), \"cosine\", \"manhattan\", \"correlation\" (1 minus Pearson correlation), \"hamming\" \"bhamming\" (hamming binary data bitset internal memory optimization). init Initial query neighbor graph optimize. provided, k random neighbors created. provided, input format list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. input distances omitted, calculated . epsilon Controls trade-accuracy search cost, specifying distance tolerance whether explore neighbors candidate points. larger value, neighbors searched. value 0.1 allows query-candidate distances 10% larger current -distant neighbor query point, 0.2 means 20%, . Suggested values 0-0.5, although value highly dependent distribution distances dataset (higher dimensional data choose smaller cutoff). large value epsilon result query search approaching brute force comparison. Use parameter conjunction prepare_search_graph() prevent excessive run time. Default 0.1. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices specifying row neighbor reference. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"Hajebi, K., Abbasi-Yadkori, Y., Shahbazi, H., & Zhang, H. (2011, June). Fast approximate nearest-neighbor search k-nearest neighbor graph. Twenty-Second International Joint Conference Artificial Intelligence. Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722). Iwasaki, M., & Miyazaki, D. (2018). Optimization indexing based k-nearest neighbor graph proximity search high-dimensional data. arXiv preprint arXiv:1810.07355.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # First, find the approximate 4-nearest neighbor graph for the references: iris_ref_graph <- nnd_knn(iris_ref, k = 4)  # For each item in iris_query find the 4 nearest neighbors in iris_ref. # You need to pass both the reference data and the reference graph. # If you pass a data frame, non-numeric columns are removed. # set verbose = TRUE to get details on the progress being made iris_query_nn <- graph_knn_query(iris_query, iris_ref, iris_ref_graph,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 22:28:15 Initializing from random neighbors #> 22:28:15 Generating random k-nearest neighbor graph from reference with k = 4 #> 22:28:15 Finished #> 22:28:16 Searching nearest neighbor graph #> 22:28:16 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/hamming_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the Hamming distance between two vectors — hamming_distance","title":"Find the Hamming distance between two vectors — hamming_distance","text":"Find Hamming distance two vectors","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/hamming_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the Hamming distance between two vectors — hamming_distance","text":"","code":"hamming_distance(x, y)"},{"path":"https://jlmelville.github.io/rnndescent/reference/hamming_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the Hamming distance between two vectors — hamming_distance","text":"x integer vector. y integer vector length x.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":null,"dir":"Reference","previous_headings":"","what":"k-Occurrence — k_occur","title":"k-Occurrence — k_occur","text":"Calculates k-occurrence given nearest neighbor matrix","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"k-Occurrence — k_occur","text":"","code":"k_occur(idx, k = NULL, include_self = TRUE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-Occurrence — k_occur","text":"idx integer matrix containing nearest neighbor indices, integers labeled starting 1. Note integer labels refer rows idx, example nearest neighbor result querying one set objects respect another (instance running graph_knn_query()). may also pass nearest neighbor graph object (e.g. output running nnd_knn()), indices extracted , sparse matrix format returned prepare_search_graph(). k number closest neighbors use. Must 1 number columns idx. default, columns idx used. Ignored idx sparse. include_self logical indicating whether label idx considered valid neighbor found row . default TRUE. can set FALSE labels idx refer row indices idx, case results nnd_knn(). case may want consider trivial case object neighbor . cases leave set TRUE.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"k-Occurrence — k_occur","text":"vector length max(idx), containing number times object idx found nearest neighbor list objects represented row indices idx.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"k-Occurrence — k_occur","text":"k-occurrence object number times occurs among k-nearest neighbors objects dataset. can take values 0 size dataset. larger k-occurrence object, \"popular\" . large values k-occurrence (much larger k) indicates object \"hub\" also implies existence \"anti-hubs\": objects never appear k-nearest neighbors objects. presence hubs can reduce accuracy nearest-neighbor descent approximate nearest neighbor algorithms terms retrieving exact k-nearest neighbors. However appearance hubs can still detected approximate results, calculating k-occurrences output nearest neighbor descent useful diagnostic step.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"k-Occurrence — k_occur","text":"Radovanovic, M., Nanopoulos, ., & Ivanovic, M. (2010). Hubs space: Popular nearest neighbors high-dimensional data. Journal Machine Learning Research, 11, 2487-2531. https://www.jmlr.org/papers/v11/radovanovic10a.html Bratic, B., Houle, M. E., Kurbalija, V., Oria, V., & Radovanovic, M. (2019). Influence Hubness NN-Descent. International Journal Artificial Intelligence Tools, 28(06), 1960002. https://doi.org/10.1142/S0218213019600029","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"k-Occurrence — k_occur","text":"","code":"iris_nbrs <- brute_force_knn(iris, k = 15) iris_ko <- k_occur(iris_nbrs$idx) # items 42 and 107 are not in 15 nearest neighbors of any other members of # iris # for convenience you can also pass iris_nbrs directly: # iris_ko <- k_occur(iris_nbrs) which(iris_ko == 1) # they are only their own nearest neighbor #> [1]  42 107 max(iris_ko) # most \"popular\" item appears on 29 15-nearest neighbor lists #> [1] 29 which(iris_ko == max(iris_ko)) # it's iris item 64 #> [1] 64 # with k = 15, a maximum k-occurrence = 29 ~= 1.9 * k, which is not a cause # for concern"},{"path":"https://jlmelville.github.io/rnndescent/reference/l2sqr_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the squared Euclidean (squared L2) distance between two vectors — l2sqr_distance","title":"Find the squared Euclidean (squared L2) distance between two vectors — l2sqr_distance","text":"Find squared Euclidean (squared L2) distance two vectors","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/l2sqr_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the squared Euclidean (squared L2) distance between two vectors — l2sqr_distance","text":"","code":"l2sqr_distance(x, y)"},{"path":"https://jlmelville.github.io/rnndescent/reference/l2sqr_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the squared Euclidean (squared L2) distance between two vectors — l2sqr_distance","text":"x numeric vector. y numeric vector length x.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/local_scale_nn.html","id":null,"dir":"Reference","previous_headings":"","what":"Locally Scaled Nearest Neighbors — local_scale_nn","title":"Locally Scaled Nearest Neighbors — local_scale_nn","text":"Find subset nearest neighbors smallest generalized scaled distance (Zelnik-Manor Perona, 2004) selection criterion. means reduce influence hub points, suggested Schnitzer co-workers (2012).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/local_scale_nn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Locally Scaled Nearest Neighbors — local_scale_nn","text":"","code":"local_scale_nn(nn, k = 15, k_scale = 2, ret_scales = FALSE, n_threads = 0)"},{"path":"https://jlmelville.github.io/rnndescent/reference/local_scale_nn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Locally Scaled Nearest Neighbors — local_scale_nn","text":"nn Nearest neighbor data dense list format. k scaled neighbors chosen candidates provided , size neighborhoods nn least k. k size desired scaled neighborhood. k_scale neighbor nn use scale distances. May single value (1 size neighborhood) vector two values indicating inclusive range neighbors use. latter case, average distance neighbors range used scale distances. ret_scales TRUE return vector local scales (average distance based k_scale observation nn). n_threads number threads use parallel processing.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/local_scale_nn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Locally Scaled Nearest Neighbors — local_scale_nn","text":"scaled k nearest neighbors dense list format. distances returned unscaled distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/local_scale_nn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Locally Scaled Nearest Neighbors — local_scale_nn","text":"Local scaling carried dividing distance k-th nearest neighbor, take account difference local distance statistics neighborhood. function supports choosing average distance range k, used Jegou co-workers (2007). Note scaled distances used select k neighbors larger list candidate neighbors, returned neighbor data uses original unscaled distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/local_scale_nn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Locally Scaled Nearest Neighbors — local_scale_nn","text":"Jegou, H., Harzallah, H., & Schmid, C. (2007, June). contextual dissimilarity measure accurate efficient image search. 2007 IEEE Conference Computer Vision Pattern Recognition (pp. 1-8). IEEE. Schnitzer, D., Flexer, ., Schedl, M., & Widmer, G. (2012). Local global scaling reduce hubs space. Journal Machine Learning Research, 13(10). Zelnik-Manor, L., & Perona, P. (2004). Self-tuning spectral clustering. Advances neural information processing systems, 17.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/local_scale_nn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Locally Scaled Nearest Neighbors — local_scale_nn","text":"","code":"set.seed(42) # 100 x 10 Gaussian data: exhibits mild hubness m <- matrix(rnorm(1000), nrow = 100, ncol = 10)  # Find 50 nearest neighbors to give a reasonable set of candidates nn50 <- brute_force_knn(m, k = 50)  # Using the 15-nearest neighbors, maximum k-occurrence > 15 indicates any # observations showing up more than expected nn15_hubness <- max(k_occur(nn50, k = 15))  # Find 15 locally scaled nearest neighbors from the 50 NN # use average distance to neighbors 5-7 to represent the local distance # statistics lnn15 <- local_scale_nn(nn50, k = 15, k_scale = c(5, 7))  lnn15_hubness <- max(k_occur(lnn15))  # hubness has been reduced lnn15_hubness < nn15_hubness # TRUE #> [1] TRUE"},{"path":"https://jlmelville.github.io/rnndescent/reference/manhattan_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the Manhattan (L1) distance between two vectors — manhattan_distance","title":"Find the Manhattan (L1) distance between two vectors — manhattan_distance","text":"Find Manhattan (L1) distance two vectors","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/manhattan_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the Manhattan (L1) distance between two vectors — manhattan_distance","text":"","code":"manhattan_distance(x, y)"},{"path":"https://jlmelville.github.io/rnndescent/reference/manhattan_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the Manhattan (L1) distance between two vectors — manhattan_distance","text":"x numeric vector. y numeric vector length x.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge two approximate nearest neighbors graphs — merge_knn","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"Merge two approximate nearest neighbors graphs","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"","code":"merge_knn(   nn_graph1,   nn_graph2,   is_query = FALSE,   n_threads = 0,   verbose = FALSE )"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"nn_graph1 nearest neighbor graph merge. consist list containing: idx n k matrix containing k nearest neighbor indices. dist n k matrix containing k nearest neighbor distances. nn_graph2 Another nearest neighbor graph merge format nn_graph1. number neighbors can differ graphs, merged result number neighbors specified nn_graph1. is_query TRUE graphs treated result knn query, knn building process. : graph bipartite? set TRUE nn_graphs results using e.g. graph_knn_query() random_knn_query(), set FALSE results nnd_knn() random_knn(). difference is_query = FALSE, index p found nn_graph1[, ], .e. p neighbor distance d, assumed neighbor p distance. is_query = TRUE, p indexes two different datasets symmetry hold. sure case applies , safe (potentially inefficient) set is_query = TRUE n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"list containing: idx n k matrix containing merged nearest neighbor indices. dist n k matrix containing merged nearest neighbor distances. size k output graph nn_graph1.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"","code":"set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1)  # Merged results should be an improvement over either individual results iris_mnn <- merge_knn(iris_rnn1, iris_rnn2) sum(iris_mnn$dist) < sum(iris_rnn1$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn2$dist) #> [1] TRUE"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge a list of approximate nearest neighbors graphs — merge_knnl","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"Merge list approximate nearest neighbors graphs","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"","code":"merge_knnl(nn_graphs, is_query = FALSE, n_threads = 0, verbose = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"nn_graphs list nearest neighbor graph merge. item list consist sub-list containing: idx n k matrix containing k nearest neighbor indices. dist n k matrix containing k nearest neighbor distances. number neighbors can differ graphs, merged result number neighbors first graph list. is_query TRUE graphs treated result knn query, knn building process. : graph bipartite? set TRUE nn_graphs results using e.g. graph_knn_query() random_knn_query(), set FALSE results nnd_knn() random_knn(). difference is_query = FALSE, index p found nn_graph1[, ], .e. p neighbor distance d, assumed neighbor p distance. is_query = TRUE, p indexes two different datasets symmetry hold. sure case applies , safe (potentially inefficient) set is_query = TRUE. n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"list containing: idx n k matrix containing merged nearest neighbor indices. dist n k matrix containing merged nearest neighbor distances. size k output graph first item nn_graphs.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"","code":"set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn3 <- nnd_knn(iris, k = 15, n_iters = 1)  # Merged results should be an improvement over individual results iris_mnn <- merge_knnl(list(iris_rnn1, iris_rnn2, iris_rnn3)) sum(iris_mnn$dist) < sum(iris_rnn1$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn2$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn3$dist) #> [1] TRUE  # and slightly faster than running: # iris_mnn <- merge_knn(iris_rnn1, iris_rnn2) # iris_mnn <- merge_knn(iris_mnn, iris_rnn3)"},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"Distances stored column-wise, .e. neighbors first observation nn first column matrix, neighbors second observation second column, .","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"","code":"nn_to_sparse(nn)"},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"nn nearest neighbor graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"data nn sparse distance matrix dgCMatrix format.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"Zero distances dropped. typical (non-bipartite) graph case, observation usually neighbor distance zero. distances retained output, hence number neighbors nn k, k - 1 neighbors stored sparse matrix.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"","code":"# find 4 nearest neighbors of first ten iris data i10nn <- brute_force_knn(iris[1:10, ], k = 4) # Nearest neighbors of each item is itself all(i10nn$idx[, 1] == 1:10) # TRUE #> [1] TRUE # Convert to sparse i10nnsp <- nn_to_sparse(i10nn) # 3 neighbors are retained in the sparse format because we drop 0 distances all(diff(i10nnsp@p) == 3) # TRUE #> [1] TRUE"},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Nearest Neighbors and Distances — nnd_knn","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"Find Nearest Neighbors Distances","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"","code":"nnd_knn(   data,   k = NULL,   metric = \"euclidean\",   init = \"rand\",   init_args = NULL,   n_iters = 10,   max_candidates = NULL,   delta = 0.001,   low_memory = TRUE,   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   progress = \"bar\",   obs = \"R\",   ret_forest = FALSE )"},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". \"bhamming\" (hamming binary data bitset internal memory optimization). init Name initialization strategy initial data neighbor graph optimize. One : \"rand\" random initialization (default). \"tree\" use random projection tree method Dasgupta Freund (2008). pre-calculated neighbor graph. list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. input distances omitted, calculated .' k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. init_args list containing arguments pass random partition forest initialization. See rpf_knn() possible arguments. n_iters Number iterations nearest neighbor descent carry . max_candidates Maximum number candidate neighbors try item iteration. Use relative k emulate \"rho\" sampling parameter nearest neighbor descent paper. default, set k 60, whichever smaller. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1% neighbor graph must updated iteration. low_memory TRUE, use lower memory, computationally expensive approach index construction. set FALSE, see noticeable speed improvement, especially using smaller number threads, worth trying memory spare. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. progress Determines type progress information logged verbose = TRUE. Options : \"bar\": simple text progress bar. \"dist\": sum distances approximate knn graph end iteration. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage. ret_forest TRUE init = \"tree\" RP forest used initialize nearest neighbors returned nearest neighbor data. See Value section details. returned forest can used part initializing search new data: see rpf_knn_query() rpf_filter() details.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. forest (init = \"tree\" ret_forest = TRUE ): RP forest used initialize neighbor data.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452. Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. https://doi.org/10.1145/1963405.1963487.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- nnd_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- nnd_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 22:28:17 Initializing neighbors using 'rand' method #> 22:28:17 Generating random k-nearest neighbor graph with k = 4 #> 22:28:17 Finished #> 22:28:17 Running nearest neighbor descent for 10 iterations #> 22:28:17 Finished  # Nearest neighbor descent uses random initialization, but you can pass any # approximation using the init argument (as long as the metrics used to # calculate the initialization are compatible with the metric options used # by nnd_knn). iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\") iris_nn <- nnd_knn(iris, init = iris_nn, metric = \"euclidean\", verbose = TRUE) #> 22:28:17 Running nearest neighbor descent for 10 iterations #> 22:28:17 Finished  # Number of iterations controls how much optimization is attempted. A smaller # value will run faster but give poorer results iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 2)  # You can also control the amount of work done within an iteration by # setting max_candidates iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", max_candidates = 50)  # Optimization may also stop early if not much progress is being made. This # convergence criterion can be controlled via delta. A larger value will # stop progress earlier. The verbose flag will provide some information if # convergence is occurring before all iterations are carried out. set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 5, delta = 0.5)  # To ensure that descent only stops if no improvements are made, set delta = 0 set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 5, delta = 0)  # A faster version of the algorithm is available that avoids repeated # distance calculations at the cost of using more RAM. Set low_memory to # FALSE to try it. set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", low_memory = FALSE)  # Using init = \"tree\" is usually more efficient than random initialization. # arguments to the tree initialization method can be passed via the init_args # list set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, init = \"tree\", init_args = list(n_trees = 5))"},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Nearest Neighbor Graph Refinement — prepare_search_graph","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"Create graph using existing nearest neighbor data balance search speed accuracy using occlusion pruning truncation strategies Harwood Drummond (2016).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"","code":"prepare_search_graph(   data,   graph,   metric = \"euclidean\",   diversify_prob = 1,   pruning_degree_multiplier = 1.5,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"data Matrix n items, observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. graph neighbor graph data, list containing: idx n k matrix containing nearest neighbor indices data data. dist n k matrix containing nearest neighbor distances. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". \"bhamming\" (hamming binary data bitset internal memory optimization). diversify_prob degree diversification search graph removing unnecessary edges occlusion pruning. take value 0 (diversification) 1 (remove many edges possible) treated probability neighbor removed found \"occlusion\". item p q, two members neighbor list item , closer , nearer neighbor p said \"occlude\" q. likely q neighbor list p need retain neighbor list . may also set NULL skip occlusion pruning. Note occlusion pruning carried twice, forward neighbors, reverse neighbors. pruning_degree_multiplier strongly truncate final neighbor list item. neighbor list item truncated retain closest d neighbors, d = k * pruning_degree_multiplier, k original number neighbors per item graph. Roughly, values larger 1 keep nearest neighbors item, plus given fraction reverse neighbors (exist). example, setting 1.5 keep forward neighbors half many reverse neighbors, although exactly neighbors retained also dependent occlusion pruning occurs. Set NULL skip step. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"search graph data based graph, represented sparse matrix, suitable use graph_knn_query().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"approximate nearest neighbor graph useful querying via graph_knn_query(), especially query data initialized randomly: items data set may nearest neighbor list item can therefore never returned neighbor, matter close query. Even appear least one neighbor list may reachable expanding arbitrary starting list neighbor graph contains disconnected components. Converting directed graph represented neighbor graph undirected graph adding edge item j edge exists j (.e. creating mutual neighbor graph) solves problems , can result inefficient searches. Although -degree item restricted number neighbors -degree restrictions: given item \"popular\" large number neighbors lists. Therefore mutualizing neighbor graph can result items large number neighbors search. usually similar neighborhoods nothing gained searching . balance accuracy search time, following procedure carried : graph \"diversified\" occlusion pruning. reverse graph formed reversing direction edges pruned graph. reverse graph diversified occlusion pruning. pruned forward pruned reverse graph merged. outdegree node merged graph truncated. truncated merged graph returned prepared search graph. Explicit zero distances graph converted small positive number avoid dropped sparse representation. one exception \"self\" distance, .e. edge graph links node (diagonal sparse distance matrix). trivial edges useful search purposes always dropped.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # First, find the approximate 4-nearest neighbor graph for the references: ref_ann_graph <- nnd_knn(iris_ref, k = 4)  # Create a graph for querying with ref_search_graph <- prepare_search_graph(iris_ref, ref_ann_graph)  # Using the search graph rather than the ref_ann_graph directly may give # more accurate or faster results iris_query_nn <- graph_knn_query(   query = iris_query, reference = iris_ref,   reference_graph = ref_search_graph, k = 4, metric = \"euclidean\",   verbose = TRUE ) #> 22:28:18 Initializing from random neighbors #> 22:28:18 Generating random k-nearest neighbor graph from reference with k = 4 #> 22:28:18 Finished #> 22:28:18 Searching nearest neighbor graph #> 22:28:18 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly select nearest neighbors. — random_knn","title":"Randomly select nearest neighbors. — random_knn","text":"Randomly select nearest neighbors.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly select nearest neighbors. — random_knn","text":"","code":"random_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   order_by_distance = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly select nearest neighbors. — random_knn","text":"data Matrix n items generate random neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. k Number nearest neighbors return. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". \"bhamming\" (hamming binary data bitset internal memory optimization). use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. order_by_distance TRUE (default), results item returned increasing distance. need results sorted, e.g. going pass results initialization another routine like nnd_knn(), set FALSE save small amount computational time. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly select nearest neighbors. — random_knn","text":"random neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly select nearest neighbors. — random_knn","text":"","code":"# Find 4 random neighbors and calculate their Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- random_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- random_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 22:28:18 Generating random k-nearest neighbor graph with k = 4 #> 22:28:18 Finished  # These results can be improved by nearest neighbors descent. You don't need # to specify k here because this is worked out from the initial input iris_nn <- nnd_knn(iris, init = iris_nn, metric = \"euclidean\", verbose = TRUE) #> 22:28:18 Running nearest neighbor descent for 10 iterations #> 22:28:18 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Nearest Neighbors Query by Random Selection — random_knn_query","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"Nearest Neighbors Query Random Selection","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"","code":"random_knn_query(   query,   reference,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   order_by_distance = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. reference Matrix m reference items, observations rows features columns. nearest neighbors queries randomly selected data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed orientation reference. k Number nearest neighbors return. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". \"bhamming\" (hamming binary data bitset internal memory optimization). use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. order_by_distance TRUE (default), results item returned increasing distance. need results sorted, e.g. going pass results initialization another routine like graph_knn_query(), set FALSE save small amount computational time. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # For each item in iris_query find 4 random neighbors in iris_ref # If you pass a data frame, non-numeric columns are removed # set verbose = TRUE to get details on the progress being made iris_query_random_nbrs <- random_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 22:28:18 Generating random k-nearest neighbor graph from reference with k = 4 #> 22:28:18 Finished  # Manhattan (l1) distance iris_query_random_nbrs <- random_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"manhattan\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Random Projection Forest — rpf_build","title":"Create a Random Projection Forest — rpf_build","text":"Build \"forest\" Random Projection Trees (Dasgupta Freund, 2008), can later searched find approximate nearest neighbors.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Random Projection Forest — rpf_build","text":"","code":"rpf_build(   data,   metric = \"euclidean\",   n_trees = NULL,   leaf_size = 10,   margin = \"auto\",   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Random Projection Forest — rpf_build","text":"data Matrix n items generate index , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". Note metric used determine whether \"angular\" \"Euclidean\" distance used measure distance split points tree. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. leaf_size maximum number items can appear leaf. value chosen match expected number neighbors want retrieve running queries (e.g. want find 50 nearest neighbors set leaf_size = 50) set value smaller 10. margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Random Projection Forest — rpf_build","text":"forest random projection trees list. tree forest list, intended examined manipulated user. normal R data type, can safely serialized deserialized base::saveRDS() base::readRDS(). use querying pass forest parameter rpf_knn_query(). forest store data passed build tree, going search forest, also need store data used build provide search.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a Random Projection Forest — rpf_build","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Random Projection Forest — rpf_build","text":"","code":"# Build a forest of 10 trees from the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_odd_forest <- rpf_build(iris_odd, n_trees = 10)  iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_even_nn <- rpf_knn_query(query = iris_even, reference = iris_odd,                               forest = iris_odd_forest, k = 15)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter a Random Projection Forest — rpf_filter","title":"Filter a Random Projection Forest — rpf_filter","text":"Reduce size random projection forest, scoring tree k-nearest neighbors graph. top N trees retained allows faster querying. Rather rely RP Forest solely approximate nearest neighbor querying, probably cost-effective use small number trees initialize search space use input search graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter a Random Projection Forest — rpf_filter","text":"","code":"rpf_filter(nn, forest = NULL, n_trees = 1, n_threads = 0, verbose = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter a Random Projection Forest — rpf_filter","text":"nn Nearest neighbor data dense list format. derived data used build forest. forest random partition forest, e.g. created rpf_build(), representing partitions underlying data reflected nn. convenient, parameter ignored nn list contains forest entry, e.g. running rpf_knn() nnd_knn() ret_forest = TRUE, forest value extracted nn. n_trees number trees retain. default best-scoring tree retained. n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter a Random Projection Forest — rpf_filter","text":"forest best scoring n_trees trees.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Filter a Random Projection Forest — rpf_filter","text":"Trees scored based well leaf reflects neighbors specified nearest neighbor data. best use accurate nearest neighbor data can need come directly searching forest: example, nearest neighbor data running nnd_knn() optimize neighbor data output RP Forest good choice.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter a Random Projection Forest — rpf_filter","text":"","code":"# Build a knn with a forest of 10 trees using the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] # also return the forest with the knn rfknn <- rpf_knn(iris_odd, k = 15, n_trees = 10, ret_forest = TRUE)  # keep the best 2 trees: iris_odd_filtered_forest <- rpf_filter(rfknn)  # get some new data to search iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ]  # search with the filtered forest iris_even_nn <- rpf_knn_query(query = iris_even, reference = iris_odd,                               forest = iris_odd_filtered_forest, k = 15)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"Find approximate nearest neighbors using \"forest\" Random Projection Trees (Dasgupta Freund, 2008).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"","code":"rpf_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_trees = NULL,   leaf_size = NULL,   include_self = TRUE,   ret_forest = FALSE,   margin = \"auto\",   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". \"bhamming\" (hamming binary data bitset internal memory optimization). use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. leaf_size maximum number items can appear leaf. default NULL means number leaves chosen based number requested neighbors k. include_self TRUE (default) item considered neighbor . Hence first nearest neighbor results item . convention many nearest neighbor methods software adopt, want use resulting knn graph function downstream applications compare methods, probably keep set TRUE. However, planning using result initialization another nearest neighbor method (e.g. nnd_knn()), set FALSE. ret_forest TRUE also return search forest can used future querying (via rpf_knn_query()) filtering (via rpf_filter()). default FALSE. Setting TRUE change output list nested (see Value section ). margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. forest (ret_forest = TRUE) RP forest generated neighbor graph, can used query new data. k neighbors per observation guaranteed found. Missing data represented index 0 distance NA.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- rpf_knn(iris, k = 4, metric = \"euclidean\", leaf_size = 3)  # If you want to initialize another method (e.g. nearest neighbor descent) # with the result of the RP forest, then it's more efficient to skip # evaluating whether an item is a neighbor of itself by setting # `include_self = FALSE`: iris_rp <- rpf_knn(iris, k = 4, n_trees = 3, include_self = FALSE) # Use it with e.g. `nnd_knn` -- this should be better than a random start iris_nnd <- nnd_knn(iris, k = 4, init = iris_rp) # but note you can also run nnd_knn(iris, k = 4, init = \"tree\") to initialize # from an RP forest directly  # for future querying you may want to also return the RP forest: iris_rpf <- rpf_knn(iris, k = 4, n_trees = 3, include_self = FALSE,                     ret_forest = TRUE) # forest and nn data can be used to create a smaller forest for querying # filtered_forest <- rpf_filter(iris_rpf)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Search a Random Projection Forest — rpf_knn_query","title":"Search a Random Projection Forest — rpf_knn_query","text":"Run queries \"forest\" Random Projection Trees (Dasgupta Freund, 2008), return nearest neighbors reference data used build forest.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search a Random Projection Forest — rpf_knn_query","text":"","code":"rpf_knn_query(   query,   reference,   forest,   k,   metric = \"euclidean\",   cache = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search a Random Projection Forest — rpf_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data data used build forest. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed orientation reference. forest random partition forest, created rpf_build(), representing partitions data reference. k Number nearest neighbors return. unlikely get good results choose value substantially larger value leaf_size used build forest. metric Type distance calculation use. One : \"euclidean\". \"l2sqr\" (squared Euclidean). \"cosine\". \"cosine-preprocess\": cosine preprocessing: trades memory potential speed distance calculation.give results cosine, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"manhattan\". \"correlation\" (1 minus Pearson correlation). \"correlation-preprocess\": correlation preprocessing. trades memory potential speed distance calculation. give results correlation, give take minor numerical changes. aware distance two identical items may always give exactly zero method. \"hamming\". Note metric used determine whether \"angular\" \"Euclidean\" distance used measure distance split points tree. cache TRUE (default) candidate indices found leaves forest cached avoid recalculating distance repeatedly. incurs extra memory cost scales n_threads. Set FALSE disable distance caching. n_threads Number threads use. Note parallelism search done observations query trees forest. Thus single observation see speed-increasing n_threads. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search a Random Projection Forest — rpf_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. k neighbors per observation guaranteed found. Missing data represented index 0 distance NA.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Search a Random Projection Forest — rpf_knn_query","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search a Random Projection Forest — rpf_knn_query","text":"","code":"# Build a forest of 10 trees from the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_odd_forest <- rpf_build(iris_odd, n_trees = 10)  iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_even_nn <- rpf_knn_query(query = iris_even, reference = iris_odd,                               forest = iris_odd_forest, k = 15)"},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-12","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.12","text":"New function: rpf_knn. Calculates approximate k-nearest neighbors using random partition forest. New function: rpf_build. Builds random partition forest. New function: rpf_knn_query. Queries random partition forest (built rpf_build find approximate nearest neighbors query points. New function: rpf_filter. Retains best “scoring” trees forest, tree scored based well reproduces given knn. New initialization method nnd_knn: init = \"tree\". Uses RP Forest initialization method. New parameter nnd_knn: ret_forest. Returns search forest used init = \"tree\" can used future searching filtering. New parameter nnd_knn: init_opts. Options can passed RP forest initialization (rpf_knn).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-11","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.11","text":"Progess report nnd_knn n_threads > 0 reporting double actual number iterations. made progress bar way optimistic. bug flagging neighbors 0.0.10 made nearest neighbor descent inefficient.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-10","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.10","text":"change metric: \"cosine\" \"correlation\" renamed \"cosine-preprocess\" \"correlation-preprocess\" respectively. reflects preprocessing data front make subsequent distance calculations faster. endeavored avoid unnecessary allocations copying preprocessing, still chance memory usage. cosine correlation metrics still available option, now use implementation doesn’t preprocessing. preprocessing non-preprocessing version give numerical results, give take minor numerical differences, distance zero, preprocessing versions may give values slightly different zero (e.g. 1e-7). New functions: correlation_distance, cosine_distance, euclidean_distance, hamming_distance, l2sqr_distance, manhattan_distance calculating distance two vectors, may useful arbitrary distance calculations nearest neighbor routines , although won’t efficient (call C++ code, though). cosine correlation calculations use non-preprocessing implementations. Generalize hamming metric standard definition. old implementation hamming metric worked binary data renamed bhamming. (contributed Vitalie Spinu) New parameter obs added functions: set obs = \"C\" can pass input data column-oriented format.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-10","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.10","text":"random_knn function used always return item neighbor, n_nbrs - 1 returned neighbors actually selected random. Even forgot doesn’t make lot sense, now really just get back n_nbrs random selections. providing pre-calculated neighbors init parameter nnd_knn graph_knn_query: previously, k specified larger number neighbors included init, gave error. Now, init augmented random neighbors reach desired k. useful way “restart” neighbor search better--random location k found small initially. Note random selection take account identities already chosen neighbors, duplicates may included augmented result, reduce effective size initialized number neighbors. Removed block_size grain_size parameters functions. related amount work done per thread, ’s obvious outside user set . long-running computations update progress indicators frequently (verbose = TRUE) respond user-requested cancellation.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-9","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.9 (20 June 2021)","text":"nnd_knn_query renamed graph_knn_query now closely follows current pynndescent graph search method (including backtracking search). New function: prepare_search_graph preparing search graph neighbor graph use graph_knn_query, using reverse nearest neighbors, occlusion pruning truncation. Sparse graphs supported input graph_knn_query.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-008-10-october-2020","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.8 (10 October 2020)","title":"rnndescent 0.0.8 (10 October 2020)","text":"major rewrite internal organization C++ less R-specific.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"minor-license-change-0-0-8","dir":"Changelog","previous_headings":"","what":"Minor License Change","title":"rnndescent 0.0.8 (10 October 2020)","text":"license rnndescent changed GPLv3 GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-8","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.8 (10 October 2020)","text":"New metric: \"correlation\". (1 minus) Pearson correlation. New function: k_occur counts k-occurrences item idx matrix, number times item appears k-nearest neighbor list dataset. distribution k-occurrences can used diagnose “hubness” dataset. Items large k-occurrence (>> k, e.g. 10k), may indicate low accuracy approximate nearest neighbor result.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-7","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.7 (1 March 2020)","text":"avoid undefined behavior issues, rnndescent now uses internal implementation RcppParallel’s parallelFor loop works std::thread load Intel’s TBB library.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-6","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.6 (29 November 2019)","text":"reason, thought ok use dqrng sample routines inside thread, despite clearly using R API extensively. ’s ok causes lots crashes. now re-implementation dqrng’s sample routines using plain std::vectors src/rnn_sample.h. file licensed AGPL (rnndescent whole remains GPL3).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-5","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.5 (23 November 2019)","text":"New function: merge_knn, combine two nearest neighbor graphs. Useful combining results multiple runs nnd_knn random_knn. Also, merge_knnl, operates list multiple neighbor graphs, can provide speed merge_knn don’t mind storing multiple graphs memory .","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-5","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.5 (23 November 2019)","text":"thread-locking issue converting R matrices internal heap data structure affected nnd_knn n_threads > 1 random_knn n_threads > 1 order_by_distance = TRUE. Potential minor speed improvement nnd_knn n_threads > 1 due use mutex pool.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-004-21-november-2019","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.4 (21 November 2019)","title":"rnndescent 0.0.4 (21 November 2019)","text":"Mainly internal clean-reduce duplication.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-4","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.4 (21 November 2019)","text":"default, nnd_knn nnd_knn_query use progress bar brute force random neighbor functions. Bring back old per-iteration logging also showed current distance sum knn progress = \"dist\" option. random_knn random_knn_query, order_by_distance = TRUE n_threads > 0, final sorting knn graph multi-threaded. Initialization nearest neighbor descent data structures also multi-threaded n_threads > 0. Progress bar updating cancellation now consistent less likely cause hanging crashing across different methods. Using Cosine Hamming distance may take less memory run bit faster.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-3","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.3 (15 November 2019)","text":"now “query” versions three functions: nnd_knn_query useful, brute_force_knn_query random_knn_query also available. allows query data search reference data, .e. returned indices distances relative reference data, member query. methods also available multi-threaded mode, nnd_knn_query low high memory version.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-3","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.3 (15 November 2019)","text":"Incremental search nearest neighbor descent didn’t work correctly, retained new neighbors marked new rather old. made search repeat distance calculations unnecessarily. Heap initialization ignored existing distances input distance matrix. l2 metric renamed l2sqr accurately reflect : square L2 (Euclidean) metric. New option use_alt_metric. Set FALSE don’t want alternative, faster metrics (keep distance ordering metric) used internal calculations. Currently applies metric = \"euclidean\", squared Euclidean distance used internally. worth setting FALSE think alternative causing numerical issues (bug, please report !). Random brute force methods make use alternative metrics. New option block_size parallel methods, determines amount work done parallel checking user interrupt request updating progress. random_knn now returns results sorted order. can turn order_distances = FALSE, don’t need sorting (e.g. using results input something else). Progress bars brute_force random methods now correct.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-2","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.2 (7 November 2019)","text":"Brute force nearest neighbor function renamed brute_force_knn. Random nearest neighbors renamed random_knn. Brute force random nearest neighbors now interruptible. Progress bar shown verbose = TRUE. fast_rand option removed, applied single-threading, negligible effect. Also, number changes inspired recent work https://github.com/lmcinnes/pynndescent: parallel nearest neighbor descent now faster. rho sampling parameter removed. size candidates (general neighbors) list now controlled entirely max_candidates. Default max_candidates reduced 20. use_set logical flag replaced low_memory, opposite meaning. now also works using multiple threads. follows pynndescent implementation, ’s still experimental, low_memory = TRUE default moment. low_memory = FALSE implementation n_threads = 0 (originally equivalent use_set = TRUE) faster. New parameter block_size, balances interleaving queuing updates versus applying current graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-2","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.2 (7 November 2019)","text":"incremental search, neighbors marked selected new candidate list even later removed due finite size candidates heap. Now, indices still retained candidate building marked new. Improved man pages (examples plus link nearest neighbor descent reference). Removed dependency Boost headers.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-001-27-october-2019","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.1 (27 October 2019)","title":"rnndescent 0.0.1 (27 October 2019)","text":"Initial release.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-1","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.1 (27 October 2019)","text":"Nearest Neighbor Descent following metrics: Euclidean, Cosine, Manhattan, Hamming. Support multi-threading RcppParallel. Initialization random neighbors. Brute force nearest neighbor calculation.","code":""}]
