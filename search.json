[{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://jlmelville.github.io/rnndescent/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) 2019 James Melville  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. rnndescent Copyright (C) 2019 James Melville This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"comparing-low--and-high-dimensional-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Comparing Low- and High-Dimensional Nearest Neighbors","title":"Hubness","text":"Let’s look distribution nearest neighbor distances high low dimensions (easier comparison, normalized respect largest distance)   Compared low dimensional data, can see high dimensional distances distributed around higher distance well symmetric distribution. distribution neighbor distances high-dimensional case neighbors found NND:  Pretty much indistinguishable exact results, seems like isn’t obvious diagnostic distances . distribution errors NND results uniform items neighborhoods noticeably better predicted others? function calculate vector relative RMS error two sets neighbors terms distances: don’t include first nearest neighbor distances, invariably self distances leads uninteresting number zero error results. measure error bit less strict nn_accuracyv neighbor outside true kNN, comparable distance, penalized less harshly distant point. ’s histogram RRMS distance errors:  None relative errors actually large, care value kth nearest neighbor distances, even 1000D case, still get decent results case. can also see clear distribution errors, appreciable number items zero RRMS distance errors, items largest error. histograms accuracies:  shows similar pattern: items high accuracy noticeably worse accuracy average. completeness, relationship accuracy RRMSE:  Nothing surprising : ’s fairly consistent spread RRMSE given accuracy. whether use error distance accuracy measure well approximate nearest neighbors method working, least case, high dimensional dataset affects items others.","code":"hist(g2d_nnbf$dist[, -1] / max(g2d_nnbf$dist[, -1]), xlab = \"distances\", main = \"2D 15-NN\") hist(g1000d_nnbf$dist[, -1] / max(g1000d_nnbf$dist[, -1]), xlab = \"distances\", main = \"1000D 15-NN\") hist(g1000d_nnd$dist[, -1] / max(g1000d_nnd$dist[, -1]),   xlab = \"distances\",   main = \"1000D 15-NND\" ) nn_rrmsev <- function(nn, ref) {   n <- ncol(ref$dist) - 1   sqrt(apply((nn$dist[, -1] - ref$dist[, -1])^2 / n, 1, sum) /     apply(ref$dist[, -1]^2, 1, sum)) } g1000d_rrmse <- nn_rrmsev(g1000d_nnd, g1000d_nnbf) hist(g1000d_rrmse,   main = \"1000D distance error\",   xlab = \"Relative RMS error\" ) g1000d_nnd_acc <- nn_accuracyv(g1000d_nnbf, g1000d_nnd, k = 15) hist(g1000d_nnd_acc,   main = \"1000D accuracy\",   xlab = \"accuracy\",   xlim = c(0, 1) ) plot(   g1000d_nnd_acc,   g1000d_rrmse,   main = \"RRMSE vs accuracy\",   xlab = \"accuracy\",   ylab = \"RRMSE\" )"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"detecting-hubness","dir":"Articles","previous_headings":"","what":"Detecting Hubness","title":"Hubness","text":"(Radovanovic, Nanopoulos, Ivanovic 2010) discusses technique detecting hubness: look items appear frequently k-nearest neighbor graph. k_occur function counts “k-occurrences” item dataset, .e. count number times item appears k-nearest neighbor graph. can also see reversing direction edges k-nearest neighbor graph counting -degree item. distribution neighbors entirely uniform expect see item appear \\(k\\) times. hubs k-occurrence get large size dataset, \\(N\\). item appears neighbor graph fewer \\(k\\) times termed “antihub”. definition neighbor item always includes item , expect minimum \\(k\\)-occurrence \\(1\\). Also, always \\(Nk\\) edges \\(k\\)-nearest neighbor graph, item appears expected amount implies items must -represented. Practically speaking, always going items larger \\(k\\)-occurrence expected hence lower \\(k\\)-occurrence, hubness anti-hubness case deciding cut-presence item lot neighbors starts causing problems, going dependent planning neighbor graph (probably number neighbors want).","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"k-occurrence-in-the-2d-case","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence in the 2D case","title":"Hubness","text":"First, let’s look 2D case using exact k-nearest neighbors: mean average \\(k\\)-occurrence never helpful: noted always \\(Nk\\) edges neighbor graph, mean \\(k\\)-occurrence always \\(k\\). However descriptions distribution informative. median \\(k\\)-occurrence also 15, good sign, values 25% 75% aren’t different . maximum \\(k\\)-occurrence less \\(2k\\). minimum value 1 means anti-hubs dataset, : one anti-hub dataset. ’s histogram k-occurrences:  unremarkable-looking distribution visual indication dataset without lot hubness anti-hubs lurking cause problems nearest neighbor descent.","code":"g2d_bfko <- k_occur(g2d_nnbf, k = 15) summary(g2d_bfko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1      13      15      15      17      24 sum(g2d_bfko == 1) #> [1] 1 hist(g2d_bfko, main = \"2D 15-NN\", xlab = \"k-occurrences\")"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"k-occurrence-in-the-1000d-case","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence in the 1000D case","title":"Hubness","text":"’s k-occurrence histogram looks like high dimensional case:  differences pretty stark. first thing notice x-axis. 2D case, maximum k-occurrence ~20. 1000D looking ~300. ’s hard see details, let’s zoom region 2D case clipping k-occurrence larger largest 2D k-occurrence:  ’s different distribution 2D case: large number anti-hubs noticeable number hubs. ’s certainly peak k-occurrence 15. Comparing numerical summary 2D case instructive: , ’s good reminder mean k-occurrence value. median k-occurrence immediately communicates difference 2D case. can also see maximum k-occurrence means one point considered close neighbor one third dataset. many anti-hubs ? quarter dataset appear neighbor point. serious implications using neighbor graph certain purposes: reach quarter dataset starting arbitrary point graph following neighbors. also might point nearest neighbor descent trouble high dimensional case: rely points turning neighbors points order introduce potential neighbors, fact many points dataset aren’t anyone’s actual neighbors suggest unlikely get involved local join procedure much points.","code":"g1000d_bfko <- k_occur(g1000d_nnbf$idx, k = 15) hist(g1000d_bfko, main = \"1000D 15-NN\", xlab = \"k-occurrences\") hist(pmin(g1000d_bfko, max(g2d_bfko)),   main = \"1000D 15-NN zoomed\",   xlab = \"k-occurrences\" ) summary(g1000d_bfko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    1.00    2.00    5.00   15.00   14.25  345.00 sum(g1000d_bfko == 1) #> [1] 221"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"k-occurrence-as-a-diagnostic-of-nnd-failure","dir":"Articles","previous_headings":"Detecting Hubness","what":"k-occurrence as a diagnostic of NND failure","title":"Hubness","text":"now shown can use k-occurrences exact nearest neighbors low high dimensional data detect existence hubs, turn might lead us suspect approximate nearest neighbors found nearest neighbor descent may accurate. ’s useful diagnostic exact neighbors don’t need run NND first place. even approximate nearest neighbor graph produced NND isn’t highly accurate, still show similar characteristics hubness?  seems similar true results, zooming like exact results:  Visually looks lot like distribution exact results. Next, numerical summary: Quantitatively, also tracks exact results: median k-occurrence much smaller \\(k\\), hub large number neighbors (larger exact case similar degree) similar number anti-hubs. suggests way diagnose nearest neighbor descent routine may low accuracy: look distribution k-occurrences resulting approximate nearest neighbor graph (even just maximum value). value \\(\\gg k\\) may mean reduced accuracy. course, isn’t foolproof, even NND perfect job still get sorts values, ’s starting point. Taking distribution k-occurrences whole, approximate results seem track exact results fairly well, seen, errors approximate results uniformly distributed across data. let’s see well NND k-occurrences “predict” exact results:  overall relationship seems strong. line plot x=y, can see high values k-occurrence NND results tend -estimate k-occurrence, large values hardly matters, ambiguity nodes hub-like. Zooming lower values k-occurrence:  seems tendency -estimate k-occurrence. Anti-hubs also perfectly identified, true anti-hubs appear small number times approximate neighbor graph.","code":"g1000d_nndko <- k_occur(g1000d_nnd$idx, k = 15) hist(g1000d_nndko, main = \"1000D 15-NND\", xlab = \"k-occurrences\") hist(pmin(g1000d_nndko, max(g2d_bfko)),   main = \"1000D 15-NND zoomed\",   xlab = \"k-occurrences\" ) summary(g1000d_nndko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       4      15      12     406 sum(g1000d_nndko == 1) #> [1] 254 plot(g1000d_nndko, g1000d_bfko,   xlab = \"approximate\", ylab = \"exact\",   xlim = c(0, max(g1000d_nndko, g1000d_bfko)),   ylim = c(0, max(g1000d_nndko, g1000d_bfko)),   main = \"1000D k-occ\" ) abline(a = 0, b = 1) cor(g1000d_nndko, g1000d_bfko, method = \"pearson\") #> [1] 0.9939508 plot(g1000d_nndko, g1000d_bfko,   xlab = \"approximate\", ylab = \"exact\",   xlim = c(0, max(g2d_bfko)),   ylim = c(0, max(g2d_bfko)),   main = \"1000D low k-occ\" ) abline(a = 0, b = 1)"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"detecting-poorly-predicted-neighbors","dir":"Articles","previous_headings":"Detecting Hubness","what":"Detecting Poorly Predicted Neighbors","title":"Hubness","text":"’ve seen objects neighbors predicted better others. Based everything ’ve seen far k-occurrences NND, reasonable wonder: items dataset poorly predicted neighbors anti-hubs (predicted exact)? least give us way detecting items likely low accuracy neighborhoods: perhaps treated specially (algorithm). ’s plot accuracy k-occurrences NND neighbors:  answer question “really”, trend. empty space lower right plot indicates items large k-occurrence (hubs) well predicted. k-occurrence 150, guaranteed perfectly predict neighborhood item. However, end k-occurrence spectrum, can see lower bound predicted accuracy plummet k-occurrence reduced, anti-hubs actually neighborhoods accurately predicted . Unfortunately means k-occurrence bit rough use predict poorly-predicted items. Let’s say wanted get items neighborhood less 90% accurate: ’s already quite lot items: three-quarters entire dataset. largest k-occurrence item dataset accuracy threshold? , guarantee found items might poorly predicted, need filter every item k-occurrence smaller value, even though know well-predicted: ’s dataset. dropped threshold 80 accuracy, help? bit, ’s still substantial majority dataset. whatever decided items wouldn’t saving huge amount effort. much idea. suggests can’t improve results , just effort identifying individual points filter , treat differently merge back final neighbor graph means just reprocessing entire dataset different way likely competitive solution.","code":"plot(g1000d_nndko, g1000d_nnd_acc,   xlab = \"NND k-occ\", ylab = \"accuracy\",   xlim = c(0, max(g1000d_nndko, g1000d_bfko)),   main = \"1000D acc vs NND k-occ\" ) sum(g1000d_nnd_acc < 0.9) #> [1] 767 max(g1000d_nndko[g1000d_nnd_acc < 0.9]) #> [1] 48 sum(g1000d_nndko <= max(g1000d_nndko[g1000d_nnd_acc < 0.9])) #> [1] 929 sum(g1000d_nndko <= max(g1000d_nndko[g1000d_nnd_acc < 0.8])) #> [1] 860"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"detecting-problems-early","dir":"Articles","previous_headings":"Detecting Hubness","what":"Detecting Problems Early","title":"Hubness","text":"Back looking k-occurrence distribution whole: can see converged NND results, despite 100% accurate good job expressing hubness underlying data. converged results need ? think NND tool identifying hubness datasets whole rather accurate approximate nearest neighbor graphs? much less unconverged NND graph, obviously even less accurate, still correctly identify dataset hubs? test , let’s run NND method one iteration get k-occurrences result: accurate results? Ok, think can agree accurate neighbor graph. let’s take look k-occurrence distribution:  Looking familiar. Zooming …  distribution least similar converged version. Taking look numbers: Compared converged (exact) distribution, median k-occurrence low, object largest k-occurrence, large (\\(> 10k\\), seems like good threshold concerned presence hubs) large, fewer objects anti-hubs. least dataset, hubness can qualitatively detected even inaccurate neighbor graph. datasets don’t contain hubs? Let’s just check seeing artifact unconverged nearest neighbor descent, running procedure 2D dataset:  can see neighbor graph also accurate 1 iteration 2D case, distribution k-occurrences also qualitatively resembles exact result. time, compared exact results slightly anti-hubs maximum k-occurrence increased, trends slightly reversed compared 1000D data. least qualitative identification hubness, , one iteration nearest neighbor descent might enough.","code":"g1000d_nnd_iter1 <- nnd_knn(g1000d, k = 15, metric = \"euclidean\", n_iters = 1) g1000d_nndkoi1 <- k_occur(g1000d_nnd_iter1$idx, k = 15) nn_accuracy(g1000d_nnbf, g1000d_nnd_iter1, k = 15) #> [1] 0.3202 hist(g1000d_nndkoi1, main = \"1000D 15-NND (1 iter)\", xlab = \"k-occurrences\") hist(pmin(g1000d_nndkoi1, max(g2d_bfko)),   main = \"1000D 15-NND (1 iter, zoomed)\",   xlab = \"k-occurrences\" ) summary(g1000d_nndkoi1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       2       7      15      17     212 sum(g1000d_nndkoi1 == 1) #> [1] 163 g2d_nnd_iter1 <- nnd_knn(g2d, k = 15, metric = \"euclidean\", n_iters = 1) g2d_nndkoi1 <- k_occur(g2d_nnd_iter1$idx, k = 15) hist(g2d_nndkoi1, main = \"2D 15-NND (1 iter)\", xlab = \"k-occurrences\") summary(g2d_nndkoi1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1      11      15      15      19      33 sum(g2d_nndkoi1 == 1) #> [1] 4 nn_accuracy(g2d_nnbf, g2d_nnd_iter1, k = 15) #> [1] 0.3496667"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"improving-accuracy","dir":"Articles","previous_headings":"","what":"Improving accuracy","title":"Hubness","text":"know nearest neighbor descent (least typical settings) may give highly accurate results high dimensions. help k-occurrences, can even detect might happening. can ?","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"use-more-neighbors","dir":"Articles","previous_headings":"Improving accuracy","what":"Use More Neighbors","title":"Hubness","text":"One simple (slightly expensive) way keep neighbors calculation. example, double number neighbors 30, get top-15 accuracy: ’s big improvement, increasing k way can quite expensive terms run time.","code":"g1000d_nnd_k30 <- nnd_knn(g1000d, k = 30, metric = \"euclidean\") nn_accuracy(g1000d_nnbf, g1000d_nnd_k30, k = 15) #> [1] 0.9746667"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"merging-multiple-independent-results","dir":"Articles","previous_headings":"Improving accuracy","what":"Merging Multiple Independent Results","title":"Hubness","text":"taking advantage stochastic nature algorithm? results sufficiently diverse runs NND, generate two graphs two separate runs, merge results. Let’s repeat NND see accuracy new result like. ’s similar first run. ’s re-assuring sense variance accuracy doesn’t seem high one run next. hopefully doesn’t also mean NND producing similar neighbor graph time, case merging won’t helpful. Time find : ’s big improvement. seem like diversity results.  Despite similar overall accuracies, ’s quite large variance runs terms items accurate neighborhoods. might scope improving results merging different runs, especially can run individual NND routines parallel.","code":"g1000d_nnd_rep <- nnd_knn(g1000d, k = 15, metric = \"euclidean\") nn_accuracy(g1000d_nnbf, g1000d_nnd_rep, k = 15) #> [1] 0.7812 g1000d_nnd_merge <- merge_knn(g1000d_nnd, g1000d_nnd_rep) nn_accuracy(g1000d_nnbf, g1000d_nnd_merge, k = 15) #> [1] 0.9078667 g1000d_nnd_rep_acc <- nn_accuracyv(g1000d_nnbf, g1000d_nnd_rep, k = 15) plot(   g1000d_nnd_acc,   g1000d_nnd_rep_acc,   main = \"1000D NND accuracy comparison\",   xlab = \"accuracy run 1\",   ylab = \"accuracy run 2\" ) cor(g1000d_nnd_acc, g1000d_nnd_rep_acc) #> [1] 0.5265773"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"using-a-search-graph","dir":"Articles","previous_headings":"Improving accuracy","what":"Using a Search Graph","title":"Hubness","text":"Practically, simplest way improve results rnndescent convert neighbor graph search graph, query original data. First, preparation step: augments neighbor graph reversed edges neighbor graph, \\(\\) one nearest neighbors \\(j\\), guarantee \\(j\\) also considered near neighbor \\(\\). ameliorates issue anti-hubs \\(k\\) neighbors anti-hub now neighbor list. downside including reversed edges neighbor graph neighbor list hub now going large consists \\(k\\) nearest neighbors hub items consider hub near neighbor, definition lot. can make search graph inefficient, disproportionate amount time spent searching neighbors hub. diversify_prob pruning_degree_multiplier parameters used reduce back -degree node (number -going edges). results objects varying number neighbors, case maximum 22. 50% larger k = 15 account introduction reverse edges. Anti-hubs can reintroduced due edge reduction, hopefully distribution edges bit equitable. summary histogram k-occurrences search graph:  quite skewed neighbor graph, still lot room improvement. rate, search graph hand, can now search using original data query: results improved? Yes, accuracy now nearly perfect. disadvantages search graph approach building neighbor graph less efficient NND: graph_knn_query must assume query data entirely different reference data. advantage can make use reverse edges , importantly, back-tracking (controlled via epsilon parameter), seems make difference example. procedure recommended practice using graph_knn_query search graph generated neighbor graph. required use search graph argument reference_graph parameter. back-tracking search using neighbor graph directly everything else : Accuracies nearly good. can save even time turning back-tracking (epsilon = 0): accuracies now noticeably less improved. Using search graph without back-tracking gives slightly better accuracies: seems like sort back-tracking recommended approach.","code":"g1000d_search_graph <-   prepare_search_graph(     data = g1000d,     graph = g1000d_nnd,     metric = \"euclidean\",     diversify_prob = 1,     pruning_degree_multiplier = 1.5   ) g1000d_sgko <- k_occur(g1000d_search_graph) hist(g1000d_sgko, main = \"search graph k-occurrences\", xlab = \"k-occurrences\") summary(g1000d_sgko) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   1.000   4.000   5.000   7.186   9.000  22.000 sum(g1000d_sgko == 1) #> [1] 24 g1000d_search <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_search_graph,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0.1   ) nn_accuracy(g1000d_nnbf, g1000d_search, k = 15) #> [1] 0.9978 g1000d_nnd_search <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_nnd,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0.1   ) nn_accuracy(g1000d_nnbf, g1000d_nnd_search, k = 15) #> [1] 0.9845333 g1000d_nnd_search0 <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_nnd,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0   ) nn_accuracy(g1000d_nnbf, g1000d_nnd_search0, k = 15) #> [1] 0.8286667 g1000d_search0 <-   graph_knn_query(     query = g1000d,     reference = g1000d,     reference_graph = g1000d_search_graph,     k = 15,     metric = \"euclidean\",     init = g1000d_nnd,     epsilon = 0   ) nn_accuracy(g1000d_nnbf, g1000d_search0, k = 15) #> [1] 0.8467333"},{"path":"https://jlmelville.github.io/rnndescent/articles/hubness.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Hubness","text":"High dimensional data leads hubs. “hubness” item dataset can measured k-occurrence corresponding nearest neighbor graph. higher k-occurrence, hub . existence hubs implies existence “anti-hubs”, .e. items low k-occurrence. small number hubs can create disproportionately larger number anti-hubs, larger value k-occurrence creating anti-hubs. accuracy nearest neighbor descent reduced presence hubs: specifically, lower k-occurrence item, greater probability low accuracy nearest neighbors. Accuracy nearest neighbor descent can improved searching larger number neighbors truncating result desired size, cost longer run-time memory usage. Alternatively, can run nearest neighbor descent multiple times different random starting points merge results. accurate efficient results obtained converting nearest neighbor descent results search graph querying graph original data, using nearest neighbor results initialization back-tracking search. concerned potential hubs interfering accuracy neighbor graph, suggest following steps: Generate neighbor graph nnd_knn default parameters. Evaluate hubness graph k_occur. maximum k-occurrence exceeds threshold (maybe 10 * k good starting point), use prepare_search_graph graph_knn_query back-tracking search (set epsilon > 0) refine results . provide robust approach producing accurate approximate nearest neighbors without spending time unnecessary graph search results probably already quite good. effect hubness nearest neighbors, advanced attempts fix problem, see work Flexer co-workers (Schnitzer et al. 2012; Flexer 2016; Feldbauer Flexer 2019; Feldbauer, Rattei, Flexer 2019) Radovanović co-workers (Radovanovic, Nanopoulos, Ivanovic 2010; Bratić et al. 2019).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/metrics.html","id":"specialized-binary-metrics","dir":"Articles","previous_headings":"","what":"Specialized Binary Metrics","title":"Metrics","text":"metrics intended use binary data. means : numeric data consist two distinct values, typically 0 1. get unpredictable results otherwise. provide data logical matrix, much faster implementation used. metrics can use binary data : \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" ’s example using binary data stored 0s 1s \"hamming\" metric: Now let’s convert logical matrix: results : real-world dataset, logical version much faster.","code":"set.seed(42) binary_data <- matrix(sample(c(0, 1), 100, replace = TRUE), ncol = 10) head(binary_data) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #> [1,]    0    0    0    1    0    1    0    1    1     0 #> [2,]    0    1    0    1    0    0    1    0    0     1 #> [3,]    0    0    0    1    1    1    1    1    1     0 #> [4,]    0    1    0    1    1    1    1    1    0     1 #> [5,]    1    0    0    0    1    1    1    1    0     1 #> [6,]    1    0    1    1    1    1    1    1    1     1 nn <- brute_force_knn(binary_data, k = 4, metric = \"hamming\") logical_data <- binary_data == 1 head(logical_data) #>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] #> [1,] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE #> [2,] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE #> [3,] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE #> [4,] FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE #> [5,]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE #> [6,]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE logical_nn <- brute_force_knn(logical_data, k = 4, metric = \"hamming\") all.equal(nn, logical_nn) #> [1] TRUE"},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"local-join","dir":"Articles","previous_headings":"","what":"Local Join","title":"Nearest Neighbor Descent","text":"process described involves lot looping repeated fetching neighbor vectors, NND actually uses concept “local join”. One way think consider item fielding requests nearest neighbors. repeatedly asked item considers neighbor. work start iteration know items consider neighbor, can generate candidates neighbor pairs involved . need iterate items graph. need work finding considers neighbor also requires loop graph also. clear, amount work needs done, different order, everything bit efficient terms needs fetched memory. -shot using local join approach rather iterating graph one item time, end list pairs items (, j) update graph whole . dealing kNN graph pair (, j) also (j, ) potential update, cost one distance calculation. challenges terms parallel implementation also makes caching distances bit harder ’s still better naive approach explicitly looping neighbors--neighbors.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"other-heuristics","dir":"Articles","previous_headings":"","what":"Other Heuristics","title":"Nearest Neighbor Descent","text":"Additionally, two heuristics used reduce amount work done. first candidate neighbors split “new” “old” candidates. “new” candidate neighbor neighbor entered graph previous iteration. “Old” neighbors everything else. local join, possible pairs “new” neighbors used updating graph, “old” neighbors ever paired “new” neighbors, “old” neighbors. referred “incremental search” NND paper. Also, tolerance \\(\\delta\\) used determine early stopping criterion. total number items graph \\(kN\\) \\(k\\) number neighbors \\(N\\) number items. iteration, counter incremented every time graph successfully updated. end iteration number updates less \\(\\delta kN\\) iteration stops.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"pynndescent-modifications","dir":"Articles","previous_headings":"","what":"PyNNDescent Modifications","title":"Nearest Neighbor Descent","text":"one minor change PyNNDescent works versus description NND paper, rnndescent also uses, sampling candidates works. local join, need know just neighbors , items consider neighbor, call “reverse neighbors” . always \\(k\\) “forward” neighbors graph, don’t control neighbor , neighbor many (even ) items dataset. Thus, building reverse list can bit challenging need prepared item \\(N\\) neighbors. NND paper, avoided defining sample rate \\(\\rho\\), used sample k-nearest neighbors, reverse neighbor list built sampled items. subsequent -sampling applied reverse neighbor list forward reverse neighbor list contain \\(\\rho k\\) items. Instead sample rate, rnndescent defines max_candidates parameter determines size forward reverse neighbor lists per item. candidates max_candidates value, retained candidates chosen randomly works like random sampling. Finally, instead random initialization, PyNNDescent uses k-nearest neighbors graph random projection forest. entire vignette explaining RP forest works. also option rnndescent.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Nearest Neighbor Descent","text":"’s easy enough run NND dataset. ’s example using iris dataset: contents iris_knn list two elements, \\(N\\) \\(k\\) matrices \\(N\\) number items dataset \\(k\\) number neighbors: idx contains indices neighbors: dist contains distances: Apart k, parameters may want modify: metric distance metric use. default Euclidean distance. several metrics can use. See documentation nnd_knn full list. init initialization method. default \"rand\" initializes neighbors randomly. may wish use \"tree\" uses random projection forest initialize neighbors, similar rpf_build. control tree building, can pass sort parameters rpf_build via init_args parameter. See vignette RP forest details. can also pass neighbor graph directly. format output nnd_knn, .e. list two matrices size \\(N\\) \\(k\\). NND can used refine existing graph generated methods, e.g. RcppAnnoy RcppHNSW. n_iters number iterations NND carry . default choose based \\(N\\), number items dataset. amount work done per iteration decreases quite rapidly, sticking default usually sensible, especially don’t change convergence criterion delta (see ), often causes algorithm stop early anyway. delta controls early stopping must value 0 1. given iteration, number changes neighbor graph less delta * k * N algorithm stops. default 0.001 can interpret roughly neighbor graph needs changed 0.1% avoid early stopping. max_candidates controls size forward reverse neighbor lists. default set whatever smaller, k 60. n_threads controls number threads use. default run single thread. slow part approximate nearest neighbor algorithm distance calculation using multiple threads usually good idea. ret_forest TRUE, set init = \"tree\", random projection forest used initialize neighbor graph returned well. want generate new neighbors based original data want . verbose set TRUE get information progress NND. progress affects progress NND displayed verbose = TRUE. default bar shows textual progress bar. can also set progress = \"dist\" show current value convergence criterion sum distances iteration. can help bit determine iterations different convergence criterion help. Note NND uses random number generation determine order processing candidates, reproducible results set random number seed explicitly. Also, way parallelism implemented means reproducibility possible different settings n_threads even consistent seed, e.g. going n_threads = 0 n_threads = 4 give different results, even set.seed fixed seed beforehand.","code":"iris_knn <- nnd_knn(iris, k = 15) iris_knn$idx[1:2, 1:5] #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1   18   29    5   40 #> [2,]    2   13   46   35   10 iris_knn$dist[1:2, 1:5] #>      [,1]      [,2]      [,3]      [,4]      [,5] #> [1,]    0 0.1000000 0.1414212 0.1414212 0.1414213 #> [2,]    0 0.1414213 0.1414213 0.1414213 0.1732050"},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Nearest Neighbor Descent","text":"reason believe aren’t getting results NND sufficiently accurate, probably best thing increase max_candidates. Reducing delta increasing n_iters usually less effect. Restarting nnd_knn init set output previous run usually also helps, time-efficient way improve matters. (lightly edited) sample output running tells dataset size iris, 7 iterations run. 1 / 7, 2 / 7 logged end iteration. Following sum distances neighbors heap, number updates neighbor graph convergence criterion. num_updates falls tol algorithm stops. case, third iteration updates , algorithm stopped early. case, almost certainly NND found exact nearest neighbors, wouldn’t worried modifying parameters. inclined, output shows little point increasing n_iters reducing delta. really leaves max_candidates option. vignette dealing hubness (can issue) goes bit detail use different functions rnndescent deal sort problem.","code":"iris_knn <- nnd_knn(iris, k = 15, verbose = TRUE, progress = \"dist\") Running nearest neighbor descent for 7 iterations 1 / 7 heap sum = 647.85 num_updates = 3356 tol = 2.25 2 / 7 heap sum = 599.9 num_updates = 216 tol = 2.25 3 / 7 heap sum = 599.9 num_updates = 0 tol = 2.25 Convergence: c = 0 tol = 2.25"},{"path":"https://jlmelville.github.io/rnndescent/articles/nearest-neighbor-descent.html","id":"querying-new-data","dir":"Articles","previous_headings":"","what":"Querying New Data","title":"Nearest Neighbor Descent","text":"can’t. NND can produce k-nearest neighbors graph provided data. doesn’t produce “index” kind can query. value NND local join really makes sense can take advantage fact calculating distance \\(d_{ij}\\) lets update neighbor list \\(\\) \\(j\\) . try apply concepts NND querying new data quickly end method looks lot like greedy graph-based searches. , look graph_knn_query(), although noted can also use random projection forest used initialize neighbor graph init = \"tree\". also probably want augment neighbor graph make amenable searching using prepare_search_graph().","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"brute-force","dir":"Articles","previous_headings":"","what":"Brute Force","title":"Querying Data","text":"dataset small enough, can just use brute force find neighbors. index build, worry approximate results : format brute_nbrs usual k-nearest neighbors graph format, list two matrices, dimension (nrow(iris_odd), k). first matrix, idx contains indices nearest neighbors, second matrix, dist contains distances neighbors (’ll just show first five results per row):","code":"brute_nbrs <- brute_force_knn_query(   query = iris_odd,   reference = iris_even,   k = 15 ) lapply(brute_nbrs, function(m) {   head(m[, 1:5]) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    9   20   14    4   25 #> [2,]   24    2   23   15    1 #> [3,]   19    9    4   20   14 #> [4,]   24    6   15    2   19 #> [5,]    2    7   24   23   15 #> [6,]   14   10    3   16   11 #>  #> $dist #>           [,1]      [,2]      [,3]      [,4]      [,5] #> [1,] 0.1000000 0.1414213 0.1414213 0.1732050 0.2236068 #> [2,] 0.1414213 0.2449490 0.2645753 0.3000001 0.3000002 #> [3,] 0.1414213 0.1732050 0.2236066 0.2449488 0.2449488 #> [4,] 0.2236068 0.3000002 0.3162278 0.3316627 0.4123106 #> [5,] 0.2999998 0.3464101 0.3605550 0.4242641 0.4690414 #> [6,] 0.2828429 0.3316626 0.3464102 0.3605551 0.3605553"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"random-projection-forests","dir":"Articles","previous_headings":"","what":"Random Projection Forests","title":"Querying Data","text":"build random projection forest rpf_build, can query rpf_knn_query: See Random Partition Forests vignette .","code":"rpf_index <- rpf_build(iris_even) rpf_nbrs <- rpf_knn_query(   query = iris_odd,   reference = iris_even,   forest = rpf_index,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"graph-search","dir":"Articles","previous_headings":"","what":"Graph Search","title":"Querying Data","text":"See (Dobson et al. 2023) overview graph search algorithms, can described greedy beam search graph: find nearest neighbors, start candidate graph, find distance candidate query point, update neighbor list query accordingly. candidate made neighbor list query, seems like promising direction go , add candidate’s neighbors list candidates explore. Repeat time run candidates. may want explore neighbors candidate even doesn’t make onto current neighbor list, distance sufficiently small. much tolerance controls much back-tracking hence much exploration amount time spend search. graph_knn_query implements search. least must provide reference_graph search, reference data built reference_graph (can calculate distances), k number neighbors want, course query data: aren’t using metric = \"euclidean\", also provide metric used build reference_graph. default metric always \"euclidean\" function rnndescent ’s provided examples . parameters want tweak real world case merit deeper discussion.","code":"graph_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = rpf_nbrs,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"n_threads","dir":"Articles","previous_headings":"Graph Search","what":"n_threads","title":"Querying Data","text":"n_threads controls many threads use search. aware graph_knn_query designed batch parallelism, thread responsible searching subset query points. means streaming context, queries search likely arrive one time, won’t get speed using multiple threads.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"epsilon","dir":"Articles","previous_headings":"Graph Search","what":"epsilon","title":"Querying Data","text":"epsilon controls much exploration neighbors candidate . default value 0.1. larger value, back-tracking permitted. exact meaning value related large distance considered “close enough” current neighbor list query worth exploring. epsilon = 0.1 means query-candidate distance allowed 10% larger largest distance neighbor list. set epsilon = 0.2, example, query-candidate distance allowed 20% higher largest distance neighbor list . set epsilon = 0 get pure greedy search. ’s hard give general rule value set, ’s highly dependent distribution distances dataset determined distance metric dimensionality data . recommend leaving default, modifying find search unreasonably slow (case make epsilon smaller) unreasonably inaccurate (case make epsilon larger). Yes, helpful know.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"init","dir":"Articles","previous_headings":"Graph Search","what":"init","title":"Querying Data","text":"controls search initialized. don’t provide , k random neighbors per item query generated .","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"neighbor-graph-input","dir":"Articles","previous_headings":"Graph Search > init","what":"Neighbor Graph Input","title":"Querying Data","text":"may provide input . neighbor graph format, .e. list two matrices, idx dist, described . Make sure dist matrix contains distances using metric use search.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"neighbor-indices-only","dir":"Articles","previous_headings":"Graph Search > init","what":"Neighbor Indices Only","title":"Querying Data","text":"fact, dist matrix optional. provide idx matrix, dist matrix calculated . dist matrix already available generated rnndescent reason use , neighbors come : another nearest neighbor package reason don’t distance indices different metric nonetheless believe good guess “real” metric. case might worth experimenting can cheaply binarize input data, .e. convert 0/1 FALSE/TRUE: use hamming metric another binary-specialized metric input data. Even brute force search can fast data. good way get good guess real data. contrived example iris, let’s anyway: brute force search binarized data: pass indices brute force search graph_knn_query, generate Euclidean distances : Whether worth depends whether time taken binarize data followed initial search binary data (doesn’t brute force) gives good enough guess save time “real” search graph_knn_query.","code":"numeric_iris <- iris[, sapply(iris, is.numeric)] logical_iris <- sweep(numeric_iris, 2, colMeans(numeric_iris), \">\") logical_iris_even <- logical_iris[seq_len(nrow(logical_iris)) %% 2 == 0, ] logical_iris_odd <- logical_iris[seq_len(nrow(logical_iris)) %% 2 == 1, ] head(logical_iris_even) #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> [1,]        FALSE       FALSE        FALSE       FALSE #> [2,]        FALSE        TRUE        FALSE       FALSE #> [3,]        FALSE        TRUE        FALSE       FALSE #> [4,]        FALSE        TRUE        FALSE       FALSE #> [5,]        FALSE        TRUE        FALSE       FALSE #> [6,]        FALSE        TRUE        FALSE       FALSE iris_logical_brute_nbrs <- brute_force_knn_query(   query = logical_iris_odd,   reference = logical_iris_even,   k = 15,   metric = \"hamming\" ) graph_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = rpf_nbrs,   init = iris_logical_brute_nbrs$idx,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"forest-initialization","dir":"Articles","previous_headings":"Graph Search > init","what":"Forest initialization","title":"Querying Data","text":"previously built RP Forest data may also use initialize query. can re-use rpf_index . general, RP forest initialization likely better initial guess random, terms speed/accuracy trade-, using large forest may best choice. may want use rpf_filter reduce size forest using initial guess. PyNNDescent Python package rnndescent based , one tree used initializing query results.","code":"forest_init_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = rpf_nbrs,   init = rpf_index,   k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/querying-data.html","id":"preparing-the-search-graph","dir":"Articles","previous_headings":"","what":"Preparing the Search Graph","title":"Querying Data","text":"examples far, used k-nearest neighbors graph reference_graph input graph_knn_query. actually good idea? Probably ! guarantee items original dataset can actually reached via k-nearest neighbors graph. nodes just aren’t popular may neighbor list item. means can never reach via k-nearest neighbors graph, matter thoroughly search . can solve problem reversing edges graph adding graph. can get item item j, can now get item j item . solves one problem adds just like items unpopular, items might popular appear often neighbor list items. large number edges graph can make search slow. therefore need prune edges. prepare_search_graph function take k-nearest neighbor graph add edges make useful search. procedure based process described (Harwood Drummond 2016) consists : Reversing edges graph. “Diversifying” graph “occlusion pruning”. considers triplets points, removes long edges probably redundant. item \\(\\) neighbors \\(p\\) \\(q\\) distances \\(d_{pq} \\lt d_{ip}\\) .e. neighbors closer \\(\\), said \\(p\\) occludes \\(q\\) don’t need edges \\(\\rightarrow p\\) \\(\\rightarrow q\\) – ’s likely \\(q\\) neighbor list \\(p\\) vice versa, ’s unlikely harm getting rid \\(\\rightarrow p\\). occlusion pruning, item still excessive number edges, longest edges removed number edges threshold. control pruning following parameters available: diversify_prob: probability neighbor removed found “occlusion”. take value 0 (diversification) 1 (remove many edges possible). default 1.0. pruning_degree_multiplier: controls many edges remove occlusion pruning relative number neighbors original nearest neighbor graph. default 1.5 means allow many 50% edge original graph. input graph k = 15, item search graph 15 * 1.5 = 22 edges. Let’s see works iris neighbors: returned search graph can contain different number edges per item, neighbor graph format isn’t suitable. Instead get back sparse matrix, specifically dgCMatrix. ’s histogram edges distributed:  items around k = 15 edges just like nearest neighbor graph. maximum number edges 10 edges.","code":"set.seed(42) iris_search_graph <- prepare_search_graph(   data = iris_even,   graph = rpf_nbrs,   diversify_prob = 0.1,   pruning_degree_multiplier = 1.5 ) search_graph_edges <- diff(iris_search_graph@p) hist(search_graph_edges,   main = \"Distribution of search graph edges\", xlab = \"# edges\" ) range(search_graph_edges) #> [1] 10 22 search_nbrs <- graph_knn_query(   query = iris_odd,   reference = iris_even,   reference_graph = iris_search_graph,   init = rpf_index,   k = 15 )"},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"random-partition-forests","dir":"Articles","previous_headings":"","what":"Random Partition Forests","title":"Random Partition Forests","text":"’s basic introduction random partition forests work.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"building-a-space-partitioning-tree","dir":"Articles","previous_headings":"Random Partition Forests","what":"Building a Space-Partitioning Tree","title":"Random Partition Forests","text":"First, consider recipe building space-partitioning tree: Select dimension. Select split point along dimension. Split data two child nodes based split point. Repeat steps 1-3 two groups. number items group less threshold, node now leaf, stop splitting. Variations steps 1 2 determines vast majority differences various tree-based methods.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"building-a-random-partition-tree","dir":"Articles","previous_headings":"Random Partition Forests","what":"Building a Random Partition Tree","title":"Random Partition Forests","text":"random partition tree : Select two points random. Calculate mid-point two points. enough define hyperplane data. exactly algorithm described (Dasgupta Freund 2008), ’s done similar method Annoy. Step 3 involves calculating side hyperplane point assigning data child nodes basis.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"from-trees-to-forests","dir":"Articles","previous_headings":"Random Partition Forests","what":"From Trees to Forests","title":"Random Partition Forests","text":"random partition forest just collection random partition trees. random nature trees, different.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"build-a-forest","dir":"Articles","previous_headings":"","what":"Build a Forest","title":"Random Partition Forests","text":"build forest rnndescent, use rpf_build function. ’ll use iris dataset example, goal finding 15-nearest neighbors item dataset. options disposal: metric: type distance calculation use. default euclidean, lot choose . See help text metric parameter rpf_build()` details. n_trees: number trees build. default choose based size data provided, maximum 32: eventually get diminishing returns number trees forest. leaf_size: number items leaf. splitting procedure stops fewer number items node. default 10 want leaf size scale number neighbors look , increased 15 example. bigger value accurate search , cost lot distance calculations carry . Conversely, make small compared number neighbors, may end items finding k neighbors. max_tree_depth: maximum depth tree. tree reaches depth even current node size exceeds value leaf_size, stop splitting. point splitting tree size leaf rapidly decrease go tree, ideal case decrease factor two level, ideally can process datasets vary many orders magnitude depth tree increases levels. default max_tree_depth 200, trigger limit, answer may increase depth. ’s likely something distribution data prevents splitting well. case, ’s different metric try still relevance data, ’s worth try, possibly best solution abandon tree-based approach (example initialize nearest neighbor descent random neighbors). set verbose = TRUE get warning maximum leaf size larger leaf_size. margin: makes slight modification assignment data sides hyperplane calculated. ’ll discuss . forest returned just R list, can save load saveRDS readRDS without issue. ’s something want inspect definitely don’t modify . ’s mainly useful passing functions, like one talk next.","code":"iris_forest <- rpf_build(iris, leaf_size = 15)"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"finding-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Finding Nearest Neighbors","title":"Random Partition Forests","text":"use find nearest neighbors, query point traverse tree root leaf, calculating side hyperplane encounters. items leaf ends candidates nearest neighbors. query forest just build, use rpf_knn_query function. Apart forest , also need data want query (query) data used build forest (reference), forest doesn’t store information. thus case, looking k-nearest neighbors iris, query reference , don’t . point, must also specify number neighbors want. iris_query returned list two matrices: idx contains row indices k-nearest neighbors, dist contains distances.","code":"iris_query <-   rpf_knn_query(     query = iris,     reference = iris,     forest = iris_forest,     k = 15   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"a-small-optimization-for-the-k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"A Small Optimization for the k-Nearest Neighbors","title":"Random Partition Forests","text":"use querying approach mentioned finding k-nearest neighbors data used building tree. However, data already partitioned want k-nearest neighbor data, ’s efficient way : leaf, k-nearest neighbors point leaf members leaf. usually distance calculations take time looking neighbors, avoid make tree traversals associated hyperplane distance calculations. give result running rpf_build followed rpf_knn_query (apart vagaries random number generator), lot convenient bit faster. access parameters forest building rpf_build, e.g. leaf_size, n_trees, max_tree_depth etc. Additionally, want k-nearest neighbors also want forest future querying, set ret_forest = TRUE, return value now also contain forest forest item list. example build forest (get 15-nearest neighbors) first 50 iris items query remaining 100:","code":"iris_knn <- rpf_knn(iris, k = 15) iris_knn_with_forest <-   rpf_knn(iris[1:50, ], k = 15, ret_forest = TRUE) iris_query <-   rpf_knn_query(     query = iris[51:150, ],     reference = iris[1:50, ],     forest = iris_knn_with_forest$forest,     k = 15   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"margin","dir":"Articles","previous_headings":"","what":"Margin","title":"Random Partition Forests","text":"margin parameter determines calculate side hyperplane item split belongs . usual method (margin = \"explicit\") thing PyNNDescent: way hyperplane defined use vector defined two points \\(\\) \\(b\\) normal vector plane, point midway point plane. calculate margin point \\(x\\) (effectively signed distance plane \\(x\\)) : \\[ \\text{margin}(\\mathbf{x}) = ((\\mathbf{b} - \\mathbf{}) \\cdot (\\mathbf{x} - \\frac{\\mathbf{} + \\mathbf{b}}{2})) \\] Taking dot products vectors finding mid points totally unexceptional using Euclidean metric. monotonic relationship cosine distances Euclidean distance normalization vectors, can define “angular” version calculation works normalized vectors. datasets bit weird un-natural. Imagine dataset binary vectors applying e.g. Hamming metric. mid-point two binary vectors binary vector, make sense think geometric relationship implied dot product. alternative calculating margin via explicit creation hyperplane, instead think distance \\(x\\) \\(\\), \\(d_{xa}\\) compares distance \\(x\\) \\(b\\), \\(d_{xb}\\) significance margin . Remember vector defined \\(\\) \\(b\\) normal vector hyperplane, can think line connecting \\(\\) \\(b\\), hyperplane splitting line two equal halves. Now imagine \\(x\\) somewhere line. \\(x\\) closer \\(\\) \\(b\\) must side hyperplane \\(\\), vice versa. Therefore can calculate margin comparing \\(d_{xa}\\) \\(d_{xb}\\) seeing value smaller. don’t explicitly create hyperplane, call “implicit” margin method can choose generate splits way setting margin = \"implicit\". ’ll use random binary data example. Note .logical call: rnndescent detects binary data format specify metric appropriate binary data (e.g. Hamming), use margin = \"implicit\" specialized function called much faster functions written generic floating point data mind. following give results large datasets likely noticeably slower: implicit margin method faster (makes sense metrics) ever want use explicit method? Well, implicit method faster binary data specialized metrics. downside implicit method determining side hyperplane requires two distance calculations per point, whereas explicit method requires dot product calculation, likely costly single distance calculation. floating point data, explicit method likely twice fast. ’s lot think default setting margin \"auto\", tries right thing: using binary data suitable metric, use implicit method, otherwise use explicit method normalize vectors give “angular” approach metrics put emphasis angle versus magnitude.","code":"binary_data <- matrix(as.logical(rbinom(1000, 1, 0.5)), ncol = 10) bin_knn_imp <-   rpf_knn(binary_data,     k = 15,     metric = \"hamming\",     margin = \"implicit\"   ) bin_knn_exp <-   rpf_knn(binary_data,     k = 15,     metric = \"hamming\",     margin = \"explicit\"   )"},{"path":"https://jlmelville.github.io/rnndescent/articles/random-partition-forests.html","id":"filtering-a-forest","dir":"Articles","previous_headings":"","what":"Filtering a Forest","title":"Random Partition Forests","text":"mentioned beginning vignette, rnndescent ’s expected use random partition forests initialization nearest neighbor descent. case, keeping entire forest querying new data probably unnecessary: can keep “best” trees. PyNNDescent keeps one tree purpose. determining tree “best”, mean tree reproduces k-nearest neighbor graph effectively. can comparing k-nearest neighbor graph produced single tree. rpf_filter function : n_trees number trees keep. Feel free keep like, although extra diversification step ensure trees retained good reproducing k-nearest neighbor graph diverse (perhaps reproduce different parts neighbor graph well?). higher quality k-nearest-neighbor graph , better filtering work although example uses graph forest, might get better results using graph run nearest neighbor descent forest result input.","code":"iris_filtered <-   rpf_filter(     nn = iris_query,     forest = iris_forest,     n_trees = 1   )"},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"find-the-k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Find the k-nearest neighbors","title":"rnndescent","text":"just want k-nearest neighbors data, use rnnd_knn:","code":"iris_knn <- rnnd_knn(data = iris, k = 5)"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"the-neighbor-graph-format","dir":"Articles","previous_headings":"Find the k-nearest neighbors","what":"The Neighbor Graph Format","title":"rnndescent","text":"nearest neighbor graph format returned functions package list two matrices: idx – matrix indices nearest neighbors. usual R, 1-indexed. dist – equivalent distances.","code":"lapply(iris_knn, function(x) {   head(x, 3) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1   18    5   29   28 #> [2,]    2   13   46   35   10 #> [3,]    3   48    4    7   13 #>  #> $dist #>      [,1]      [,2]      [,3]      [,4]      [,5] #> [1,]    0 0.1000000 0.1414212 0.1414212 0.1414213 #> [2,]    0 0.1414213 0.1414213 0.1414213 0.1732050 #> [3,]    0 0.1414213 0.2449490 0.2645751 0.2645753"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"build-an-index","dir":"Articles","previous_headings":"","what":"Build an Index","title":"rnndescent","text":"rnnd_knn returns k-nearest neighbors, return “index” can use query new data. , use rnnd_build: index also list lot components (none intended manual examination), apart neighbor graph can found graph component format return value rnnd_knn: aware large high-dimensional data, returned index can get large, especially set n_trees large value.","code":"iris_index <- rnnd_build(data = iris, k = 5) lapply(iris_index$graph, function(x) {   head(x, 3) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1   18    5   29   28 #> [2,]    2   13   46   35   10 #> [3,]    3   48    4    7   46 #>  #> $dist #>      [,1]      [,2]      [,3]      [,4]      [,5] #> [1,]    0 0.1000000 0.1414212 0.1414212 0.1414213 #> [2,]    0 0.1414213 0.1414213 0.1414213 0.1732050 #> [3,]    0 0.1414213 0.2449490 0.2645751 0.2645753"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"querying-data","dir":"Articles","previous_headings":"","what":"Querying Data","title":"rnndescent","text":"Build index pass prepare = TRUE: way use standalone rnnd_prepare function: don’t prepare index, rnnd_query , want call rnnd_query multiple times, go preparation step time, ’s efficient prepare index separately. Another advantage preparing index soon possible index reduced size, number trees used querying data usually substantially smaller number used building index.","code":"iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_even_index <- rnnd_build(data = iris_even, k = 5, prepare = TRUE) iris_odd_nn <- rnnd_query(   index = iris_even_index,   query = iris_odd,   k = 5 ) lapply(iris_odd_nn, function(x) {   head(x, 3) }) #> $idx #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    9   14   20    4   25 #> [2,]   24    2   23   15    1 #> [3,]   19    9    4   20   14 #>  #> $dist #>           [,1]      [,2]      [,3]      [,4]      [,5] #> [1,] 0.1000000 0.1414213 0.1414213 0.1732050 0.2236068 #> [2,] 0.1414213 0.2449490 0.2645753 0.3000001 0.3000002 #> [3,] 0.1414213 0.1732050 0.2236066 0.2449488 0.2449488 iris_even_index <- rnnd_build(data = iris_even, k = 5) iris_even_index <- rnnd_prepare(iris_even_index) iris_odd_nn <- rnnd_query(   index = iris_even_index,   query = iris_odd,   k = 5 )"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"parallelism","dir":"Articles","previous_headings":"","what":"Parallelism","title":"rnndescent","text":"rnndescent multi-threaded, default single-threaded. Set n_threads set number threads want use:","code":"iris_even_index <- rnnd_build(data = iris_even, k = 5, n_threads = 2)"},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"available-metrics","dir":"Articles","previous_headings":"","what":"Available Metrics","title":"rnndescent","text":"Several different distances available rnndescent beyond typically-supported Euclidean Cosine-based distances nearest neighbor packages. See metrics vignette details.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"supported-data-types","dir":"Articles","previous_headings":"","what":"Supported Data Types","title":"rnndescent","text":"Dense matrices data frames. Sparse matrices, dgCMatrix. distances supported dense matrices. Additionally, dense binary data, supply logical matrix, certain distances intended binary data, specialized functions used speed computation.","code":""},{"path":"https://jlmelville.github.io/rnndescent/articles/rnndescent.html","id":"parameters","dir":"Articles","previous_headings":"","what":"Parameters","title":"rnndescent","text":"several options rnnd_build rnnd_query expose can modified change behavior different stages algorithm. See documentation functions (e.g. ?rnnd_build) Random Partition Forests, Nearest Neighbor Descent Querying Data vignettes details.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"James Melville. Author, maintainer. Vitalie Spinu. Contributor.","code":""},{"path":"https://jlmelville.github.io/rnndescent/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Melville J (2023). rnndescent: Nearest Neighbor Descent Method Approximate Nearest Neighbors. R package version 0.0.16, https://jlmelville.github.io/rnndescent/.","code":"@Manual{,   title = {rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors},   author = {James Melville},   year = {2023},   note = {R package version 0.0.16},   url = {https://jlmelville.github.io/rnndescent/}, }"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"rnndescent","dir":"","previous_headings":"","what":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"R package finding approximate nearest neighbors, translated Python package PyNNDescent written great Leland McInnes. name suggests, uses Nearest Neighbor Descent method (Dong et al., 2011), also makes use Random Partition Trees (Dasgupta Freund, 2008) well things . Tantalizingly close releasable, can now use rnndescent : optimizing initial set nearest neighbors, e.g. generated RcppAnnoy RcppHNSW. using package nearest neighbor search … … including finding nearest neighbors sparse data, packages R ecosystem . much larger number metrics packages.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"See Get Started article basics. vignettes go detail.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"current-status","dir":"","previous_headings":"","what":"Current Status","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"19 Nov 2023 rnnd_build function rnnd_query functions added simplify creating knn/building index querying , respectively main way using package. functions remain need flexibility. functions removed: local scaling standalone distance functions. latter return different package point.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"missing-features","dir":"","previous_headings":"Current Status","what":"Missing Features","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Compared pynndescent, rnndescent currently lacking, decreasing order likelihood implementation: parallel batch queries currently supported. means trying stream queries, querying one item time, get parallelism. index always passed C++ R layers building index querying. useful portability easy serialize index (can use saveRDS like R data example), ’s efficient. Keeping index R-wrapped C++ class downsides fix . index can also get large large (high-dimensional) datasets. distance metrics. large number currently supported though. See Missing Metrics currently available. Custom metrics. just isn’t feasible C++ implementation. issues around index serialization parallel behavior make rnndescent currently unsuitable streaming applications querying one item time. batch queries, querying multiple items , rnndescent fine: example, generating nearest neighbors UMAP (maybe use uwot). Dimensionality reduction personal use case nearest neighbors calculation like get rnndescent onto CRAN useful--something state. result targeting initial release support streaming case. like fix subsequent release.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"CRAN release, must install repo: packages makes use C++ code must compiled. may carry extra steps able build: Windows: install Rtools ensure C:\\Rtools\\bin path. Mac OS X: using custom ~/.R/Makevars may cause linking errors. sort thing potential problem platforms seems bite Mac owners . R Mac OS X FAQ may helpful work can get away . safe side, advise building without custom Makevars. rnndescent uses C++17. shouldn’t big problem R platforms support (sorry affects ).","code":"# install.packages(\"pak\") pak::pkg_install(\"jlmelville/rnndescent\")"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Optimizing initial set approximate nearest neighbors: can also search neighbor graph new query items: Although example shows basic procedure building graph querying new data , raw nearest neighbor graph isn’t efficient general. ’s highly advisable use refined search graph based original data neighbor graph: See help text prepare_search_graph various parameters can change control trade speed search accuracy.","code":"library(rnndescent)  # both hnsw_knn and nnd_knn will remove non-numeric columns from data-frames # for you, but to avoid confusion, these examples will use a matrix irism <- as.matrix(iris[, -5])  # If you just want sensible defaults that will probably work: iris_knn <- rnnd_knn(irism, k = 15)  # If you would like to query new data, then you should build an index iris_index <- rnnd_build(irism) # the nearest neighbor graph is in iris_index$graph  # For more control: # Generate a Random Projection knn (set n_threads for parallel search): iris_rp_nn <- rpf_knn(irism, k = 15)  # nn descent improves results: set verbose = TRUE and progress = \"dist\", to  # track distance sum progress over iterations res <-   nnd_knn(irism,     metric = \"euclidean\",     init = iris_rp_nn,     verbose = TRUE,     progress = \"dist\"   )  # search can be multi-threaded res <-   nnd_knn(     irism,     metric = \"euclidean\",     init = iris_rp_nn,     verbose = TRUE,     n_threads = 4   )  # a (potentially) faster version of the algorithm is available that avoids some  # repeated distance calculations at the cost of using more memory. Currently off # by default. res <-   nnd_knn(     irism,     metric = \"euclidean\",     init = iris_rp_nn,     verbose = TRUE,     n_threads = 4,     low_memory = FALSE   )    # You can optimize results from other methods or packages too:   # install.packages(\"RcppHNSW\") # Use settings that don't get perfect results straight away iris_hnsw_nn <-   RcppHNSW::hnsw_knn(irism,     k = 15,     M = 2,     distance = \"euclidean\"   )  res <-   nnd_knn(     irism,     metric = \"euclidean\",     init = iris_hnsw_nn,     verbose = TRUE,     n_threads = 4,     low_memory = FALSE   ) # 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # The simple way. First build an index: index <- iris_index <- rnnd_build(iris_ref, prepare = TRUE) # Then query it iris_query_nn <- rnnd_query(index, iris_query, k = 10)  # For more control: # First, find the approximate 10-nearest neighbor graph for the references: iris_ref_knn <- nnd_knn(iris_ref, k = 10)  # For each item in iris_query find the 10 nearest neighbors in iris_ref # You need to pass both the reference data and the knn graph. iris_query_nn <-   graph_knn_query(     query = iris_query,     reference = iris_ref,     reference_graph = iris_ref_knn,     k = 10,     metric = \"euclidean\",     verbose = TRUE   ) iris_search_graph <-   prepare_search_graph(iris_ref, iris_ref_knn, verbose = TRUE) iris_query_nn <-   graph_knn_query(     query = iris_query,     reference = iris_ref,     reference_graph = iris_search_graph,     k = 4,     metric = \"euclidean\",     verbose = TRUE   )"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"initialization","dir":"","previous_headings":"","what":"Initialization","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"default nnd_knn initialize random neighbors. Set init = \"tree\" use random partition tree initialization. initializing knn query, also random_knn_query.","code":"library(rnndescent)  irism <- as.matrix(iris[, -5])  # picks indices at random and then carries out nearest neighbor descent res <- nnd_knn(irism,   k = 15,   metric = \"euclidean\",   n_threads = 4 )  # if you want the random indices and their distances: iris_rand_nn <-   random_knn(irism,     k = 15,     metric = \"euclidean\",     n_threads = 4   )    # use RP tree initialization res <- nnd_knn(irism,   k = 15,   metric = \"euclidean\",   n_threads = 4,   init = \"tree\" )"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"brute-force","dir":"","previous_headings":"","what":"Brute Force","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"comparison exact results, also brute_force_knn brute_force_knn_query functions, generate exact nearest neighbors simple process trying every possible pair dataset. Obviously becomes time consuming process dataset grows size, even multithreading (although iris dataset example doesn’t present issues).","code":"iris_exact_nn <-   brute_force_knn(irism,     k = 15,     metric = \"euclidean\",     n_threads = 4   )"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"merging","dir":"","previous_headings":"","what":"Merging","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Also available two functions merging multiple approximate nearest neighbor graphs, result new graph least good best graph provided. merging pairs graphs, use merge_knn: two graphs stored memory, ’s efficient use list-based version, merge_knnl:","code":"set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1)  # Merged results should be an improvement over either individual results iris_mnn <- merge_knn(iris_rnn1, iris_rnn2) sum(iris_mnn$dist) < sum(iris_rnn1$dist) sum(iris_mnn$dist) < sum(iris_rnn2$dist) set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn3 <- nnd_knn(iris, k = 15, n_iters = 1)  iris_mnn <- merge_knnl(list(iris_rnn1, iris_rnn2, iris_rnn3))"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"hubness-diagnostic","dir":"","previous_headings":"","what":"Hubness Diagnostic","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"k_occur function takes idx matrix returns vector k-occurrences item dataset. just number times item found k-nearest neighbor list another item. think idx matrix representing directed graph element idx[, j] matrix edge node node idx[, j], k_occurrences calculated reversing edge counts number edges incident node. Alternatively, nomenclature nearest neighbor descent, ’s size “reverse neighbor” list node. k-occurrence can take value 0 N number items dataset. Values much larger k indicate item potentially hub. presence hubs dataset can reduce accuracy approximate nearest neighbors returned nearest neighbor descent, presence hubs determined distribution k-occurrences quite robust even case approximate nearest neighbor graph low accuracy. Therefore calculating k-occurrences output nearest neighbor descent worth : maximum k-occurrence lot larger k (suggest 10 * k danger sign), accuracy approximate nearest neighbors may compromised. Items low k-occurrences likely affected way. Increasing k max_candidates parameter can help situations. Alternatively, querying data graph_knn_query can help: hubness nearest neighbors, see example Radovanović co-workers, 2010 Bratić co-workers, 2019.","code":"iris_nnd <- nnd_knn(iris, k = 15) kos <- k_occur(iris_nnd$idx) # Purposely don't do a very good job with NND so we have something to improve iris_nnd <- nnd_knn(iris, k = 15, n_iters = 1) iris_search_graph <-   prepare_search_graph(iris, iris_nnd)  # query and reference are the same iris_query_nn <-   graph_knn_query(     query = iris,     reference = iris,     reference_graph = iris_search_graph,     init = iris_nnd,     k = 15   ) # Compare  sum(iris_nnd$dist) sum(iris_query_nn$dist)"},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"supported-metrics","dir":"","previous_headings":"","what":"Supported Metrics","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Euclidean, Manhattan, Cosine, Correlation (1 - Pearson correlation, implemented cosine distance row-centered data) Hamming. Note implemented simple fashion, clever (non-portable) optimizations using AVX/SSE specialized popcount routines used.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World wide web (pp. 577-586). ACM. doi.org/10.1145/1963405.1963487.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"R package whole licensed GPLv3 later. following files licensed differently: inst/include/dqsample.h modification sampling code dqrng AGPLv3 later. inst/include/RcppPerpendicular.h modification code RcppParallel GPLv2 later underlying nearest neighbor descent C++ library, can found inst/include/tdoann, licensed BSD 2-Clause. far know, licenses compatible re-licensing GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"missing-metrics","dir":"","previous_headings":"","what":"Missing Metrics","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"following metrics PyNNDescent supported rnndescent: Circular Kantorovich Haversine Kantorovich Mahalanobis Minkowski Sinkhorn Standardised Euclidean Wasserstein 1d Weighted Minkowski require passing extra information part metric definition, currently supported.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"see-also","dir":"","previous_headings":"","what":"See Also","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"PyNNDescent, Python implementation. nndescent, C++ implementation. NearestNeighborDescent.jl, Julia implementation. NNDescent.cpp, another C++ implementation. nndescent, another C++ implementation, Python bindings.","code":""},{"path":"https://jlmelville.github.io/rnndescent/index.html","id":"old-news","dir":"","previous_headings":"","what":"Old News","title":"Nearest Neighbor Descent Method for Approximate Nearest Neighbors","text":"13 November 2023. added metrics don’t need extra parameters sparse non-sparse data, e.g. braycurtis, dice, jaccard, hellinger etc. See Missing Metrics section end README implemented. breaking changes (mainly around hamming metric, see NEWS.md exact details). 06 November 2023 Sparse data support added. able use e.g. dgCMatrix methods currently supported metrics easily dense matrix. 30 October 2023 last, workable random partition forest implementation added. can used standalone (e.g. rpf_knn, rpf_build, rpt_knn_query) initialization nearest neighbor descent (nnd_knn(init = \"tree\", ...)). forest can serialized saveRDS pay price convenience pass back forth R C++ layer querying. now access underlying C++ class via R like RcppHNSW RcppAnnoy may suitable use cases. 19 October 2023 Inevitably 0.0.11 bug 0.0.10 nearest neighbor descent correctly flagging new/old neighbors reduced performance (actual result). 18 October 2023 long-postponed major internal refactoring means might able make bit progress package. now, cosine correlation metrics migrated preprocessing data (versions still available cosine-preprocess correlation-preprocess respectively). Also, exported distance metrics R functions (e.g. cosine_distance, euclidean_distance). 18 September 2021 \"hamming\" metric now supports integer-valued (just binary) inputs, thanks contribution Vitalie Spinu. older metric code path binary data supported via metric = \"bhamming\". 20 June 2021 big step forward usefulness addition prepare_search_graph function creates prunes undirected search graph neighbor graph use (now re-named) graph_knn_query function. latter now also capable backtracking search performs fairly well. 4 October 2020 Added \"correlation\" metric k_occur function help diagnose potential hubness dataset. 23 November 2019 Added merge_knn merge_knnl combining multiple nn results. 15 November 2019 now possible query reference set data produce approximate knn graph relative references (.e. none queries selected neighbors) via nnd_knn_query (related brute_force random variants). 27 October 2019 rnndescent creeps towards usability. multi-threaded implementation (using RcppParallel) now added.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"Calculate Exact Nearest Neighbors Brute Force","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"","code":"brute_force_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Exact Nearest Neighbors by Brute Force — brute_force_knn","text":"","code":"# Find the 4 nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- brute_force_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- brute_force_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- brute_force_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- brute_force_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 21:20:44 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:44 Calculating brute force k-nearest neighbors with k = 4 #> 21:20:44 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"Query Exact Nearest Neighbors Brute Force","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"","code":"brute_force_knn_query(   query,   reference,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed format orientation reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices reference. dist n k matrix containing nearest neighbor distances items reference.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/brute_force_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query Exact Nearest Neighbors by Brute Force — brute_force_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # For each item in iris_query find the 4 nearest neighbors in iris_ref # If you pass a data frame, non-numeric columns are removed # set verbose = TRUE to get details on the progress being made iris_query_nn <- brute_force_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 21:20:45 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:45 Calculating brute force k-nearest neighbors from reference with k = 4 #> 21:20:45 Finished  # Manhattan (l1) distance iris_query_nn <- brute_force_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"manhattan\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Nearest Neighbors and Distances — graph_knn_query","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"Find Nearest Neighbors Distances","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"","code":"graph_knn_query(   query,   reference,   reference_graph,   k = NULL,   metric = \"euclidean\",   init = NULL,   epsilon = 0.1,   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed format orientation reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. reference_graph Search graph reference data. neighbor graph, output nnd_knn() can used, preferably suitably prepared sparse search graph used, output prepare_search_graph(). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" init Initial query neighbor graph optimize. provided, k random neighbors created. provided, input format must one : list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. input distances omitted, calculated . random projection forest, returned rpf_build() rpf_knn() ret_forest = TRUE. epsilon Controls trade-accuracy search cost, specifying distance tolerance whether explore neighbors candidate points. larger value, neighbors searched. value 0.1 allows query-candidate distances 10% larger current -distant neighbor query point, 0.2 means 20%, . Suggested values 0-0.5, although value highly dependent distribution distances dataset (higher dimensional data choose smaller cutoff). large value epsilon result query search approaching brute force comparison. Use parameter conjunction prepare_search_graph() prevent excessive run time. Default 0.1. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. search forest used initialization via init parameter, metric fetched setting ignored. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices specifying row neighbor reference. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"Hajebi, K., Abbasi-Yadkori, Y., Shahbazi, H., & Zhang, H. (2011, June). Fast approximate nearest-neighbor search k-nearest neighbor graph. Twenty-Second International Joint Conference Artificial Intelligence. Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722). Iwasaki, M., & Miyazaki, D. (2018). Optimization indexing based k-nearest neighbor graph proximity search high-dimensional data. arXiv preprint arXiv:1810.07355.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/graph_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Nearest Neighbors and Distances — graph_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # First, find the approximate 4-nearest neighbor graph for the references: iris_ref_graph <- nnd_knn(iris_ref, k = 4)  # For each item in iris_query find the 4 nearest neighbors in iris_ref. # You need to pass both the reference data and the reference graph. # If you pass a data frame, non-numeric columns are removed. # set verbose = TRUE to get details on the progress being made iris_query_nn <- graph_knn_query(iris_query, iris_ref, iris_ref_graph,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 21:20:46 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:46 Initializing from random neighbors #> 21:20:46 Generating random k-nearest neighbor graph from reference with k = 4 #> 21:20:46 Searching nearest neighbor graph #> 21:20:46 Finished  # A more complete example, converting the initial knn into a search graph # and using a filtered random projection forest to initialize the search # create initial knn and forest iris_ref_graph <- nnd_knn(iris_ref, k = 4, init = \"tree\", ret_forest = TRUE) # keep the best tree in the forest forest <- rpf_filter(iris_ref_graph, n_trees = 1) # expand the knn into a search graph iris_ref_search_graph <- prepare_search_graph(iris_ref, iris_ref_graph) # run the query with the improved graph and initialization iris_query_nn <- graph_knn_query(iris_query, iris_ref, iris_ref_search_graph,   init = forest, k = 4 )"},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":null,"dir":"Reference","previous_headings":"","what":"k-Occurrence — k_occur","title":"k-Occurrence — k_occur","text":"Calculates k-occurrence given nearest neighbor matrix","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"k-Occurrence — k_occur","text":"","code":"k_occur(idx, k = NULL, include_self = TRUE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-Occurrence — k_occur","text":"idx integer matrix containing nearest neighbor indices, integers labeled starting 1. Note integer labels refer rows idx, example nearest neighbor result querying one set objects respect another (instance running graph_knn_query()). may also pass nearest neighbor graph object (e.g. output running nnd_knn()), indices extracted , sparse matrix format returned prepare_search_graph(). k number closest neighbors use. Must 1 number columns idx. default, columns idx used. Ignored idx sparse. include_self logical indicating whether label idx considered valid neighbor found row . default TRUE. can set FALSE labels idx refer row indices idx, case results nnd_knn(). case may want consider trivial case object neighbor . cases leave set TRUE.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"k-Occurrence — k_occur","text":"vector length max(idx), containing number times object idx found nearest neighbor list objects represented row indices idx.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"k-Occurrence — k_occur","text":"k-occurrence object number times occurs among k-nearest neighbors objects dataset. can take values 0 size dataset. larger k-occurrence object, \"popular\" . large values k-occurrence (much larger k) indicates object \"hub\" also implies existence \"anti-hubs\": objects never appear k-nearest neighbors objects. presence hubs can reduce accuracy nearest-neighbor descent approximate nearest neighbor algorithms terms retrieving exact k-nearest neighbors. However appearance hubs can still detected approximate results, calculating k-occurrences output nearest neighbor descent useful diagnostic step.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"k-Occurrence — k_occur","text":"Radovanovic, M., Nanopoulos, ., & Ivanovic, M. (2010). Hubs space: Popular nearest neighbors high-dimensional data. Journal Machine Learning Research, 11, 2487-2531. https://www.jmlr.org/papers/v11/radovanovic10a.html Bratic, B., Houle, M. E., Kurbalija, V., Oria, V., & Radovanovic, M. (2019). Influence Hubness NN-Descent. International Journal Artificial Intelligence Tools, 28(06), 1960002. https://doi.org/10.1142/S0218213019600029","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/k_occur.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"k-Occurrence — k_occur","text":"","code":"iris_nbrs <- brute_force_knn(iris, k = 15) iris_ko <- k_occur(iris_nbrs$idx) # items 42 and 107 are not in 15 nearest neighbors of any other members of # iris # for convenience you can also pass iris_nbrs directly: # iris_ko <- k_occur(iris_nbrs) which(iris_ko == 1) # they are only their own nearest neighbor #> [1]  42 107 max(iris_ko) # most \"popular\" item appears on 29 15-nearest neighbor lists #> [1] 29 which(iris_ko == max(iris_ko)) # it's iris item 64 #> [1] 64 # with k = 15, a maximum k-occurrence = 29 ~= 1.9 * k, which is not a cause # for concern"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge two approximate nearest neighbors graphs — merge_knn","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"Merge two approximate nearest neighbors graphs","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"","code":"merge_knn(   nn_graph1,   nn_graph2,   is_query = FALSE,   n_threads = 0,   verbose = FALSE )"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"nn_graph1 nearest neighbor graph merge. consist list containing: idx n k matrix containing k nearest neighbor indices. dist n k matrix containing k nearest neighbor distances. nn_graph2 Another nearest neighbor graph merge format nn_graph1. number neighbors can differ graphs, merged result number neighbors specified nn_graph1. is_query TRUE graphs treated result knn query, knn building process. : graph bipartite? set TRUE nn_graphs results using e.g. graph_knn_query() random_knn_query(), set FALSE results nnd_knn() random_knn(). difference is_query = FALSE, index p found nn_graph1[, ], .e. p neighbor distance d, assumed neighbor p distance. is_query = TRUE, p indexes two different datasets symmetry hold. sure case applies , safe (potentially inefficient) set is_query = TRUE n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"list containing: idx n k matrix containing merged nearest neighbor indices. dist n k matrix containing merged nearest neighbor distances. size k output graph nn_graph1.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge two approximate nearest neighbors graphs — merge_knn","text":"","code":"set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1)  # Merged results should be an improvement over either individual results iris_mnn <- merge_knn(iris_rnn1, iris_rnn2) sum(iris_mnn$dist) < sum(iris_rnn1$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn2$dist) #> [1] TRUE"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge a list of approximate nearest neighbors graphs — merge_knnl","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"Merge list approximate nearest neighbors graphs","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"","code":"merge_knnl(nn_graphs, is_query = FALSE, n_threads = 0, verbose = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"nn_graphs list nearest neighbor graph merge. item list consist sub-list containing: idx n k matrix containing k nearest neighbor indices. dist n k matrix containing k nearest neighbor distances. number neighbors can differ graphs, merged result number neighbors first graph list. is_query TRUE graphs treated result knn query, knn building process. : graph bipartite? set TRUE nn_graphs results using e.g. graph_knn_query() random_knn_query(), set FALSE results nnd_knn() random_knn(). difference is_query = FALSE, index p found nn_graph1[, ], .e. p neighbor distance d, assumed neighbor p distance. is_query = TRUE, p indexes two different datasets symmetry hold. sure case applies , safe (potentially inefficient) set is_query = TRUE. n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"list containing: idx n k matrix containing merged nearest neighbor indices. dist n k matrix containing merged nearest neighbor distances. size k output graph first item nn_graphs.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/merge_knnl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge a list of approximate nearest neighbors graphs — merge_knnl","text":"","code":"set.seed(1337) # Nearest neighbor descent with 15 neighbors for iris three times, # starting from a different random initialization each time iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1) iris_rnn3 <- nnd_knn(iris, k = 15, n_iters = 1)  # Merged results should be an improvement over individual results iris_mnn <- merge_knnl(list(iris_rnn1, iris_rnn2, iris_rnn3)) sum(iris_mnn$dist) < sum(iris_rnn1$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn2$dist) #> [1] TRUE sum(iris_mnn$dist) < sum(iris_rnn3$dist) #> [1] TRUE  # and slightly faster than running: # iris_mnn <- merge_knn(iris_rnn1, iris_rnn2) # iris_mnn <- merge_knn(iris_mnn, iris_rnn3)"},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"Distances stored column-wise, .e. neighbors first observation nn first column matrix, neighbors second observation second column, .","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"","code":"nn_to_sparse(nn)"},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"nn nearest neighbor graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"data nn sparse distance matrix dgCMatrix format.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"Zero distances dropped. typical (non-bipartite) graph case, observation usually neighbor distance zero. distances retained output, hence number neighbors nn k, k - 1 neighbors stored sparse matrix.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nn_to_sparse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Neighbor Graph to Sparse Distance Matrix — nn_to_sparse","text":"","code":"# find 4 nearest neighbors of first ten iris data i10nn <- brute_force_knn(iris[1:10, ], k = 4) # Nearest neighbors of each item is itself all(i10nn$idx[, 1] == 1:10) # TRUE #> [1] TRUE # Convert to sparse i10nnsp <- nn_to_sparse(i10nn) # 3 neighbors are retained in the sparse format because we drop 0 distances all(diff(i10nnsp@p) == 3) # TRUE #> [1] TRUE"},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Nearest Neighbors and Distances — nnd_knn","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"Uses Nearest Neighbor Descent (Dong co-workers, 2011) optimize approximate nearest neighbor graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"","code":"nnd_knn(   data,   k = NULL,   metric = \"euclidean\",   init = \"rand\",   init_args = NULL,   n_iters = NULL,   max_candidates = NULL,   delta = 0.001,   low_memory = TRUE,   use_alt_metric = TRUE,   n_threads = 0,   verbose = FALSE,   progress = \"bar\",   obs = \"R\",   ret_forest = FALSE )"},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" init Name initialization strategy initial data neighbor graph optimize. One : \"rand\" random initialization (default). \"tree\" use random projection tree method Dasgupta Freund (2008). pre-calculated neighbor graph. list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. input distances omitted, calculated .' k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. init_args list containing arguments pass random partition forest initialization. See rpf_knn() possible arguments. avoid inconsistences tree calculation subsequent nearest neighbor descent optimization, attempt provide metric use_alt_metric option list ignored. n_iters Number iterations nearest neighbor descent carry . default, chosen based number observations data. max_candidates Maximum number candidate neighbors try item iteration. Use relative k emulate \"rho\" sampling parameter nearest neighbor descent paper. default, set k 60, whichever smaller. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1% neighbor graph must updated iteration. low_memory TRUE, use lower memory, computationally expensive approach index construction. set FALSE, see noticeable speed improvement, especially using smaller number threads, worth trying memory spare. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_threads Number threads use. verbose TRUE, log information console. progress Determines type progress information logged verbose = TRUE. Options : \"bar\": simple text progress bar. \"dist\": sum distances approximate knn graph end iteration. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage. ret_forest TRUE init = \"tree\" RP forest used initialize nearest neighbors returned nearest neighbor data. See Value section details. returned forest can used part initializing search new data: see rpf_knn_query() rpf_filter() details.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. forest (init = \"tree\" ret_forest = TRUE ): RP forest used initialize neighbor data.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452. Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. https://doi.org/10.1145/1963405.1963487.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/nnd_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Nearest Neighbors and Distances — nnd_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- nnd_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- nnd_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 21:20:47 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:47 Initializing neighbors using 'rand' method #> 21:20:47 Generating random k-nearest neighbor graph with k = 4 #> 21:20:47 Running nearest neighbor descent for 7 iterations #> 21:20:47 Finished  # Nearest neighbor descent uses random initialization, but you can pass any # approximation using the init argument (as long as the metrics used to # calculate the initialization are compatible with the metric options used # by nnd_knn). iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\") iris_nn <- nnd_knn(iris, init = iris_nn, metric = \"euclidean\", verbose = TRUE) #> 21:20:47 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:47 Applying metric correction to initial distances from 'euclidean' to 'sqeuclidean' #> 21:20:47 Running nearest neighbor descent for 7 iterations #> 21:20:47 Finished  # Number of iterations controls how much optimization is attempted. A smaller # value will run faster but give poorer results iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 2)  # You can also control the amount of work done within an iteration by # setting max_candidates iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", max_candidates = 50)  # Optimization may also stop early if not much progress is being made. This # convergence criterion can be controlled via delta. A larger value will # stop progress earlier. The verbose flag will provide some information if # convergence is occurring before all iterations are carried out. set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 5, delta = 0.5)  # To ensure that descent only stops if no improvements are made, set delta = 0 set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", n_iters = 5, delta = 0)  # A faster version of the algorithm is available that avoids repeated # distance calculations at the cost of using more RAM. Set low_memory to # FALSE to try it. set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, metric = \"euclidean\", low_memory = FALSE)  # Using init = \"tree\" is usually more efficient than random initialization. # arguments to the tree initialization method can be passed via the init_args # list set.seed(1337) iris_nn <- nnd_knn(iris, k = 4, init = \"tree\", init_args = list(n_trees = 5))"},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Nearest Neighbor Graph Refinement — prepare_search_graph","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"Create graph using existing nearest neighbor data balance search speed accuracy using occlusion pruning truncation strategies Harwood Drummond (2016).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"","code":"prepare_search_graph(   data,   graph,   metric = \"euclidean\",   diversify_prob = 1,   pruning_degree_multiplier = 1.5,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"data Matrix n items, observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). graph neighbor graph data, list containing: idx n k matrix containing nearest neighbor indices data data. dist n k matrix containing nearest neighbor distances. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" diversify_prob degree diversification search graph removing unnecessary edges occlusion pruning. take value 0 (diversification) 1 (remove many edges possible) treated probability neighbor removed found \"occlusion\". item p q, two members neighbor list item , closer , nearer neighbor p said \"occlude\" q. likely q neighbor list p need retain neighbor list . may also set NULL skip occlusion pruning. Note occlusion pruning carried twice, forward neighbors, reverse neighbors. pruning_degree_multiplier strongly truncate final neighbor list item. neighbor list item truncated retain closest d neighbors, d = k * pruning_degree_multiplier, k original number neighbors per item graph. Roughly, values larger 1 keep nearest neighbors item, plus given fraction reverse neighbors (exist). example, setting 1.5 keep forward neighbors half many reverse neighbors, although exactly neighbors retained also dependent occlusion pruning occurs. Set NULL skip step. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"search graph data based graph, represented sparse matrix, suitable use graph_knn_query().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"approximate nearest neighbor graph useful querying via graph_knn_query(), especially query data initialized randomly: items data set may nearest neighbor list item can therefore never returned neighbor, matter close query. Even appear least one neighbor list may reachable expanding arbitrary starting list neighbor graph contains disconnected components. Converting directed graph represented neighbor graph undirected graph adding edge item j edge exists j (.e. creating mutual neighbor graph) solves problems , can result inefficient searches. Although -degree item restricted number neighbors -degree restrictions: given item \"popular\" large number neighbors lists. Therefore mutualizing neighbor graph can result items large number neighbors search. usually similar neighborhoods nothing gained searching . balance accuracy search time, following procedure carried : graph \"diversified\" occlusion pruning. reverse graph formed reversing direction edges pruned graph. reverse graph diversified occlusion pruning. pruned forward pruned reverse graph merged. outdegree node merged graph truncated. truncated merged graph returned prepared search graph. Explicit zero distances graph converted small positive number avoid dropped sparse representation. one exception \"self\" distance, .e. edge graph links node (diagonal sparse distance matrix). trivial edges useful search purposes always dropped.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/prepare_search_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nearest Neighbor Graph Refinement — prepare_search_graph","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # First, find the approximate 4-nearest neighbor graph for the references: ref_ann_graph <- nnd_knn(iris_ref, k = 4)  # Create a graph for querying with ref_search_graph <- prepare_search_graph(iris_ref, ref_ann_graph)  # Using the search graph rather than the ref_ann_graph directly may give # more accurate or faster results iris_query_nn <- graph_knn_query(   query = iris_query, reference = iris_ref,   reference_graph = ref_search_graph, k = 4, metric = \"euclidean\",   verbose = TRUE ) #> 21:20:47 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:47 Initializing from random neighbors #> 21:20:47 Generating random k-nearest neighbor graph from reference with k = 4 #> 21:20:47 Searching nearest neighbor graph #> 21:20:47 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly select nearest neighbors. — random_knn","title":"Randomly select nearest neighbors. — random_knn","text":"Randomly select nearest neighbors.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly select nearest neighbors. — random_knn","text":"","code":"random_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   order_by_distance = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly select nearest neighbors. — random_knn","text":"data Matrix n items generate random neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. order_by_distance TRUE (default), results item returned increasing distance. need results sorted, e.g. going pass results initialization another routine like nnd_knn(), set FALSE save small amount computational time. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly select nearest neighbors. — random_knn","text":"random neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly select nearest neighbors. — random_knn","text":"","code":"# Find 4 random neighbors and calculate their Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\")  # Manhattan (l1) distance iris_nn <- random_knn(iris, k = 4, metric = \"manhattan\")  # Multi-threading: you can choose the number of threads to use: in real # usage, you will want to set n_threads to at least 2 iris_nn <- random_knn(iris, k = 4, metric = \"manhattan\", n_threads = 1)  # Use verbose flag to see information about progress iris_nn <- random_knn(iris, k = 4, metric = \"euclidean\", verbose = TRUE) #> 21:20:47 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:47 Generating random k-nearest neighbor graph with k = 4 #> 21:20:47 Finished  # These results can be improved by nearest neighbors descent. You don't need # to specify k here because this is worked out from the initial input iris_nn <- nnd_knn(iris, init = iris_nn, metric = \"euclidean\", verbose = TRUE) #> 21:20:47 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:47 Applying metric correction to initial distances from 'euclidean' to 'sqeuclidean' #> 21:20:47 Running nearest neighbor descent for 7 iterations #> 21:20:47 Finished"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Nearest Neighbors Query by Random Selection — random_knn_query","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"Nearest Neighbors Query Random Selection","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"","code":"random_knn_query(   query,   reference,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   order_by_distance = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries randomly selected data. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed orientation format reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. k Number nearest neighbors return. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. order_by_distance TRUE (default), results item returned increasing distance. need results sorted, e.g. going pass results initialization another routine like graph_knn_query(), set FALSE save small amount computational time. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input query reference orientation stores observation column (orientation must consistent). default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/random_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nearest Neighbors Query by Random Selection — random_knn_query","text":"","code":"# 100 reference iris items iris_ref <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ]  # 50 query items iris_query <- iris[iris$Species == \"versicolor\", ]  # For each item in iris_query find 4 random neighbors in iris_ref # If you pass a data frame, non-numeric columns are removed # set verbose = TRUE to get details on the progress being made iris_query_random_nbrs <- random_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"euclidean\", verbose = TRUE ) #> 21:20:47 Using alt metric 'sqeuclidean' for 'euclidean' #> 21:20:47 Generating random k-nearest neighbor graph from reference with k = 4 #> 21:20:47 Finished  # Manhattan (l1) distance iris_query_random_nbrs <- random_knn_query(iris_query,   reference = iris_ref,   k = 4, metric = \"manhattan\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","title":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","text":"function builds approximate nearest neighbors graph convenient defaults. also optionally prepare index querying new data, later use rnnd_query(). control process, please see functions package.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","text":"","code":"rnnd_build(   data,   k = 30,   metric = \"euclidean\",   use_alt_metric = TRUE,   init = \"tree\",   n_trees = NULL,   leaf_size = NULL,   max_tree_depth = 200,   margin = \"auto\",   n_iters = NULL,   delta = 0.001,   max_candidates = NULL,   low_memory = TRUE,   prepare = FALSE,   n_search_trees = 1,   pruning_degree_multiplier = 1.5,   diversify_prob = 1,   n_threads = 0,   verbose = FALSE,   progress = \"bar\",   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. init Name initialization strategy initial data neighbor graph optimize. One : \"rand\" random initialization (default). \"tree\" use random projection tree method Dasgupta Freund (2008). pre-calculated neighbor graph. list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. input distances omitted, calculated .' k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. used init = \"tree\". leaf_size maximum number items can appear leaf. value chosen match expected number neighbors want retrieve running queries (e.g. want find 50 nearest neighbors set leaf_size = 50) set value smaller 10. used init = \"tree\". max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. used init = \"tree\". margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. used init = \"tree\". n_iters Number iterations nearest neighbor descent carry . default, chosen based number observations data. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1% neighbor graph must updated iteration. max_candidates Maximum number candidate neighbors try item iteration. Use relative k emulate \"rho\" sampling parameter nearest neighbor descent paper. default, set k 60, whichever smaller. low_memory TRUE, use lower memory, computationally expensive approach index construction. set FALSE, see noticeable speed improvement, especially using smaller number threads, worth trying memory spare. prepare TRUE, prepare index querying. default FALSE enough work calculate k-nearest neighbor graph input data. prepare index querying later, can always call rnnd_prepare() subsequently. parameter provided convenience goal immediately query index building. n_search_trees, number trees keep search forest part index preparation. default 1. Ignored prepare = FALSE. pruning_degree_multiplier strongly truncate final neighbor list item. neighbor list item truncated retain closest d neighbors, d = k * pruning_degree_multiplier, k original number neighbors per item graph. Roughly, values larger 1 keep nearest neighbors item, plus given fraction reverse neighbors (exist). example, setting 1.5 keep forward neighbors half many reverse neighbors, although exactly neighbors retained also dependent occlusion pruning occurs. Set NULL skip step. diversify_prob degree diversification search graph removing unnecessary edges occlusion pruning. take value 0 (diversification) 1 (remove many edges possible) treated probability neighbor removed found \"occlusion\". item p q, two members neighbor list item , closer , nearer neighbor p said \"occlude\" q. likely q neighbor list p need retain neighbor list . may also set NULL skip occlusion pruning. Note occlusion pruning carried twice, forward neighbors, reverse neighbors. n_threads Number threads use. verbose TRUE, log information console. progress Determines type progress information logged nearest neighbor descent stage verbose = TRUE. Options : \"bar\": simple text progress bar. \"dist\": sum distances approximate knn graph end iteration. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","text":"approximate nearest neighbor index, list containing: graph k-nearest neighbor graph, list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. list items intended internal use functions rnnd_prepare() rnnd_query().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","text":"process k-nearest neighbor graph construction using Random Projection Forests (Dasgupta Freund, 2008) initialization Nearest Neighbor Descent (Dong co-workers, 2011) refinement. Index preparation, followed, uses graph diversification method Harwood Drummond (2016).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452. Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. https://doi.org/10.1145/1963405.1963487. Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_build.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Approximate Nearest Neighbors Index and KNN Graph — rnnd_build","text":"","code":"iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ]  # Find 4 (approximate) nearest neighbors using Euclidean distance iris_knn <- rnnd_build(iris, k = 4)  # Can also prepare the index for later querying in the same step iris_even_index <- rnnd_build(iris_even, k = 4, prepare = TRUE) iris_odd_nbrs <- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)  # You can prepare the index later using prepare = TRUE just saves a step iris_even_index <- rnnd_build(iris_even, k = 4) iris_even_index <- rnnd_prepare(iris_even_index) iris_odd_nbrs <- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Approximate Nearest Neighbors Graph — rnnd_knn","title":"Build Approximate Nearest Neighbors Graph — rnnd_knn","text":"function builds approximate nearest neighbors graph convenient defaults. return index later querying.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Approximate Nearest Neighbors Graph — rnnd_knn","text":"","code":"rnnd_knn(   data,   k = 30,   metric = \"euclidean\",   use_alt_metric = TRUE,   init = \"tree\",   n_trees = NULL,   leaf_size = NULL,   max_tree_depth = 200,   margin = \"auto\",   n_iters = NULL,   delta = 0.001,   max_candidates = NULL,   low_memory = TRUE,   n_threads = 0,   verbose = FALSE,   progress = \"bar\",   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Approximate Nearest Neighbors Graph — rnnd_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. init Name initialization strategy initial data neighbor graph optimize. One : \"rand\" random initialization (default). \"tree\" use random projection tree method Dasgupta Freund (2008). pre-calculated neighbor graph. list containing: idx n k matrix containing nearest neighbor indices. dist (optional) n k matrix containing nearest neighbor distances. input distances omitted, calculated .' k init specified arguments function, number neighbors provided init equal k : k smaller, k closest values init retained. k larger, random neighbors chosen fill init size k. Note checking random neighbors duplicates already init effectively fewer k neighbors may chosen observations circumstances. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. used init = \"tree\". leaf_size maximum number items can appear leaf. value chosen match expected number neighbors want retrieve running queries (e.g. want find 50 nearest neighbors set leaf_size = 50) set value smaller 10. used init = \"tree\". max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. used init = \"tree\". margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. used init = \"tree\". n_iters Number iterations nearest neighbor descent carry . default, chosen based number observations data. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1% neighbor graph must updated iteration. max_candidates Maximum number candidate neighbors try item iteration. Use relative k emulate \"rho\" sampling parameter nearest neighbor descent paper. default, set k 60, whichever smaller. low_memory TRUE, use lower memory, computationally expensive approach index construction. set FALSE, see noticeable speed improvement, especially using smaller number threads, worth trying memory spare. n_threads Number threads use. verbose TRUE, log information console. progress Determines type progress information logged nearest neighbor descent stage verbose = TRUE. Options : \"bar\": simple text progress bar. \"dist\": sum distances approximate knn graph end iteration. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Approximate Nearest Neighbors Graph — rnnd_knn","text":"approximate nearest neighbor index, list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build Approximate Nearest Neighbors Graph — rnnd_knn","text":"process k-nearest neighbor graph construction using Random Projection Forests (Dasgupta Freund, 2008) initialization Nearest Neighbor Descent (Dong co-workers, 2011) refinement. sure want query new data compared rnnd_build() function advantage storing index, can large.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Build Approximate Nearest Neighbors Graph — rnnd_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452. Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. https://doi.org/10.1145/1963405.1963487.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Approximate Nearest Neighbors Graph — rnnd_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance iris_knn <- rnnd_knn(iris, k = 4)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_prepare.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","title":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","text":"Takes nearest neighbor index produced rnnd_build uses graph diversification method Harwood Drummond (2016) prepare nearest neighbor index use querying rnnd_query().","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_prepare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","text":"","code":"rnnd_prepare(index, n_threads = 0, verbose = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_prepare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","text":"index approximate nearest neighbor index produced rnnd_build(). n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_prepare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","text":"approximate nearest neighbor index, prepared querying.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_prepare.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","text":"ran rnnd_build() prepare = TRUE need run function calling rnnd_query(). run rnnd_query() index prepared, prepared query run. Calling rnnd_prepare already prepared index nothing.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_prepare.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","text":"Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_prepare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare Approximate Nearest Neighbors Index for Querying — rnnd_prepare","text":"","code":"iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ]  # Build index for knn only iris_even_index <- rnnd_build(iris_even, k = 4)  # Prepare it for querying iris_even_index <- rnnd_prepare(iris_even_index)  # Query new data iris_odd_nbrs <- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query Approximate Nearest Neighbors Index — rnnd_query","title":"Query Approximate Nearest Neighbors Index — rnnd_query","text":"Takes nearest neighbor index produced rnnd_build() uses find nearest neighbors query set observations. index prepared searching (.e. used generate k-nearest neighbor graph), prepared querying begins, using graph diversification method Harwood Drummond (2016). can time-consuming process, plan query index multiple times, recommended run rnnd_prepare() index first (ensure pass prepare = TRUE rnnd_build()).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query Approximate Nearest Neighbors Index — rnnd_query","text":"","code":"rnnd_query(   index,   query,   k = 30,   epsilon = 0.1,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query Approximate Nearest Neighbors Index — rnnd_query","text":"index nearest neighbor index produced rnnd_build(). query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). Sparse non-sparse data mixed, data used build index sparse, query data must also sparse. vice versa. k Number nearest neighbors return. epsilon Controls trade-accuracy search cost, specifying distance tolerance whether explore neighbors candidate points. larger value, neighbors searched. value 0.1 allows query-candidate distances 10% larger current -distant neighbor query point, 0.2 means 20%, . Suggested values 0-0.5, although value highly dependent distribution distances dataset (higher dimensional data choose smaller cutoff). large value epsilon result query search approaching brute force comparison. Use parameter conjunction rnnd_prepare() prevent excessive run time. Default 0.1. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query Approximate Nearest Neighbors Index — rnnd_query","text":"approximate nearest neighbor index, list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Query Approximate Nearest Neighbors Index — rnnd_query","text":"Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnnd_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query Approximate Nearest Neighbors Index — rnnd_query","text":"","code":"iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ]  iris_even_index <- rnnd_build(iris_even, k = 4, prepare = TRUE) iris_odd_nbrs <- rnnd_query(index = iris_even_index, query = iris_odd, k = 4)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":null,"dir":"Reference","previous_headings":"","what":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"Nearest Neighbor Descent method (Dong co-workers, doi:10.1145/1963405.1963487 ) finding approximate nearest neighbors. based heavily Python implementation, \"PyNNDescent\" (https://github.com/lmcinnes/pynndescent).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"rnndescent package provides functions create approximate nearest neighbors using Nearest Neighbor Descent Random Partition Tree methods. comparison packages, offers metrics can used sparse matrices. querying new data, uses graph diversification methods improve search performance. package also provides functions diagnose hubness nearest neighbor results. General resources: Website rnndescent package: https://github.com/jlmelville/rnndescent Website PyNNDescent package: https://github.com/lmcinnes/pynndescent Resources specific topics: Create exact nearest neighbors: brute_force_knn(), brute_force_knn_query() Create approximate nearest neighbors: rpf_knn(), nnd_knn() Querying new data: prepare_search_graph(), graph_knn_query() Diagnostics hubness: k_occur()","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452. Radovanovic, M., Nanopoulos, ., & Ivanovic, M. (2010). Hubs space: Popular nearest neighbors high-dimensional data. Journal Machine Learning Research, 11, 2487-2531. https://www.jmlr.org/papers/v11/radovanovic10a.html Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. https://doi.org/10.1145/1963405.1963487. Harwood, B., & Drummond, T. (2016). Fanng: Fast approximate nearest neighbour graphs. Proceedings IEEE Conference Computer Vision Pattern Recognition (pp. 5713-5722).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rnndescent-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"rnndescent: Nearest Neighbor Descent Method for Approximate Nearest Neighbors — rnndescent-package","text":"Maintainer: James Melville jlmelville@gmail.com contributors: Vitalie Spinu [contributor]","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Random Projection Forest — rpf_build","title":"Create a Random Projection Forest — rpf_build","text":"Build \"forest\" Random Projection Trees (Dasgupta Freund, 2008), can later searched find approximate nearest neighbors.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Random Projection Forest — rpf_build","text":"","code":"rpf_build(   data,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_trees = NULL,   leaf_size = 10,   max_tree_depth = 200,   margin = \"auto\",   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Random Projection Forest — rpf_build","text":"data Matrix n items generate index , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" Note margin = \"explicit\", metric used determine whether \"angular\" \"Euclidean\" distance used measure distance split points tree. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"). Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. applies implicit margin method used. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. leaf_size maximum number items can appear leaf. value chosen match expected number neighbors want retrieve running queries (e.g. want find 50 nearest neighbors set leaf_size = 50) set value smaller 10. max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Random Projection Forest — rpf_build","text":"forest random projection trees list. tree forest list, intended examined manipulated user. normal R data type, can safely serialized deserialized base::saveRDS() base::readRDS(). use querying pass forest parameter rpf_knn_query(). forest store data passed build tree, going search forest, also need store data used build provide search.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a Random Projection Forest — rpf_build","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_build.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Random Projection Forest — rpf_build","text":"","code":"# Build a forest of 10 trees from the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_odd_forest <- rpf_build(iris_odd, n_trees = 10)  iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_even_nn <- rpf_knn_query(   query = iris_even, reference = iris_odd,   forest = iris_odd_forest, k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter a Random Projection Forest — rpf_filter","title":"Filter a Random Projection Forest — rpf_filter","text":"Reduce size random projection forest, scoring tree k-nearest neighbors graph. top N trees retained allows faster querying. Rather rely RP Forest solely approximate nearest neighbor querying, probably cost-effective use small number trees initialize search space use input search graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter a Random Projection Forest — rpf_filter","text":"","code":"rpf_filter(nn, forest = NULL, n_trees = 1, n_threads = 0, verbose = FALSE)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter a Random Projection Forest — rpf_filter","text":"nn Nearest neighbor data dense list format. derived data used build forest. forest random partition forest, e.g. created rpf_build(), representing partitions underlying data reflected nn. convenient, parameter ignored nn list contains forest entry, e.g. running rpf_knn() nnd_knn() ret_forest = TRUE, forest value extracted nn. n_trees number trees retain. default best-scoring tree retained. n_threads Number threads use. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter a Random Projection Forest — rpf_filter","text":"forest best scoring n_trees trees.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Filter a Random Projection Forest — rpf_filter","text":"Trees scored based well leaf reflects neighbors specified nearest neighbor data. best use accurate nearest neighbor data can need come directly searching forest: example, nearest neighbor data running nnd_knn() optimize neighbor data output RP Forest good choice.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter a Random Projection Forest — rpf_filter","text":"","code":"# Build a knn with a forest of 10 trees using the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] # also return the forest with the knn rfknn <- rpf_knn(iris_odd, k = 15, n_trees = 10, ret_forest = TRUE)  # keep the best 2 trees: iris_odd_filtered_forest <- rpf_filter(rfknn)  # get some new data to search iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ]  # search with the filtered forest iris_even_nn <- rpf_knn_query(   query = iris_even, reference = iris_odd,   forest = iris_odd_filtered_forest, k = 15 )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"Find approximate nearest neighbors using \"forest\" Random Projection Trees (Dasgupta Freund, 2008).","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"","code":"rpf_knn(   data,   k,   metric = \"euclidean\",   use_alt_metric = TRUE,   n_trees = NULL,   leaf_size = NULL,   max_tree_depth = 200,   include_self = TRUE,   ret_forest = FALSE,   margin = \"auto\",   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"data Matrix n items generate neighbors , observations rows features columns. Optionally, input can passed observations columns, setting obs = \"C\", efficient. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). k Number nearest neighbors return. Optional init specified. metric Type distance calculation use. One : \"braycurtis\" \"canberra\" \"chebyshev\" \"correlation\" (1 minus Pearson correlation) \"cosine\" \"dice\" \"euclidean\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"sqeuclidean\" (squared Euclidean) \"manhattan\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" (1 minus Spearman rank correlation) \"symmetrickl\" (symmetric Kullback-Leibler divergence) \"tsss\" (Triangle Area Similarity-Sector Area Similarity TS-SS metric) \"yule\" non-sparse data, following variants available preprocessing: trades memory potential speed distance calculation. minor numerical differences expected compared non-preprocessed versions: \"cosine-preprocess\": cosine preprocessing. \"correlation-preprocess\": correlation preprocessing. non-sparse binary data passed logical matrix, following metrics specialized variants substantially faster non-binary variants (cases logical data treated dense numeric vector 0s 1s): \"dice\" \"hamming\" \"jaccard\" \"kulsinski\" \"matching\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"yule\" Note margin = \"explicit\", metric used determine whether \"angular\" \"Euclidean\" distance used measure distance split points tree. use_alt_metric TRUE, use faster metrics maintain ordering distances internally (e.g. squared Euclidean distances using metric = \"euclidean\"), apply correction end. Probably reason set FALSE suspect sort numeric issue occurring data alternative code path. n_trees number trees use RP forest. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations data. leaf_size maximum number items can appear leaf. default NULL means number leaves chosen based number requested neighbors k. max_tree_depth maximum depth tree build (default = 200). maximum tree depth exceeded leaf size tree may exceed leaf_size can result large number neighbor distances calculated. verbose = TRUE message logged indicate leaf size large. However, increasing max_tree_depth may help: may something unusual distribution data set chose metric makes tree-based initialization inappropriate. include_self TRUE (default) item considered neighbor . Hence first nearest neighbor results item . convention many nearest neighbor methods software adopt, want use resulting knn graph function downstream applications compare methods, probably keep set TRUE. However, planning using result initialization another nearest neighbor method (e.g. nnd_knn()), set FALSE. ret_forest TRUE also return search forest can used future querying (via rpf_knn_query()) filtering (via rpf_filter()). default FALSE. Setting TRUE change output list nested (see Value section ). margin character string specifying method used  assign points one side hyperplane . Possible values : \"explicit\" categorizes distance metrics either Euclidean Angular (Euclidean normalization), explicitly calculates hyperplane offset, calculates margin based dot product hyperplane. \"implicit\" calculates distance point points defining normal vector. margin calculated comparing two distances: point assigned side hyperplane normal vector point closest distance belongs . \"auto\" (default) picks margin method depending whether binary-specific metric \"bhammming\" chosen, case \"implicit\" used, \"explicit\" otherwise: binary-specific metrics involve storing data way efficient \"explicit\" method binary-specific metric usually lot faster generic equivalent cost two distance calculations margin method still faster. n_threads Number threads use. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. forest (ret_forest = TRUE) RP forest generated neighbor graph, can used query new data. k neighbors per observation guaranteed found. Missing data represented index 0 distance NA.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Nearest Neighbors and Distances Using A Random Projection Forest — rpf_knn","text":"","code":"# Find 4 (approximate) nearest neighbors using Euclidean distance # If you pass a data frame, non-numeric columns are removed iris_nn <- rpf_knn(iris, k = 4, metric = \"euclidean\", leaf_size = 3)  # If you want to initialize another method (e.g. nearest neighbor descent) # with the result of the RP forest, then it's more efficient to skip # evaluating whether an item is a neighbor of itself by setting # `include_self = FALSE`: iris_rp <- rpf_knn(iris, k = 4, n_trees = 3, include_self = FALSE) # Use it with e.g. `nnd_knn` -- this should be better than a random start iris_nnd <- nnd_knn(iris, k = 4, init = iris_rp) # but note you can also run nnd_knn(iris, k = 4, init = \"tree\") to initialize # from an RP forest directly  # for future querying you may want to also return the RP forest: iris_rpf <- rpf_knn(iris,   k = 4, n_trees = 3, include_self = FALSE,   ret_forest = TRUE ) # forest and nn data can be used to create a smaller forest for querying # filtered_forest <- rpf_filter(iris_rpf)"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Search a Random Projection Forest — rpf_knn_query","title":"Search a Random Projection Forest — rpf_knn_query","text":"Run queries \"forest\" Random Projection Trees (Dasgupta Freund, 2008), return nearest neighbors reference data used build forest.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search a Random Projection Forest — rpf_knn_query","text":"","code":"rpf_knn_query(   query,   reference,   forest,   k,   cache = TRUE,   n_threads = 0,   verbose = FALSE,   obs = \"R\" )"},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search a Random Projection Forest — rpf_knn_query","text":"query Matrix n query items, observations rows features columns. Optionally, data may passed observations columns, setting obs = \"C\", efficient. reference data must passed orientation query. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. Dataframes converted numerical matrix format internally, data columns logical intended used specialized binary metrics, convert logical matrix first (otherwise get slower dense numerical version). reference Matrix m reference items, observations rows features columns. nearest neighbors queries calculated data data used build forest. Optionally, data may passed observations columns, setting obs = \"C\", efficient. query data must passed format orientation reference. Possible formats base::data.frame(), base::matrix() Matrix::sparseMatrix(). Sparse matrices dgCMatrix format. forest random partition forest, created rpf_build(), representing partitions data reference. k Number nearest neighbors return. unlikely get good results choose value substantially larger value leaf_size used build forest. cache TRUE (default) candidate indices found leaves forest cached avoid recalculating distance repeatedly. incurs extra memory cost scales n_threads. Set FALSE disable distance caching. n_threads Number threads use. Note parallelism search done observations query trees forest. Thus single observation see speed-increasing n_threads. verbose TRUE, log information console. obs set \"C\" indicate input data orientation stores observation column. default \"R\" means observations stored row. Storing data row usually convenient, internally data converted column storage. Passing already column-oriented save memory (small amount ) CPU usage.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search a Random Projection Forest — rpf_knn_query","text":"approximate nearest neighbor graph list containing: idx n k matrix containing nearest neighbor indices. dist n k matrix containing nearest neighbor distances. k neighbors per observation guaranteed found. Missing data represented index 0 distance NA.","code":""},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Search a Random Projection Forest — rpf_knn_query","text":"Dasgupta, S., & Freund, Y. (2008, May). Random projection trees low dimensional manifolds. Proceedings fortieth annual ACM symposium Theory computing (pp. 537-546). https://doi.org/10.1145/1374376.1374452.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/reference/rpf_knn_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search a Random Projection Forest — rpf_knn_query","text":"","code":"# Build a forest of 10 trees from the odd rows iris_odd <- iris[seq_len(nrow(iris)) %% 2 == 1, ] iris_odd_forest <- rpf_build(iris_odd, n_trees = 10)  iris_even <- iris[seq_len(nrow(iris)) %% 2 == 0, ] iris_even_nn <- rpf_knn_query(   query = iris_even, reference = iris_odd,   forest = iris_odd_forest, k = 15 )"},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-16","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.16","text":"New function: rnnd_knn. Behaves lot like rnnd_build, returns knn graph index built. index can large size high dimensional large datasets, function useful care knn graph won’t ever want query new data.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-16","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.16","text":"sparse spearmanr distance fixed. tree-building n_threads = 0, progress/interrupt monitoring occurring.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"breaking-changes-0-0-15","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"rnndescent 0.0.15","text":"Standalone distance functions removed. hadn’t expanded match distances available nearest neighbor functions, sparse support added. increase size package’s API even . may show another package. local_scale_nn removed, similar reasons removal standalone distance functions. remains localscale branch github repo. search graph returned prepare_search_graph now transposed. prevents repeatedly transpose inside every call graph_knn_query multiple queries made. need either regenerate saved search graphs transpose Matrix::t(search_graph).","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-15","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.15","text":"New functions: rnnd_build, rnnd_query rnnd_prepare. functions streamline process building k-nearest neighbor graph, preparing search graph querying .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"breaking-changes-0-0-14","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"rnndescent 0.0.14","text":"bhamming metric longer exists specialized metric. Instead, pass logical matrix data, reference query parameter (depending function) specify metric = \"hamming\" automatically get binary-specific version hamming metric. hamming bhamming metrics now normalized respect number features, consistent binary-style metrics (PyNNDescent). need old distances, multiply distance matrix number columns, e.g. something like: metric l2sqr renamed sqeuclidean consistent PyNNDescent.","code":"res <- brute_force_knn(X, metric = \"hamming\") res$dist <- res$dist * ncol(X)"},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-14","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.14","text":"Metrics? got ’em! metric parameter now accepts much larger number metrics. See rdoc full list supported metrics. Currently, metrics PyNNDescent don’t require extra parameters supported. number specialized binary metrics also expanded. New parameter rpf_knn rpf_build: max_tree_depth controls depth tree set 100 internally. default doubled 200 can now user-controlled. verbose = TRUE largest leaf forest exceeds leaf_size parameter, message warning logged indicates maximum tree depth exceeded. Increasing max_tree_depth may answer: ’s likely something unusual distribution distances dataset random initialization might better use time.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-13","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.13","text":"Sparse data now supported. Pass dgCMatrix data, reference query parameters usually use dense matrix data frame. cosine, euclidean, manhattan, hamming correlation available, alternative versions dense case, e.g. cosine-preprocess binary-specific bhamming dense data . new init option graph_knn_query: can now pass RP forest initialize , e.g. rpf_build, setting ret_forest = TRUE nnd_knn rpf_knn. may want cut size forest used initialization rpf_filter first, though (single tree may enough). also use metric data forest, setting metric (use_alt_metric) function ignored.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-13","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.13","text":"knn graph pass prepare_search_graph graph_knn_query contains missing data, longer cause error (still might best idea though).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-12","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.12","text":"New function: rpf_knn. Calculates approximate k-nearest neighbors using random partition forest. New function: rpf_build. Builds random partition forest. New function: rpf_knn_query. Queries random partition forest (built rpf_build find approximate nearest neighbors query points. New function: rpf_filter. Retains best “scoring” trees forest, tree scored based well reproduces given knn. New initialization method nnd_knn: init = \"tree\". Uses RP Forest initialization method. New parameter nnd_knn: ret_forest. Returns search forest used init = \"tree\" can used future searching filtering. New parameter nnd_knn: init_opts. Options can passed RP forest initialization (rpf_knn).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-11","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.11","text":"Progess report nnd_knn n_threads > 0 reporting double actual number iterations. made progress bar way optimistic. bug flagging neighbors 0.0.10 made nearest neighbor descent inefficient.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-10","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.10","text":"change metric: \"cosine\" \"correlation\" renamed \"cosine-preprocess\" \"correlation-preprocess\" respectively. reflects preprocessing data front make subsequent distance calculations faster. endeavored avoid unnecessary allocations copying preprocessing, still chance memory usage. cosine correlation metrics still available option, now use implementation doesn’t preprocessing. preprocessing non-preprocessing version give numerical results, give take minor numerical differences, distance zero, preprocessing versions may give values slightly different zero (e.g. 1e-7). New functions: correlation_distance, cosine_distance, euclidean_distance, hamming_distance, l2sqr_distance, manhattan_distance calculating distance two vectors, may useful arbitrary distance calculations nearest neighbor routines , although won’t efficient (call C++ code, though). cosine correlation calculations use non-preprocessing implementations. Generalize hamming metric standard definition. old implementation hamming metric worked binary data renamed bhamming. (contributed Vitalie Spinu) New parameter obs added functions: set obs = \"C\" can pass input data column-oriented format.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-10","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.10","text":"random_knn function used always return item neighbor, n_nbrs - 1 returned neighbors actually selected random. Even forgot doesn’t make lot sense, now really just get back n_nbrs random selections. providing pre-calculated neighbors init parameter nnd_knn graph_knn_query: previously, k specified larger number neighbors included init, gave error. Now, init augmented random neighbors reach desired k. useful way “restart” neighbor search better--random location k found small initially. Note random selection take account identities already chosen neighbors, duplicates may included augmented result, reduce effective size initialized number neighbors. Removed block_size grain_size parameters functions. related amount work done per thread, ’s obvious outside user set . long-running computations update progress indicators frequently (verbose = TRUE) respond user-requested cancellation.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-9","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.9 (20 June 2021)","text":"nnd_knn_query renamed graph_knn_query now closely follows current pynndescent graph search method (including backtracking search). New function: prepare_search_graph preparing search graph neighbor graph use graph_knn_query, using reverse nearest neighbors, occlusion pruning truncation. Sparse graphs supported input graph_knn_query.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-008-10-october-2020","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.8 (10 October 2020)","title":"rnndescent 0.0.8 (10 October 2020)","text":"major rewrite internal organization C++ less R-specific.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"minor-license-change-0-0-8","dir":"Changelog","previous_headings":"","what":"Minor License Change","title":"rnndescent 0.0.8 (10 October 2020)","text":"license rnndescent changed GPLv3 GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-8","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.8 (10 October 2020)","text":"New metric: \"correlation\". (1 minus) Pearson correlation. New function: k_occur counts k-occurrences item idx matrix, number times item appears k-nearest neighbor list dataset. distribution k-occurrences can used diagnose “hubness” dataset. Items large k-occurrence (>> k, e.g. 10k), may indicate low accuracy approximate nearest neighbor result.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-7","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.7 (1 March 2020)","text":"avoid undefined behavior issues, rnndescent now uses internal implementation RcppParallel’s parallelFor loop works std::thread load Intel’s TBB library.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-6","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.6 (29 November 2019)","text":"reason, thought ok use dqrng sample routines inside thread, despite clearly using R API extensively. ’s ok causes lots crashes. now re-implementation dqrng’s sample routines using plain std::vectors src/rnn_sample.h. file licensed AGPL (rnndescent whole remains GPL3).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-5","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.5 (23 November 2019)","text":"New function: merge_knn, combine two nearest neighbor graphs. Useful combining results multiple runs nnd_knn random_knn. Also, merge_knnl, operates list multiple neighbor graphs, can provide speed merge_knn don’t mind storing multiple graphs memory .","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-5","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.5 (23 November 2019)","text":"thread-locking issue converting R matrices internal heap data structure affected nnd_knn n_threads > 1 random_knn n_threads > 1 order_by_distance = TRUE. Potential minor speed improvement nnd_knn n_threads > 1 due use mutex pool.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-004-21-november-2019","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.4 (21 November 2019)","title":"rnndescent 0.0.4 (21 November 2019)","text":"Mainly internal clean-reduce duplication.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-4","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.4 (21 November 2019)","text":"default, nnd_knn nnd_knn_query use progress bar brute force random neighbor functions. Bring back old per-iteration logging also showed current distance sum knn progress = \"dist\" option. random_knn random_knn_query, order_by_distance = TRUE n_threads > 0, final sorting knn graph multi-threaded. Initialization nearest neighbor descent data structures also multi-threaded n_threads > 0. Progress bar updating cancellation now consistent less likely cause hanging crashing across different methods. Using Cosine Hamming distance may take less memory run bit faster.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-3","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.3 (15 November 2019)","text":"now “query” versions three functions: nnd_knn_query useful, brute_force_knn_query random_knn_query also available. allows query data search reference data, .e. returned indices distances relative reference data, member query. methods also available multi-threaded mode, nnd_knn_query low high memory version.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-3","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.3 (15 November 2019)","text":"Incremental search nearest neighbor descent didn’t work correctly, retained new neighbors marked new rather old. made search repeat distance calculations unnecessarily. Heap initialization ignored existing distances input distance matrix. l2 metric renamed l2sqr accurately reflect : square L2 (Euclidean) metric. New option use_alt_metric. Set FALSE don’t want alternative, faster metrics (keep distance ordering metric) used internal calculations. Currently applies metric = \"euclidean\", squared Euclidean distance used internally. worth setting FALSE think alternative causing numerical issues (bug, please report !). Random brute force methods make use alternative metrics. New option block_size parallel methods, determines amount work done parallel checking user interrupt request updating progress. random_knn now returns results sorted order. can turn order_distances = FALSE, don’t need sorting (e.g. using results input something else). Progress bars brute_force random methods now correct.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-2","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.2 (7 November 2019)","text":"Brute force nearest neighbor function renamed brute_force_knn. Random nearest neighbors renamed random_knn. Brute force random nearest neighbors now interruptible. Progress bar shown verbose = TRUE. fast_rand option removed, applied single-threading, negligible effect. Also, number changes inspired recent work https://github.com/lmcinnes/pynndescent: parallel nearest neighbor descent now faster. rho sampling parameter removed. size candidates (general neighbors) list now controlled entirely max_candidates. Default max_candidates reduced 20. use_set logical flag replaced low_memory, opposite meaning. now also works using multiple threads. follows pynndescent implementation, ’s still experimental, low_memory = TRUE default moment. low_memory = FALSE implementation n_threads = 0 (originally equivalent use_set = TRUE) faster. New parameter block_size, balances interleaving queuing updates versus applying current graph.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-2","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"rnndescent 0.0.2 (7 November 2019)","text":"incremental search, neighbors marked selected new candidate list even later removed due finite size candidates heap. Now, indices still retained candidate building marked new. Improved man pages (examples plus link nearest neighbor descent reference). Removed dependency Boost headers.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"rnndescent-001-27-october-2019","dir":"Changelog","previous_headings":"","what":"rnndescent 0.0.1 (27 October 2019)","title":"rnndescent 0.0.1 (27 October 2019)","text":"Initial release.","code":""},{"path":"https://jlmelville.github.io/rnndescent/news/index.html","id":"new-features-0-0-1","dir":"Changelog","previous_headings":"","what":"New features","title":"rnndescent 0.0.1 (27 October 2019)","text":"Nearest Neighbor Descent following metrics: Euclidean, Cosine, Manhattan, Hamming. Support multi-threading RcppParallel. Initialization random neighbors. Brute force nearest neighbor calculation.","code":""}]
