# rnndescent

[![Travis-CI Build Status](https://travis-ci.org/jlmelville/rnndescent.svg?branch=master)](https://travis-ci.org/jlmelville/rnndescent) [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/jlmelville/rnndescent?branch=master&svg=true)](https://ci.appveyor.com/project/jlmelville/rnndescent) [![Coverage Status](https://img.shields.io/codecov/c/github/jlmelville/rnndescent/master.svg)](https://codecov.io/github/jlmelville/rnndescent?branch=master)

An R package implementing the Nearest Neighbor Descent method
([Dong et al., 2011](https://doi.org/10.1145/1963405.1963487)) for finding
approximate nearest neighbors, based on the Python library
[PyNNDescent](https://github.com/lmcinnes/pynndescent).

Still heavily in development, but possibly not totally useless for optimizing
an initial set of nearest neighbors, e.g. those generated by
[RcppAnnoy](https://cran.r-project.org/package=RcppAnnoy) or
[RcppHNSW](https://cran.r-project.org/package=RcppHNSW).

## Current Status

*27 October 2019* `rnndescent` creeps towards usability. A multi-threaded
implementation (using
[RcppParallel](https://cran.r-project.org/package=RcppParallel)) has now been
added.

## Installation

```r
remotes::install_github("jlmelville/rnndescent")
```

This packages makes use of C++ code which must be compiled. You may have to
carry out a few extra steps before being able to build:

**Windows**: install
[Rtools](https://cran.r-project.org/bin/windows/Rtools/) and ensure
`C:\Rtools\bin` is on your path.

**Mac OS X**: using a custom `~/.R/Makevars`
[may cause linking errors](https://github.com/jlmelville/uwot/issues/1).
This sort of thing is a potential problem on all platforms but seems to bite
Mac owners more.
[The R for Mac OS X FAQ](https://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#Installation-of-source-packages)
may be helpful here to work out what you can get away with. To be on the safe
side, I would advise building `uwot` without a custom `Makevars`.

**Everybody**: if your `.Rprofile` files print any messages, be sure to wrap
them in `if (interactive()) { ... }` statements:

```R
# Example of a good .Rprofile file:
if (interactive()) {
    cat("I am printing this message in interactive sessions\n")
}

# Example of a bad .Rprofile file:
cat("I am printing this message all of the time\n")
```

This project's `Makevars` file relies on a clean output to correctly configure [RcppParallel](https://cran.r-project.org/package=RcppParallel). If compilation
fails and you see startup messages in the build output, this is what is
happening.

## Example

Optimizing an initial set of approximate nearest neighbors:

```R
# install.packages("RcppHNSW")
library(rnndescent)

irism <- as.matrix(iris[, -5])
# Use settings that don't get perfect results straight away
iris_hnsw_nn <- RcppHNSW::hnsw_knn(irism, k = 15, M = 2, distance = "euclidean")

# nn descent improves results: set verbose = TRUE to track distance sum progress
# over iterations
res <- nnd_knn(irism, metric = "euclidean", init = iris_hnsw_nn, verbose = TRUE)

# search can be multi-threaded
res <- nnd_knn(irism, metric = "euclidean", init = iris_hnsw_nn, verbose = TRUE, n_threads = 4)

# a faster version of the algorithm is available that avoids some repeated distance
# calculations at the cost of using more memory. Currently off by default.
res <- nnd_knn(irism, metric = "euclidean", init = iris_hnsw_nn, verbose = TRUE, n_threads = 4, 
               low_memory = FALSE)
```

Currently, only the nearest neighbor descent part of PyNNDescent is implemented,
not the random projection tree method to create the initial set of neighbors. If
you really wish to avoid any dependencies from another library, then the search
is initialized from a random set of neighbors:

```R
library(rnndescent)

irism <- as.matrix(iris[, -5])

# picks indices at random and then carries out nearest neighbor descent
res <- nnd_knn(irism, k = 15, metric = "euclidean", n_threads = 4)

# if you want the random indices and their distances:
iris_rand_nn <- random_knn(irism, k = 15, metric = "euclidean", n_threads = 4)
```

Although the initialization can also be multi-threaded (and at least has speed
in its favor), the descent stage will take more iterations to get to a good
result and may converge to a less optimal result.

For comparison with exact results, there is also a `brute_force_knn` function,
that will generate the exact nearest neighbors by the simple process of trying
every possible pair in the dataset. Obviously this becomes a very time consuming
process as your dataset grows in size, even with multithreading (although the
`iris` dataset in the example below doesn't present any issues).

```R
iris_exact_nn <- brute_force_knn(irism, k = 15, metric = "euclidean", n_threads = 4)
```

## Supported Metrics

Euclidean, Manhattan, Cosine and Hamming. Note that these have been implemented
in a simple fashion, so no clever (but non-portable) optimizations using
AVX/SSE or specialized `popcount` routines are used.

## Citation

Dong, W., Moses, C., & Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In *Proceedings of the 20th international conference on World wide web* (pp. 577-586). ACM.
[doi.org/10.1145/1963405.1963487](https://doi.org/10.1145/1963405.1963487).

## License

The R and Rcpp interface code is
[GPLv3 or later](https://www.gnu.org/licenses/gpl-3.0.txt). The underlying C++
library, which can be found under `inst/include`, is
[BSD 2-Clause](https://opensource.org/licenses/BSD-2-Clause) licensed.

## See Also

* [PyNNDescent](https://github.com/lmcinnes/pynndescent), the Python implementation.
* [nndescent](https://github.com/TatsuyaShirakawa/nndescent), a C++ implementation.
* [NearestNeighborDescent.jl](https://github.com/dillondaudert/NearestNeighborDescent.jl),
a Julia implementation.
* [nn_descent](https://github.com/eskomski/nn_descent), a C implementation.
