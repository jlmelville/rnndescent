# rnndescent

<!-- badges: start -->
[![R-CMD-check](https://github.com/jlmelville/rnndescent/workflows/R-CMD-check/badge.svg)](https://github.com/jlmelville/rnndescent/actions)
[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/jlmelville/rnndescent?branch=master&svg=true)](https://ci.appveyor.com/project/jlmelville/rnndescent)
[![Coverage Status](https://img.shields.io/codecov/c/github/jlmelville/rnndescent/master.svg)](https://codecov.io/github/jlmelville/rnndescent?branch=master)
[![Last Commit](https://img.shields.io/github/last-commit/jlmelville/rnndescent)](https://github.com/jlmelville/rnndescent)
<!-- badges: end -->

An R package for finding approximate nearest neighbors, translated from the 
Python package [PyNNDescent](https://github.com/lmcinnes/pynndescent) written 
by the great Leland McInnes. As the name suggests, it uses the Nearest Neighbor 
Descent method ([Dong et al., 2011](https://doi.org/10.1145/1963405.1963487)), 
but also makes use of Random Partition Trees 
([Dasgupta and Freund, 2008](https://doi.org/10.1145/1374376.1374452))
as well as some other things too. For more details, you will need to read the
currently non-existent vignettes which I promise to get round to writing, or 
check out the
[PyNNDescent documentation](https://pynndescent.readthedocs.io/en/latest/).

Tantalizingly close to being releasable, you can now use rnndescent for:

* optimizing an initial set of nearest neighbors, e.g. those generated by
[RcppAnnoy](https://cran.r-project.org/package=RcppAnnoy) or
[RcppHNSW](https://cran.r-project.org/package=RcppHNSW).
* using this package for nearest neighbor search all on its own...
* ... including finding nearest neighbors on sparse data, which most other 
packages in the R ecosystem cannot do.
* and a much larger number of metrics than most other packages.

## Current Status

*13 November 2023. I have added most of the metrics that don't need extra
parameters for both sparse and non-sparse data, e.g. `braycurtis`, `dice`,
`jaccard`, `hellinger` etc. See the `Missing Metrics` section at the end of this
README for those which are not implemented. There are a few breaking changes
(mainly around the hamming metric, see `NEWS.md` for the exact details).

### Missing Features 

Compared to pynndescent, rnndescent is currently lacking, in decreasing order
of likelihood of implementation:

* meaningful documentation.
* a class to hide some of the complexities of pulling all the methods together
simply.
* some of the distance metrics. A large number are currently supported though.
See `Missing Metrics` below for those that are currently not available.
* custom metrics. This just isn't feasible with a C++ implementation.

## Installation

No CRAN release, so you must install from this repo:

```r
# install.packages("pak")
pak::pkg_install("jlmelville/rnndescent")
```

This packages makes use of C++ code which must be compiled. You may have to
carry out a few extra steps before being able to build:

**Windows**: install
[Rtools](https://cran.r-project.org/bin/windows/Rtools/) and ensure
`C:\Rtools\bin` is on your path.

**Mac OS X**: using a custom `~/.R/Makevars`
[may cause linking errors](https://github.com/jlmelville/uwot/issues/1).
This sort of thing is a potential problem on all platforms but seems to bite
Mac owners more.
[The R for Mac OS X FAQ](https://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#Installation-of-source-packages)
may be helpful here to work out what you can get away with. To be on the safe
side, I would advise building without a custom `Makevars`.

rnndescent uses C++17. This shouldn't be too big a problem but not all R 
platforms support it (sorry if this affects you).

## Example

Optimizing an initial set of approximate nearest neighbors:

```R
library(rnndescent)

# both hnsw_knn and nnd_knn will remove non-numeric columns from data-frames
# for you, but to avoid confusion, these examples will use a matrix
irism <- as.matrix(iris[, -5])


# Generate a Random Projection knn (set n_threads for parallel search):
iris_rp_nn <- rpf_knn(irism, k = 15)

# nn descent improves results: set verbose = TRUE and progress = "dist", to 
# track distance sum progress over iterations
res <-
  nnd_knn(irism,
    metric = "euclidean",
    init = iris_rp_nn,
    verbose = TRUE,
    progress = "dist"
  )

# search can be multi-threaded
res <-
  nnd_knn(
    irism,
    metric = "euclidean",
    init = iris_rp_nn,
    verbose = TRUE,
    n_threads = 4
  )

# a (potentially) faster version of the algorithm is available that avoids some 
# repeated distance calculations at the cost of using more memory. Currently off
# by default.
res <-
  nnd_knn(
    irism,
    metric = "euclidean",
    init = iris_rp_nn,
    verbose = TRUE,
    n_threads = 4,
    low_memory = FALSE
  )
  
# You can optimize results from other methods or packages too:  
# install.packages("RcppHNSW")
# Use settings that don't get perfect results straight away
iris_hnsw_nn <-
  RcppHNSW::hnsw_knn(irism,
    k = 15,
    M = 2,
    distance = "euclidean"
  )

res <-
  nnd_knn(
    irism,
    metric = "euclidean",
    init = iris_hnsw_nn,
    verbose = TRUE,
    n_threads = 4,
    low_memory = FALSE
  )
```

You can also search the neighbor graph with new query items:

```R
# 100 reference iris items
iris_ref <- iris[iris$Species %in% c("setosa", "versicolor"), ]

# 50 query items
iris_query <- iris[iris$Species == "versicolor", ]

# First, find the approximate 10-nearest neighbor graph for the references:
iris_ref_knn <- nnd_knn(iris_ref, k = 10)

# For each item in iris_query find the 10 nearest neighbors in iris_ref
# You need to pass both the reference data and the knn graph.
iris_query_nn <-
  graph_knn_query(
    query = iris_query,
    reference = iris_ref,
    reference_graph = iris_ref_knn,
    k = 4,
    metric = "euclidean",
    verbose = TRUE
  )
```

Although the above example shows the basic procedure of building the graph and
then querying new data with it, the raw nearest neighbor graph isn't very
efficient in general. It's highly advisable to use a refined search graph based
on the original data and neighbor graph:

```R
iris_search_graph <-
  prepare_search_graph(iris_ref, iris_ref_knn, verbose = TRUE)
iris_query_nn <-
  graph_knn_query(
    query = iris_query,
    reference = iris_ref,
    reference_graph = iris_search_graph,
    k = 4,
    metric = "euclidean",
    verbose = TRUE
  )
```

See the help text to `prepare_search_graph` for various parameters you can 
change to control the trade off between speed and search accuracy.

## Initialization

The default for `nnd_knn` is to initialize with random neighbors. Set
`init = "tree"` to use the random partition tree initialization.

```R
library(rnndescent)

irism <- as.matrix(iris[, -5])

# picks indices at random and then carries out nearest neighbor descent
res <- nnd_knn(irism,
  k = 15,
  metric = "euclidean",
  n_threads = 4
)

# if you want the random indices and their distances:
iris_rand_nn <-
  random_knn(irism,
    k = 15,
    metric = "euclidean",
    n_threads = 4
  )
  
# use RP tree initialization
res <- nnd_knn(irism,
  k = 15,
  metric = "euclidean",
  n_threads = 4,
  init = "tree"
)
```

For initializing a knn query, there is also `random_knn_query`.

## Brute Force

For comparison with exact results, there is also `brute_force_knn` and
`brute_force_knn_query` functions, that will generate the exact nearest
neighbors by the simple process of trying every possible pair in the dataset.
Obviously this becomes a very time consuming process as your dataset grows in
size, even with multithreading (although the `iris` dataset in the example below
doesn't present any issues).

```R
iris_exact_nn <-
  brute_force_knn(irism,
    k = 15,
    metric = "euclidean",
    n_threads = 4
  )
```

## Merging

Also available are two functions for merging multiple approximate nearest neighbor
graphs, which will result in a new graph which is at least as good as the best
graph provided. For merging pairs of graphs, use `merge_knn`:

```R
set.seed(1337)
# Nearest neighbor descent with 15 neighbors for iris three times,
# starting from a different random initialization each time
iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1)
iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1)

# Merged results should be an improvement over either individual results
iris_mnn <- merge_knn(iris_rnn1, iris_rnn2)
sum(iris_mnn$dist) < sum(iris_rnn1$dist)
sum(iris_mnn$dist) < sum(iris_rnn2$dist)
```

If you have more than two graphs stored in memory, it's more efficient to use
the list-based version, `merge_knnl`:

```R
set.seed(1337)
# Nearest neighbor descent with 15 neighbors for iris three times,
# starting from a different random initialization each time
iris_rnn1 <- nnd_knn(iris, k = 15, n_iters = 1)
iris_rnn2 <- nnd_knn(iris, k = 15, n_iters = 1)
iris_rnn3 <- nnd_knn(iris, k = 15, n_iters = 1)

iris_mnn <- merge_knnl(list(iris_rnn1, iris_rnn2, iris_rnn3))
```

## Hubness Diagnostic

The `k_occur` function takes an `idx` matrix and returns a vector of the
k-occurrences for each item in the dataset. This is just the number of times an
item was found in the k-nearest neighbor list of another item. If you think of
the `idx` matrix as representing a directed graph where the element `idx[i, j]`
in the matrix is an edge from node `i` to node `idx[i, j]`, then the
k_occurrences are calculated by reversing each edge and then counts the number
of edges incident to each node. Alternatively, in the nomenclature of nearest
neighbor descent, it's the size of the "reverse neighbor" list for each node.

```R
iris_nnd <- nnd_knn(iris, k = 15)
kos <- k_occur(iris_nnd$idx)
```

The k-occurrence can take a value between 0 and `N` the number of items in the
dataset. Values much larger than `k` indicate that the item is potentially a
hub. The presence of hubs in a dataset can reduce the accuracy of the
approximate nearest neighbors returned by nearest neighbor descent, but the
presence of hubs as determined by the distribution of k-occurrences is quite
robust even in the case of an approximate nearest neighbor graph of low
accuracy. Therefore calculating the k-occurrences on the output of nearest
neighbor descent is worth doing: if the maximum k-occurrence is a lot larger
than `k` (I suggest `10 * k` as a danger sign), then the accuracy of the
approximate nearest neighbors may be compromised. Items with low k-occurrences
are most likely to be affected in this way. Increasing `k` or the
`max_candidates` parameter can help in these situations. Alternatively, querying
the data against itself with `graph_knn_query` can help:

```R
# Purposely don't do a very good job with NND so we have something to improve
iris_nnd <- nnd_knn(iris, k = 15, n_iters = 1)
iris_search_graph <-
  prepare_search_graph(iris, iris_nnd)

# query and reference are the same
iris_query_nn <-
  graph_knn_query(
    query = iris,
    reference = iris,
    reference_graph = iris_search_graph,
    init = iris_nnd,
    k = 15
  )
# Compare 
sum(iris_nnd$dist)
sum(iris_query_nn$dist)
```

For more on hubness and nearest neighbors, see for example 
[Radovanović and co-workers, 2010](https://www.jmlr.org/papers/v11/radovanovic10a.html)
and [Bratić and co-workers, 2019](https://doi.org/10.1142/S0218213019600029).

## Supported Metrics

Euclidean, Manhattan, Cosine, Correlation (1 - the Pearson correlation,
implemented as cosine distance on row-centered data) and Hamming. Note that
these have been implemented in a simple fashion, so no clever (but non-portable)
optimizations using AVX/SSE or specialized `popcount` routines are used.

## Citation

Dong, W., Moses, C., & Li, K. (2011, March).
Efficient k-nearest neighbor graph construction for generic similarity measures.
In *Proceedings of the 20th international conference on World wide web* (pp. 577-586). ACM.
[doi.org/10.1145/1963405.1963487](https://doi.org/10.1145/1963405.1963487).

## License

The R package as a whole is licensed under
[GPLv3 or later](https://www.gnu.org/licenses/gpl-3.0.txt). The following
files are licensed differently:

* `inst/include/dqsample.h` is a modification of some sampling code
from [dqrng](https://github.com/daqana/dqrng) and is
[AGPLv3 or later](https://www.gnu.org/licenses/agpl-3.0.en.html).
* `inst/include/RcppPerpendicular.h` is a modification of some code from
from [RcppParallel](https://github.com/RcppCore/RcppParallel) and is
[GPLv2 or later](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)
* The underlying nearest neighbor descent C++ library, which can be found under
`inst/include/tdoann`, is licensed under the 
[BSD 2-Clause](https://opensource.org/licenses/BSD-2-Clause).

As far as I know, these licenses are all compatible with re-licensing under
GPLv3 or later.

## Missing Metrics

The following metrics are in PyNNDescent but are not supported in rnndescent:

* Circular Kantorovich
* Haversine
* Kantorovich
* Mahalanobis
* Minkowski
* Sinkhorn
* Standardised Euclidean
* Wasserstein 1d
* Weighted Minkowski

These require passing extra information as part of the metric definition, which
is not currently supported. 

## See Also

* [PyNNDescent](https://github.com/lmcinnes/pynndescent), the Python implementation.
* [nndescent](https://github.com/TatsuyaShirakawa/nndescent), a C++ implementation.
* [NearestNeighborDescent.jl](https://github.com/dillondaudert/NearestNeighborDescent.jl),
a Julia implementation.
* [nn_descent](https://github.com/eskomski/nn_descent), a C implementation.
* [NNDescent.cpp](https://github.com/AnabelSMRuggiero/NNDescent.cpp), another
C++ implementation.
* [nndescent](https://github.com/brj0/nndescent), another C++ implementation, with Python bindings.

## Old News

*06 November 2023* Sparse data support has been added. You should be able to 
use e.g. a `dgCMatrix` with all the methods and currently supported metrics as
easily as a dense matrix.

*30 October 2023* At last, a workable random partition forest implementation
has been added. This can be used standalone (e.g. `rpf_knn`, 
`rpf_build`, `rpt_knn_query`) or as initialization to nearest neighbor descent
(`nnd_knn(init = "tree", ...)`). The forest itself can be serialized with 
`saveRDS` but you will pay a price for that convenience by having to pass it 
back and forth from the R to C++ layer when querying. For now there is no 
access to the underlying C++ class via R like in RcppHNSW and RcppAnnoy so it
may not be suitable for some use cases.

*19 October 2023* Inevitably 0.0.11 is here because of a bug in 0.0.10 where 
nearest neighbor descent was not correctly flagging new/old neighbors which
reduced performance (but not the actual result).

*18 October 2023* A long-postponed major internal refactoring means I might be 
able to make a bit of progress on this package. For now, the `cosine` and
`correlation` metrics have migrated to not preprocessing their data (these 
versions are still available as `cosine-preprocess` and `correlation-preprocess`
respectively). Also, I have exported the distance metrics as R functions (e.g.
`cosine_distance`, `euclidean_distance`).

*18 September 2021* The `"hamming"` metric now supports integer-valued (not just
binary) inputs, thanks to a contribution from
[Vitalie Spinu](https://github.com/vspinu). The older metric code path for
binary data only is supported via `metric = "bhamming"`.

*20 June 2021* A big step forward in usefulness with the addition of the
`prepare_search_graph` function which creates and prunes an undirected search
graph from the neighbor graph for use with the (now re-named) `graph_knn_query`
function. The latter is now also capable of backtracking search and performs
fairly well.

*4 October 2020* Added `"correlation"` as a metric and the `k_occur` function
to help diagnose potential
[hubness](https://www.jmlr.org/papers/v11/radovanovic10a.html) in a dataset.

*23 November 2019* Added `merge_knn` and `merge_knnl` for combining multiple
nn results.

*15 November 2019* It is now possible to query a reference set of data to
produce the approximate knn graph relative to the references (i.e. none of the
queries will be selected as neighbors) via `nnd_knn_query` (and related
`brute_force` and `random` variants).

*27 October 2019* `rnndescent` creeps towards usability. A multi-threaded
implementation (using
[RcppParallel](https://cran.r-project.org/package=RcppParallel)) has now been
added.
